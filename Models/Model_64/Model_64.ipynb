{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "id": "sxZ2y5b_LpJm"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ReviewsEN.csv')"
      ],
      "metadata": {
        "id": "04nR9h2oMQI9"
      },
      "execution_count": 73,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-1 :\", df['sentiment'].value_counts()[-1])\n",
        "print(\"0 :\", df['sentiment'].value_counts()[0])\n",
        "print(\"1 :\", df['sentiment'].value_counts()[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G8nNLbYiMdXW",
        "outputId": "cc613959-b117-438b-f4cc-815a55865e1a"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 : 553\n",
            "0 : 404\n",
            "1 : 948\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['sentiment'].unique()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rv-VpmbjMgaH",
        "outputId": "047ad3b0-0670-47cd-9c4c-8522dd8dd863"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 0,  1, -1])"
            ]
          },
          "metadata": {},
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace values in pandas DataFrame.\n",
        "df['sentiment'] = df['sentiment'].replace([1], 2)\n",
        "df['sentiment'] = df['sentiment'].replace([0], 1)\n",
        "df['sentiment'] = df['sentiment'].replace([-1], 0)"
      ],
      "metadata": {
        "id": "FpS1MYCRMxLH"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df['reviews'] = df['reviews'].apply(str.lower)\n",
        "print(df)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xJyQLbZhM4Tv",
        "outputId": "576646dd-2ddf-4364-eab3-ae162d9017f6"
      },
      "execution_count": 77,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      sentiment                                            reviews\n",
            "0             1  teacher are punctual but they should also give...\n",
            "1             2                                               good\n",
            "2             2  excellent lectures are delivered by teachers a...\n",
            "3             2  teachers give us all the information required ...\n",
            "4             2                                                yes\n",
            "...         ...                                                ...\n",
            "1900          2  the greatest teacher i have ever had. he's bri...\n",
            "1901          2  the best professor that we have in stcloud sta...\n",
            "1902          2  highest recommendations. his accent and integr...\n",
            "1903          2  great teacher, go to class and do hw, you'll d...\n",
            "1904          2  very good professor, probably the best in the ...\n",
            "\n",
            "[1905 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "X172llSbQdiH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file in read mode\n",
        "my_file = open(\"stopwords.txt\", \"r\")\n",
        "  \n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "  \n",
        "# replacing end splitting the text \n",
        "# when newline ('\\n') is seen.\n",
        "stopwords_data = data.split(\"\\n\")\n",
        "print(stopwords_data)\n",
        "my_file.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "G5FAJTR4QhB8",
        "outputId": "8e341e9f-e7c9-495d-b628-5b3bbe2a7427"
      },
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Remove punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "    return text_without_punctuation"
      ],
      "metadata": {
        "id": "yBnKXvrHxUxZ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "for i in range(len(df)):\n",
        "  cleaned_text.append(remove_punctuation(df['reviews'][i]))\n",
        "\n",
        "df['reviews'] = cleaned_text\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xgZ5cILbxWj3",
        "outputId": "d1d11400-c018-4d54-b82c-7ec1b4c16ace"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['teacher are punctual but they should also give us the some practical knowledge other than theortical',\n",
              "       'good',\n",
              "       'excellent lectures are delivered by teachers and all teachers are very punctual',\n",
              "       ...,\n",
              "       'highest recommendations his accent and integrity will keep you awake',\n",
              "       'great teacher go to class and do hw youll do good',\n",
              "       'very good professor probably the best in the csci department'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: remove_stopwords\n",
        "def remove_stopwords(sentence, data):\n",
        "    \"\"\"\n",
        "    Removes a list of stopwords\n",
        "    \n",
        "    Args:\n",
        "        sentence (string): sentence to remove the stopwords from\n",
        "    \n",
        "    Returns:\n",
        "        sentence (string): lowercase sentence without the stopwords\n",
        "    \"\"\"\n",
        "    # List of stopwords\n",
        "\n",
        "    stopwords = data + [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    numbers_stopwords = [\"1\", \"2\", \"3\", \"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n",
        "                         \"one\", \"two\",\"three\",\"four\",\"five\"]\n",
        "    final_stopwords = stopwords + numbers_stopwords\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    words = sentence.split()\n",
        "    tempWords = []\n",
        "    for i in words:\n",
        "        if i not in final_stopwords:\n",
        "            tempWords.append(i)\n",
        "            sentence = ' '.join(tempWords)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "GnGSEv3TQlvX"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VG1f0s-lVej3",
        "outputId": "5fa22d8c-fee2-4f22-f923-00ef1618c474"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['teacher are punctual but they should also give us the some practical knowledge other than theortical',\n",
              "       'good',\n",
              "       'excellent lectures are delivered by teachers and all teachers are very punctual',\n",
              "       ...,\n",
              "       'highest recommendations his accent and integrity will keep you awake',\n",
              "       'great teacher go to class and do hw youll do good',\n",
              "       'very good professor probably the best in the csci department'],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 82
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cKomen = []\n",
        "for i in range(len(df)):\n",
        "  cKomen.append(remove_stopwords(df['reviews'][i], stopwords_data))\n",
        "\n",
        "df['reviews'] = cKomen"
      ],
      "metadata": {
        "id": "FfCv1uX9RW1Z"
      },
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Splitting Dataset"
      ],
      "metadata": {
        "id": "Z_J2_awbdhPE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to split 80% of the data for training and the rest of the data will become testing data"
      ],
      "metadata": {
        "id": "ExYtSB8DdmmM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "split = round(len(df)*0.95)\n",
        "train_reviews = df['reviews'][:split]\n",
        "train_label = df['sentiment'][:split]\n",
        "test_reviews = df['reviews'][split:]\n",
        "test_label = df['sentiment'][split:]"
      ],
      "metadata": {
        "id": "FD8H87rrdlQ8"
      },
      "execution_count": 84,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "training_sentences = []\n",
        "training_labels = []\n",
        "testing_sentences = []\n",
        "testing_labels = []\n",
        "for row in train_reviews:\n",
        "    training_sentences.append(str(row))\n",
        "for row in train_label:\n",
        "    training_labels.append(row)\n",
        "for row in test_reviews:\n",
        "    testing_sentences.append(str(row))\n",
        "for row in test_label:\n",
        "    testing_labels.append(row)"
      ],
      "metadata": {
        "id": "YN6nTHHXdw3E"
      },
      "execution_count": 85,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Hyper Parameter "
      ],
      "metadata": {
        "id": "Rlgyk8UX7KcE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameter\n",
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 64\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post'"
      ],
      "metadata": {
        "id": "L94YTf597JzF"
      },
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "word_index"
      ],
      "metadata": {
        "id": "tqaZxRfi70Ql",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0870ecbc-c665-4059-fb78-0ac124bf9917"
      },
      "execution_count": 87,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'class': 2,\n",
              " 'students': 3,\n",
              " 'course': 4,\n",
              " 'teacher': 5,\n",
              " 'material': 6,\n",
              " 'professor': 7,\n",
              " 'lecturer': 8,\n",
              " 'easy': 9,\n",
              " 'understand': 10,\n",
              " 'hard': 11,\n",
              " 'helpful': 12,\n",
              " 'dont': 13,\n",
              " 'help': 14,\n",
              " 'lectures': 15,\n",
              " 'learning': 16,\n",
              " 'questions': 17,\n",
              " 'teaching': 18,\n",
              " 'lecture': 19,\n",
              " 'tests': 20,\n",
              " 'lecturers': 21,\n",
              " 'time': 22,\n",
              " 'lot': 23,\n",
              " 'math': 24,\n",
              " 'nice': 25,\n",
              " 'learn': 26,\n",
              " 'university': 27,\n",
              " 'recommend': 28,\n",
              " 'jokes': 29,\n",
              " 'makes': 30,\n",
              " 'study': 31,\n",
              " 'sometimes': 32,\n",
              " 'subject': 33,\n",
              " 'hes': 34,\n",
              " 'difficult': 35,\n",
              " 'exam': 36,\n",
              " 'test': 37,\n",
              " 'exams': 38,\n",
              " 'teachers': 39,\n",
              " 'materials': 40,\n",
              " 'knowledge': 41,\n",
              " 'prof': 42,\n",
              " 'available': 43,\n",
              " 'knowledgeable': 44,\n",
              " 'assignments': 45,\n",
              " 'doesnt': 46,\n",
              " 'books': 47,\n",
              " 'lab': 48,\n",
              " 'excellent': 49,\n",
              " 'didnt': 50,\n",
              " 'paper': 51,\n",
              " 'boring': 52,\n",
              " 'classes': 53,\n",
              " 'library': 54,\n",
              " 'shes': 55,\n",
              " 'guy': 56,\n",
              " 'student': 57,\n",
              " 'bad': 58,\n",
              " 'extra': 59,\n",
              " 'homework': 60,\n",
              " 'answer': 61,\n",
              " 'taking': 62,\n",
              " 'book': 63,\n",
              " 'learned': 64,\n",
              " 'final': 65,\n",
              " 'engaging': 66,\n",
              " 'feel': 67,\n",
              " 'read': 68,\n",
              " 'helped': 69,\n",
              " 'notes': 70,\n",
              " 'activities': 71,\n",
              " 'experience': 72,\n",
              " 'teach': 73,\n",
              " 'pattern': 74,\n",
              " 'instructor': 75,\n",
              " 'teaches': 76,\n",
              " 'concepts': 77,\n",
              " 'content': 78,\n",
              " 'pass': 79,\n",
              " 'fine': 80,\n",
              " 'able': 81,\n",
              " 'courses': 82,\n",
              " 'practical': 83,\n",
              " 'funny': 84,\n",
              " 'fair': 85,\n",
              " 'pretty': 86,\n",
              " 'marks': 87,\n",
              " 'means': 88,\n",
              " 'willing': 89,\n",
              " 'credit': 90,\n",
              " 'grade': 91,\n",
              " 'taught': 92,\n",
              " 'papers': 93,\n",
              " 'accent': 94,\n",
              " 'grading': 95,\n",
              " 'resources': 96,\n",
              " 'delivery': 97,\n",
              " 'definitely': 98,\n",
              " 'checking': 99,\n",
              " 'understanding': 100,\n",
              " 'interaction': 101,\n",
              " 'semester': 102,\n",
              " 'depth': 103,\n",
              " 'little': 104,\n",
              " 'youll': 105,\n",
              " 'worst': 106,\n",
              " 'attention': 107,\n",
              " 'overall': 108,\n",
              " 'follow': 109,\n",
              " 'awesome': 110,\n",
              " 'ive': 111,\n",
              " 'im': 112,\n",
              " 'extremely': 113,\n",
              " 'facilities': 114,\n",
              " 'relevant': 115,\n",
              " 'stuff': 116,\n",
              " 'labs': 117,\n",
              " 'evaluations': 118,\n",
              " 'wasnt': 119,\n",
              " 'cares': 120,\n",
              " 'write': 121,\n",
              " 'challenging': 122,\n",
              " 'fun': 123,\n",
              " 'punctuality': 124,\n",
              " 'hours': 125,\n",
              " 'tough': 126,\n",
              " 'question': 127,\n",
              " 'passionate': 128,\n",
              " 'practice': 129,\n",
              " 'wish': 130,\n",
              " 'cant': 131,\n",
              " 'provide': 132,\n",
              " 'bit': 133,\n",
              " 'getting': 134,\n",
              " 'midterm': 135,\n",
              " 'grades': 136,\n",
              " 'pay': 137,\n",
              " 'good': 138,\n",
              " 'highly': 139,\n",
              " 'explain': 140,\n",
              " 'comments': 141,\n",
              " 'average': 142,\n",
              " 'people': 143,\n",
              " 'tell': 144,\n",
              " 'answers': 145,\n",
              " 'system': 146,\n",
              " 'super': 147,\n",
              " 'times': 148,\n",
              " 'youre': 149,\n",
              " 'lots': 150,\n",
              " 'life': 151,\n",
              " 'person': 152,\n",
              " 'examples': 153,\n",
              " 'faculty': 154,\n",
              " 'helps': 155,\n",
              " 'terms': 156,\n",
              " 'love': 157,\n",
              " 'terrible': 158,\n",
              " 'okay': 159,\n",
              " 'examination': 160,\n",
              " 'provides': 161,\n",
              " 'topics': 162,\n",
              " 'level': 163,\n",
              " 'reading': 164,\n",
              " 'try': 165,\n",
              " 'environment': 166,\n",
              " 'extracurricular': 167,\n",
              " 'quizzes': 168,\n",
              " 'professors': 169,\n",
              " 'business': 170,\n",
              " 'improve': 171,\n",
              " 'proper': 172,\n",
              " 'subjects': 173,\n",
              " 'valuable': 174,\n",
              " 'explains': 175,\n",
              " 'rude': 176,\n",
              " 'week': 177,\n",
              " 'tries': 178,\n",
              " 'actually': 179,\n",
              " 'presentation': 180,\n",
              " 'effective': 181,\n",
              " 'friendly': 182,\n",
              " 'school': 183,\n",
              " 'neither': 184,\n",
              " 'writing': 185,\n",
              " 'takes': 186,\n",
              " 'attend': 187,\n",
              " 'office': 188,\n",
              " 'found': 189,\n",
              " 'unclear': 190,\n",
              " 'online': 191,\n",
              " 'cool': 192,\n",
              " 'ok': 193,\n",
              " 'thats': 194,\n",
              " 'projects': 195,\n",
              " 'feedback': 196,\n",
              " 'distribution': 197,\n",
              " 'confusing': 198,\n",
              " 'alot': 199,\n",
              " 'organized': 200,\n",
              " 'stories': 201,\n",
              " 'care': 202,\n",
              " 'presenter': 203,\n",
              " 'succeed': 204,\n",
              " 'sense': 205,\n",
              " 'guide': 206,\n",
              " 'required': 207,\n",
              " 'concept': 208,\n",
              " 'style': 209,\n",
              " 'interactive': 210,\n",
              " 'participate': 211,\n",
              " 'isnt': 212,\n",
              " 'based': 213,\n",
              " 'cover': 214,\n",
              " 'goes': 215,\n",
              " 'look': 216,\n",
              " 'skills': 217,\n",
              " 'informative': 218,\n",
              " 'amazing': 219,\n",
              " 'effort': 220,\n",
              " 'talking': 221,\n",
              " 'approachable': 222,\n",
              " 'disorganized': 223,\n",
              " 'wont': 224,\n",
              " 'stay': 225,\n",
              " 'classroom': 226,\n",
              " 'discussions': 227,\n",
              " 'guides': 228,\n",
              " 'information': 229,\n",
              " 'talk': 230,\n",
              " 'easily': 231,\n",
              " 'provided': 232,\n",
              " 'worth': 233,\n",
              " 'personal': 234,\n",
              " 'glad': 235,\n",
              " 'field': 236,\n",
              " 'looking': 237,\n",
              " 'prepared': 238,\n",
              " 'chemistry': 239,\n",
              " 'complex': 240,\n",
              " 'loved': 241,\n",
              " 'summer': 242,\n",
              " 'wonderful': 243,\n",
              " 'punctual': 244,\n",
              " 'mark': 245,\n",
              " 'opportunities': 246,\n",
              " 'real': 247,\n",
              " 'absolutely': 248,\n",
              " 'enjoyable': 249,\n",
              " 'textbook': 250,\n",
              " 'hell': 251,\n",
              " 'enthusiastic': 252,\n",
              " 'supportive': 253,\n",
              " 'research': 254,\n",
              " 'simple': 255,\n",
              " 'essays': 256,\n",
              " 'sweet': 257,\n",
              " 'improved': 258,\n",
              " 'manner': 259,\n",
              " 'coming': 260,\n",
              " 'world': 261,\n",
              " 'sufficient': 262,\n",
              " 'syllabus': 263,\n",
              " 'covered': 264,\n",
              " 'fail': 265,\n",
              " 'step': 266,\n",
              " 'avoid': 267,\n",
              " 'horrible': 268,\n",
              " 'matter': 269,\n",
              " 'career': 270,\n",
              " 'positive': 271,\n",
              " 'accessible': 272,\n",
              " 'caring': 273,\n",
              " 'heavy': 274,\n",
              " 'strong': 275,\n",
              " 'giving': 276,\n",
              " 'topic': 277,\n",
              " 'arent': 278,\n",
              " 'focus': 279,\n",
              " 'irrelevant': 280,\n",
              " 'deep': 281,\n",
              " 'english': 282,\n",
              " 'probably': 283,\n",
              " 'discussion': 284,\n",
              " 'review': 285,\n",
              " 'expected': 286,\n",
              " 'project': 287,\n",
              " 'outside': 288,\n",
              " 'job': 289,\n",
              " 'prepare': 290,\n",
              " 'held': 291,\n",
              " 'day': 292,\n",
              " 'confused': 293,\n",
              " 'apply': 294,\n",
              " 'enjoyed': 295,\n",
              " 'instructors': 296,\n",
              " 'decent': 297,\n",
              " 'created': 298,\n",
              " 'engaged': 299,\n",
              " 'studies': 300,\n",
              " 'truly': 301,\n",
              " 'humor': 302,\n",
              " 'harder': 303,\n",
              " 'hw': 304,\n",
              " 'chapter': 305,\n",
              " 'perfect': 306,\n",
              " 'thinking': 307,\n",
              " 'practicals': 308,\n",
              " 'satisfied': 309,\n",
              " 'regarding': 310,\n",
              " 'comes': 311,\n",
              " 'mean': 312,\n",
              " 'completely': 313,\n",
              " 'choice': 314,\n",
              " 'negative': 315,\n",
              " 'wrong': 316,\n",
              " 'expects': 317,\n",
              " 'waste': 318,\n",
              " 'enjoy': 319,\n",
              " 'fairly': 320,\n",
              " 'late': 321,\n",
              " 'explanations': 322,\n",
              " 'else': 323,\n",
              " 'money': 324,\n",
              " 'set': 325,\n",
              " 'lost': 326,\n",
              " 'dry': 327,\n",
              " 'wellorganized': 328,\n",
              " 'concise': 329,\n",
              " 'resource': 330,\n",
              " 'methods': 331,\n",
              " 'easier': 332,\n",
              " 'grader': 333,\n",
              " 'dr': 334,\n",
              " 'emails': 335,\n",
              " 'genuinely': 336,\n",
              " 'unorganized': 337,\n",
              " 'smart': 338,\n",
              " 'development': 339,\n",
              " 'yes': 340,\n",
              " 'is': 341,\n",
              " 'interact': 342,\n",
              " 'listen': 343,\n",
              " 'experienced': 344,\n",
              " 'thanks': 345,\n",
              " 'etc': 346,\n",
              " 'unfair': 347,\n",
              " 'trying': 348,\n",
              " 'slides': 349,\n",
              " 'term': 350,\n",
              " 'increased': 351,\n",
              " 'strict': 352,\n",
              " 'chance': 353,\n",
              " 'low': 354,\n",
              " 'chapters': 355,\n",
              " 'hour': 356,\n",
              " 'explaining': 357,\n",
              " 'seriously': 358,\n",
              " 'fast': 359,\n",
              " 'role': 360,\n",
              " 'participation': 361,\n",
              " 'miss': 362,\n",
              " 'ready': 363,\n",
              " 'passion': 364,\n",
              " 'written': 365,\n",
              " 'board': 366,\n",
              " 'unless': 367,\n",
              " 'lives': 368,\n",
              " 'outdated': 369,\n",
              " 'poorly': 370,\n",
              " 'werent': 371,\n",
              " 'realworld': 372,\n",
              " 'process': 373,\n",
              " 'unhelpful': 374,\n",
              " 'ideas': 375,\n",
              " 'particularly': 376,\n",
              " 'create': 377,\n",
              " 'inappropriate': 378,\n",
              " 'comfortable': 379,\n",
              " 'due': 380,\n",
              " 'multiple': 381,\n",
              " 'exactly': 382,\n",
              " 'wait': 383,\n",
              " 'likes': 384,\n",
              " 'instructions': 385,\n",
              " 'barrow': 386,\n",
              " 'lack': 387,\n",
              " 'quality': 388,\n",
              " 'management': 389,\n",
              " 'events': 390,\n",
              " 'properly': 391,\n",
              " 'effectively': 392,\n",
              " 'appropriate': 393,\n",
              " 'useful': 394,\n",
              " 'dedicated': 395,\n",
              " 'providing': 396,\n",
              " 'gained': 397,\n",
              " 'memorize': 398,\n",
              " 'conducted': 399,\n",
              " 'plus': 400,\n",
              " 'type': 401,\n",
              " 'future': 402,\n",
              " 'usually': 403,\n",
              " 'text': 404,\n",
              " 'incredibly': 405,\n",
              " 'couldnt': 406,\n",
              " 'reviews': 407,\n",
              " 'wouldnt': 408,\n",
              " 'struggling': 409,\n",
              " 'fantastic': 410,\n",
              " 'comprehensive': 411,\n",
              " 'gradesscores': 412,\n",
              " 'professional': 413,\n",
              " 'forward': 414,\n",
              " 'slow': 415,\n",
              " 'assignment': 416,\n",
              " 'solid': 417,\n",
              " 'favorite': 418,\n",
              " 'curves': 419,\n",
              " 'quiz': 420,\n",
              " 'ass': 421,\n",
              " 'studying': 422,\n",
              " 'expect': 423,\n",
              " 'whats': 424,\n",
              " 'watch': 425,\n",
              " 'particular': 426,\n",
              " 'according': 427,\n",
              " 'satisfactory': 428,\n",
              " 'depends': 429,\n",
              " 'staff': 430,\n",
              " 'college': 431,\n",
              " 'india': 432,\n",
              " 'lpu': 433,\n",
              " 'single': 434,\n",
              " 'huge': 435,\n",
              " 'various': 436,\n",
              " 'testing': 437,\n",
              " 'sheet': 438,\n",
              " 'key': 439,\n",
              " 'entire': 440,\n",
              " 'basis': 441,\n",
              " 'support': 442,\n",
              " 'techniques': 443,\n",
              " 'issues': 444,\n",
              " 'words': 445,\n",
              " 'simply': 446,\n",
              " 'platform': 447,\n",
              " 'mind': 448,\n",
              " 'complete': 449,\n",
              " 'participating': 450,\n",
              " 'encourages': 451,\n",
              " 'specific': 452,\n",
              " 'past': 453,\n",
              " 'agree': 454,\n",
              " 'heshe': 455,\n",
              " 'throughout': 456,\n",
              " 'personality': 457,\n",
              " 'challenge': 458,\n",
              " 'adequate': 459,\n",
              " 'patient': 460,\n",
              " 'model': 461,\n",
              " 'inspiring': 462,\n",
              " 'develop': 463,\n",
              " 'break': 464,\n",
              " 'behavior': 465,\n",
              " 'relaxed': 466,\n",
              " 'exact': 467,\n",
              " 'honestly': 468,\n",
              " 'readings': 469,\n",
              " 'language': 470,\n",
              " 'page': 471,\n",
              " 'cute': 472,\n",
              " 'literally': 473,\n",
              " 'trouble': 474,\n",
              " 'quickly': 475,\n",
              " 'chinese': 476,\n",
              " '15': 477,\n",
              " 'spend': 478,\n",
              " 'tells': 479,\n",
              " 'loves': 480,\n",
              " 'previous': 481,\n",
              " 'breaks': 482,\n",
              " 'delivered': 483,\n",
              " 'it': 484,\n",
              " 'upto': 485,\n",
              " 'compared': 486,\n",
              " 'load': 487,\n",
              " 'depending': 488,\n",
              " 'fails': 489,\n",
              " 'not': 490,\n",
              " 'related': 491,\n",
              " 'understands': 492,\n",
              " 'rest': 493,\n",
              " 'partiality': 494,\n",
              " 'south': 495,\n",
              " 'current': 496,\n",
              " 'technology': 497,\n",
              " 'major': 498,\n",
              " 'changed': 499,\n",
              " 'opinion': 500,\n",
              " 'yeah': 501,\n",
              " 'towards': 502,\n",
              " 'ability': 503,\n",
              " 'biased': 504,\n",
              " 'expectations': 505,\n",
              " 'poor': 506,\n",
              " 'examinations': 507,\n",
              " 'mcq': 508,\n",
              " 'hands': 509,\n",
              " 'mid': 510,\n",
              " 'improving': 511,\n",
              " 'please': 512,\n",
              " 'education': 513,\n",
              " 'detail': 514,\n",
              " 'child': 515,\n",
              " 'changes': 516,\n",
              " 'whatever': 517,\n",
              " 'lessons': 518,\n",
              " 'issue': 519,\n",
              " 'period': 520,\n",
              " 'weeks': 521,\n",
              " 'ones': 522,\n",
              " 'idea': 523,\n",
              " 'encouraging': 524,\n",
              " 'confidence': 525,\n",
              " 'consider': 526,\n",
              " 'efforts': 527,\n",
              " 'amount': 528,\n",
              " 'useless': 529,\n",
              " 'figure': 530,\n",
              " 'progress': 531,\n",
              " 'confident': 532,\n",
              " 'insights': 533,\n",
              " 'timeconsuming': 534,\n",
              " 'engage': 535,\n",
              " 'goals': 536,\n",
              " 'visually': 537,\n",
              " 'uninspiring': 538,\n",
              " 'incomplete': 539,\n",
              " 'hated': 540,\n",
              " 'biology': 541,\n",
              " 'critically': 542,\n",
              " 'monotone': 543,\n",
              " 'focused': 544,\n",
              " 'memorable': 545,\n",
              " 'special': 546,\n",
              " 'stressful': 547,\n",
              " 'machine': 548,\n",
              " 'liked': 549,\n",
              " 'stupid': 550,\n",
              " 'answered': 551,\n",
              " 'public': 552,\n",
              " 'speaking': 553,\n",
              " 'timely': 554,\n",
              " 'uncomfortable': 555,\n",
              " 'scared': 556,\n",
              " 'remember': 557,\n",
              " 'offered': 558,\n",
              " 'holds': 559,\n",
              " 'powerpoint': 560,\n",
              " 'costs': 561,\n",
              " 'intimidating': 562,\n",
              " 'days': 563,\n",
              " 'explained': 564,\n",
              " 'formulas': 565,\n",
              " 'thoroughly': 566,\n",
              " 'posts': 567,\n",
              " 'solve': 568,\n",
              " 'repeat': 569,\n",
              " 'jerk': 570,\n",
              " 'passed': 571,\n",
              " 'believe': 572,\n",
              " 'eyes': 573,\n",
              " 'short': 574,\n",
              " '100': 575,\n",
              " 'amy': 576,\n",
              " 'quizes': 577,\n",
              " 'passing': 578,\n",
              " 'thick': 579,\n",
              " 'curve': 580,\n",
              " 'organic': 581,\n",
              " 'immunology': 582,\n",
              " 'microbiology': 583,\n",
              " 'financial': 584,\n",
              " 'appreciated': 585,\n",
              " 'all': 586,\n",
              " 'are': 587,\n",
              " 'academic': 588,\n",
              " 'curriculum': 589,\n",
              " 'managed': 590,\n",
              " 'hisher': 591,\n",
              " 'rating': 592,\n",
              " 'activity': 593,\n",
              " 'north': 594,\n",
              " 'indians': 595,\n",
              " 'friends': 596,\n",
              " 'sports': 597,\n",
              " 'philosophy': 598,\n",
              " 'purpose': 599,\n",
              " 'clarity': 600,\n",
              " 'recommended': 601,\n",
              " 'structure': 602,\n",
              " 'increase': 603,\n",
              " '45': 604,\n",
              " 'instead': 605,\n",
              " 'exposure': 606,\n",
              " 'ca': 607,\n",
              " 'marking': 608,\n",
              " 'correct': 609,\n",
              " 'subjective': 610,\n",
              " 'success': 611,\n",
              " 'plan': 612,\n",
              " 'duration': 613,\n",
              " 'seen': 614,\n",
              " 'sucks': 615,\n",
              " 'mistakes': 616,\n",
              " 'clarify': 617,\n",
              " 'otherwise': 618,\n",
              " 'respected': 619,\n",
              " 'evaluation': 620,\n",
              " 'front': 621,\n",
              " 'software': 622,\n",
              " 'method': 623,\n",
              " 'meet': 624,\n",
              " 'live': 625,\n",
              " 'hate': 626,\n",
              " 'actual': 627,\n",
              " 'soo': 628,\n",
              " 'unprofessional': 629,\n",
              " 'hardly': 630,\n",
              " 'keeping': 631,\n",
              " 'especially': 632,\n",
              " 'till': 633,\n",
              " 'audience': 634,\n",
              " 'offers': 635,\n",
              " 'happy': 636,\n",
              " 'source': 637,\n",
              " 'half': 638,\n",
              " 'attending': 639,\n",
              " 'introduction': 640,\n",
              " 'additional': 641,\n",
              " 'preparing': 642,\n",
              " 'unprepared': 643,\n",
              " 'analyze': 644,\n",
              " 'share': 645,\n",
              " 'motivated': 646,\n",
              " 'reflection': 647,\n",
              " 'plenty': 648,\n",
              " 'name': 649,\n",
              " 'calculus': 650,\n",
              " 'physics': 651,\n",
              " 'thorough': 652,\n",
              " 'understood': 653,\n",
              " 'requires': 654,\n",
              " 'rewarding': 655,\n",
              " 'frustrating': 656,\n",
              " 'results': 657,\n",
              " 'reach': 658,\n",
              " 'concisely': 659,\n",
              " 'lacking': 660,\n",
              " 'critical': 661,\n",
              " 'assessments': 662,\n",
              " 'stress': 663,\n",
              " 'dropping': 664,\n",
              " 'understandable': 665,\n",
              " 'safe': 666,\n",
              " 'offensive': 667,\n",
              " 'setting': 668,\n",
              " 'disabilities': 669,\n",
              " 'deal': 670,\n",
              " 'essay': 671,\n",
              " 'rambles': 672,\n",
              " 'talks': 673,\n",
              " 'cd': 674,\n",
              " 'home': 675,\n",
              " 'call': 676,\n",
              " 'vague': 677,\n",
              " 'fool': 678,\n",
              " 'tried': 679,\n",
              " 'impossible': 680,\n",
              " 'appears': 681,\n",
              " 'po': 682,\n",
              " 'cared': 683,\n",
              " 'received': 684,\n",
              " 'incredible': 685,\n",
              " 'helping': 686,\n",
              " 'center': 687,\n",
              " 'struggle': 688,\n",
              " 'beginning': 689,\n",
              " 'stop': 690,\n",
              " 'finished': 691,\n",
              " 'true': 692,\n",
              " 'word': 693,\n",
              " 'buy': 694,\n",
              " 'soooo': 695,\n",
              " 'pages': 696,\n",
              " 'freshman': 697,\n",
              " 'pointless': 698,\n",
              " 'bring': 699,\n",
              " 'lady': 700,\n",
              " 'attitude': 701,\n",
              " 'fall': 702,\n",
              " 'left': 703,\n",
              " 'relate': 704,\n",
              " 'looked': 705,\n",
              " 'receive': 706,\n",
              " 'attendance': 707,\n",
              " 'bc': 708,\n",
              " 'difficulty': 709,\n",
              " 'detailed': 710,\n",
              " '125': 711,\n",
              " 'semesters': 712,\n",
              " 'equations': 713,\n",
              " 'approach': 714,\n",
              " 'saying': 715,\n",
              " 'confusion': 716,\n",
              " '50': 717,\n",
              " 'video': 718,\n",
              " 'videos': 719,\n",
              " 'posted': 720,\n",
              " 'olds': 721,\n",
              " 'involved': 722,\n",
              " 'theory': 723,\n",
              " 'tutorials': 724,\n",
              " 'ta': 725,\n",
              " 'analysis': 726,\n",
              " 'marker': 727,\n",
              " 'roberts': 728,\n",
              " 'human': 729,\n",
              " 'strategic': 730,\n",
              " 'compare': 731,\n",
              " 'btech': 732,\n",
              " 'entertaining': 733,\n",
              " 'change': 734,\n",
              " 'causes': 735,\n",
              " 'so': 736,\n",
              " 'encourage': 737,\n",
              " 'top': 738,\n",
              " 'speed': 739,\n",
              " 'dull': 740,\n",
              " 'department': 741,\n",
              " 'enough': 742,\n",
              " 'indeed': 743,\n",
              " 'studied': 744,\n",
              " 'regular': 745,\n",
              " 'improvement': 746,\n",
              " 'knowledgable': 747,\n",
              " 'includes': 748,\n",
              " 'details': 749,\n",
              " 'hand': 750,\n",
              " 'vast': 751,\n",
              " 'ample': 752,\n",
              " 'oriented': 753,\n",
              " 'advanced': 754,\n",
              " 'view': 755,\n",
              " '1st': 756,\n",
              " 'covering': 757,\n",
              " 'included': 758,\n",
              " 'schedule': 759,\n",
              " 'basic': 760,\n",
              " 'situations': 761,\n",
              " 'maximum': 762,\n",
              " 'great': 763,\n",
              " 'procedure': 764,\n",
              " 'tight': 765,\n",
              " 'frustrated': 766,\n",
              " 'including': 767,\n",
              " 'maths': 768,\n",
              " 'objective': 769,\n",
              " 'mistake': 770,\n",
              " 'much': 771,\n",
              " '55': 772,\n",
              " 'best': 773,\n",
              " 'suggest': 774,\n",
              " 'disappointing': 775,\n",
              " 'screwed': 776,\n",
              " 'weird': 777,\n",
              " 'file': 778,\n",
              " 'equipments': 779,\n",
              " 'save': 780,\n",
              " 'data': 781,\n",
              " 'supposed': 782,\n",
              " 'evaluated': 783,\n",
              " 'rules': 784,\n",
              " 'atmosphere': 785,\n",
              " 'copies': 786,\n",
              " 'issuing': 787,\n",
              " 'limit': 788,\n",
              " 'size': 789,\n",
              " 'heavily': 790,\n",
              " 'following': 791,\n",
              " 'physical': 792,\n",
              " 'views': 793,\n",
              " 'enjoys': 794,\n",
              " 'talent': 795,\n",
              " 'curricular': 796,\n",
              " 'whenever': 797,\n",
              " 'unique': 798,\n",
              " 'matters': 799,\n",
              " 'worse': 800,\n",
              " 'presentations': 801,\n",
              " 'telling': 802,\n",
              " 'beyond': 803,\n",
              " 'wrote': 804,\n",
              " 'iclicker': 805,\n",
              " 'lower': 806,\n",
              " 'mention': 807,\n",
              " 'profs': 808,\n",
              " 'angry': 809,\n",
              " 'midterms': 810,\n",
              " 'condescending': 811,\n",
              " 'applied': 812,\n",
              " 'expensive': 813,\n",
              " 'value': 814,\n",
              " 'affordable': 815,\n",
              " 'hope': 816,\n",
              " 'classmates': 817,\n",
              " 'speaker': 818,\n",
              " 'handson': 819,\n",
              " 'allowed': 820,\n",
              " 'encouraged': 821,\n",
              " 'ensure': 822,\n",
              " 'voice': 823,\n",
              " 'allow': 824,\n",
              " 'busy': 825,\n",
              " 'appealing': 826,\n",
              " 'functional': 827,\n",
              " 'consistent': 828,\n",
              " 'criteria': 829,\n",
              " 'transparent': 830,\n",
              " 'rewarded': 831,\n",
              " 'literature': 832,\n",
              " 'stimulating': 833,\n",
              " 'challenged': 834,\n",
              " 'form': 835,\n",
              " 'asleep': 836,\n",
              " 'demanding': 837,\n",
              " 'visuals': 838,\n",
              " 'audiences': 839,\n",
              " 'problemsolving': 840,\n",
              " 'overwhelming': 841,\n",
              " 'distraction': 842,\n",
              " 'disrespectful': 843,\n",
              " 'brilliant': 844,\n",
              " 'mile': 845,\n",
              " 'listener': 846,\n",
              " 'styles': 847,\n",
              " 'lighten': 848,\n",
              " 'mood': 849,\n",
              " 'control': 850,\n",
              " 'aligned': 851,\n",
              " 'track': 852,\n",
              " 'format': 853,\n",
              " 'options': 854,\n",
              " 'frequently': 855,\n",
              " 'gotten': 856,\n",
              " 'program': 857,\n",
              " 'print': 858,\n",
              " 'grudges': 859,\n",
              " 'told': 860,\n",
              " 'awful': 861,\n",
              " 'god': 862,\n",
              " 'dept': 863,\n",
              " 'formula': 864,\n",
              " 'response': 865,\n",
              " 'ethics': 866,\n",
              " 'articles': 867,\n",
              " 'nlandu': 868,\n",
              " 'skip': 869,\n",
              " 'luck': 870,\n",
              " 'culture': 871,\n",
              " 'move': 872,\n",
              " 'italian': 873,\n",
              " 'calls': 874,\n",
              " 'favorites': 875,\n",
              " 'replies': 876,\n",
              " 'insightful': 877,\n",
              " 'assigns': 878,\n",
              " 'email': 879,\n",
              " 'straightforward': 880,\n",
              " 'exciting': 881,\n",
              " 'youve': 882,\n",
              " 'sit': 883,\n",
              " 'close': 884,\n",
              " 'explanation': 885,\n",
              " 'ko': 886,\n",
              " 'art': 887,\n",
              " 'maybe': 888,\n",
              " 'mandatory': 889,\n",
              " 'obayani': 890,\n",
              " 'original': 891,\n",
              " 'decided': 892,\n",
              " 'hardest': 893,\n",
              " 'self': 894,\n",
              " 'speak': 895,\n",
              " 'advise': 896,\n",
              " 'total': 897,\n",
              " 'session': 898,\n",
              " 'failed': 899,\n",
              " 'woman': 900,\n",
              " 'crazy': 901,\n",
              " 'cuz': 902,\n",
              " 'admit': 903,\n",
              " 'id': 904,\n",
              " 'treat': 905,\n",
              " 'lucky': 906,\n",
              " 'instruction': 907,\n",
              " 'power': 908,\n",
              " 'smile': 909,\n",
              " 'theres': 910,\n",
              " 'require': 911,\n",
              " 'workload': 912,\n",
              " 'manageable': 913,\n",
              " 'allows': 914,\n",
              " 'pop': 915,\n",
              " 'lowest': 916,\n",
              " 'score': 917,\n",
              " 'solutions': 918,\n",
              " 'practices': 919,\n",
              " 'ramble': 920,\n",
              " 'laid': 921,\n",
              " 'tends': 922,\n",
              " 'sets': 923,\n",
              " 'minutes': 924,\n",
              " 'kinda': 925,\n",
              " 'award': 926,\n",
              " '113': 927,\n",
              " '25': 928,\n",
              " 'communicate': 929,\n",
              " 'dropped': 930,\n",
              " 'solving': 931,\n",
              " 'bs': 932,\n",
              " 'homeworks': 933,\n",
              " 'tutor': 934,\n",
              " 'night': 935,\n",
              " 'tho': 936,\n",
              " 'russian': 937,\n",
              " 'lazy': 938,\n",
              " '0': 939,\n",
              " 'counted': 940,\n",
              " 'hueston': 941,\n",
              " 'covers': 942,\n",
              " 'code': 943,\n",
              " 'directions': 944,\n",
              " 'graduate': 945,\n",
              " 'summary': 946,\n",
              " 'internet': 947,\n",
              " 'paycheck': 948,\n",
              " 'chem': 949,\n",
              " 'example': 950,\n",
              " 'recomend': 951,\n",
              " 'barrier': 952,\n",
              " 'active': 953,\n",
              " 'mcat': 954,\n",
              " 'upb': 955,\n",
              " 'thank': 956,\n",
              " 'campus': 957,\n",
              " 'challenges': 958,\n",
              " 'straight': 959,\n",
              " 'contact': 960,\n",
              " 'shell': 961,\n",
              " 'economics': 962,\n",
              " 'theories': 963,\n",
              " 'initially': 964,\n",
              " 'third': 965,\n",
              " 'strange': 966,\n",
              " 'near': 967,\n",
              " 'forever': 968,\n",
              " 'quick': 969,\n",
              " 'wellstructured': 970,\n",
              " 'accounting': 971,\n",
              " 'application': 972,\n",
              " 'marketing': 973,\n",
              " 'dynamics': 974,\n",
              " 'operations': 975,\n",
              " 'performance': 976,\n",
              " 'leaving': 977,\n",
              " 'delivering': 978,\n",
              " 'descent': 979,\n",
              " 'interacting': 980,\n",
              " 'accurate': 981,\n",
              " 'months': 982,\n",
              " 'served': 983,\n",
              " 'maintained': 984,\n",
              " 'qualified': 985,\n",
              " 'lacks': 986,\n",
              " 'very': 987,\n",
              " 'its': 988,\n",
              " 'everything': 989,\n",
              " 'grip': 990,\n",
              " 'behave': 991,\n",
              " 'normal': 992,\n",
              " 'remaining': 993,\n",
              " 'equal': 994,\n",
              " 'fields': 995,\n",
              " 'reduce': 996,\n",
              " 'cse': 997,\n",
              " 'faculties': 998,\n",
              " 'well': 999,\n",
              " 'extraordinary': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 87
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stemming & Tokenizing"
      ],
      "metadata": {
        "id": "CMJJNPr0br5W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.tokenize import word_tokenize\n",
        "\n",
        "nltk.download('punkt')\n",
        "ps = PorterStemmer()\n",
        " \n",
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "words = tokenizer.word_index.keys()\n",
        "\n",
        "stemmed_words = [ps.stem(word) for word in words]"
      ],
      "metadata": {
        "id": "TPp4ETZLd5n8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "af869c2a-1f6c-4121-88e1-a49f3a53a173"
      },
      "execution_count": 88,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "XBmlw1GS6fxy"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "z0ohJpED7C4K"
      },
      "execution_count": 89,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modeling"
      ],
      "metadata": {
        "id": "nX5I8voe9Dmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the model architecture\n",
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n",
        "    # tf.keras.layers.GlobalAveragePooling1D(),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(max_length, return_sequences=True)),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n",
        "    tf.keras.layers.Flatten(),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(3, activation='softmax')\n",
        "])"
      ],
      "metadata": {
        "id": "uyFDoCWr9Evs"
      },
      "execution_count": 90,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compile the model\n",
        "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "metadata": {
        "id": "ikx2134r9HJZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3a94eedc-047b-4adf-fac0-fdcdf905003e"
      },
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 64, 32)            32000     \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 64, 128)          49664     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 64)               41216     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " flatten_4 (Flatten)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_8 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dense_9 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 127,235\n",
            "Trainable params: 127,235\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras.utils import to_categorical\n",
        "\n",
        "training_labels_final = to_categorical(training_labels)\n",
        "testing_labels_final = to_categorical(testing_labels)\n",
        "\n",
        "print(training_labels_final)"
      ],
      "metadata": {
        "id": "Bi_xhDjL9JlK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2c2d64a9-9323-4a82-d89f-17beee870970"
      },
      "execution_count": 92,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " ...\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 100\n",
        "history = model.fit(padded, training_labels_final, epochs=num_epochs, validation_data=(testing_padded, testing_labels_final))"
      ],
      "metadata": {
        "id": "lrAOQga-9LdS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e7fd4731-1b00-4736-a71e-d80618a2a808"
      },
      "execution_count": 93,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/100\n",
            "57/57 [==============================] - 20s 177ms/step - loss: 1.0450 - accuracy: 0.4901 - val_loss: 0.9744 - val_accuracy: 0.5579\n",
            "Epoch 2/100\n",
            "57/57 [==============================] - 9s 159ms/step - loss: 0.9213 - accuracy: 0.5558 - val_loss: 0.9660 - val_accuracy: 0.5579\n",
            "Epoch 3/100\n",
            "57/57 [==============================] - 3s 54ms/step - loss: 0.7083 - accuracy: 0.6790 - val_loss: 0.8693 - val_accuracy: 0.6421\n",
            "Epoch 4/100\n",
            "57/57 [==============================] - 2s 28ms/step - loss: 0.6098 - accuracy: 0.7381 - val_loss: 1.0987 - val_accuracy: 0.6421\n",
            "Epoch 5/100\n",
            "57/57 [==============================] - 3s 48ms/step - loss: 0.4880 - accuracy: 0.8171 - val_loss: 1.1397 - val_accuracy: 0.6421\n",
            "Epoch 6/100\n",
            "57/57 [==============================] - 2s 40ms/step - loss: 0.4063 - accuracy: 0.8525 - val_loss: 1.2712 - val_accuracy: 0.6316\n",
            "Epoch 7/100\n",
            "57/57 [==============================] - 1s 23ms/step - loss: 0.3493 - accuracy: 0.8740 - val_loss: 1.2700 - val_accuracy: 0.6632\n",
            "Epoch 8/100\n",
            "57/57 [==============================] - 2s 34ms/step - loss: 0.3047 - accuracy: 0.8901 - val_loss: 1.3034 - val_accuracy: 0.6737\n",
            "Epoch 9/100\n",
            "57/57 [==============================] - 1s 23ms/step - loss: 0.2802 - accuracy: 0.8983 - val_loss: 1.4961 - val_accuracy: 0.6105\n",
            "Epoch 10/100\n",
            "57/57 [==============================] - 1s 26ms/step - loss: 0.2618 - accuracy: 0.9011 - val_loss: 1.5294 - val_accuracy: 0.6421\n",
            "Epoch 11/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.2473 - accuracy: 0.9105 - val_loss: 1.7363 - val_accuracy: 0.6421\n",
            "Epoch 12/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.2344 - accuracy: 0.9182 - val_loss: 1.7115 - val_accuracy: 0.6421\n",
            "Epoch 13/100\n",
            "57/57 [==============================] - 1s 25ms/step - loss: 0.2508 - accuracy: 0.9072 - val_loss: 1.4346 - val_accuracy: 0.6211\n",
            "Epoch 14/100\n",
            "57/57 [==============================] - 2s 31ms/step - loss: 0.2187 - accuracy: 0.9188 - val_loss: 1.6417 - val_accuracy: 0.5895\n",
            "Epoch 15/100\n",
            "57/57 [==============================] - 2s 34ms/step - loss: 0.2058 - accuracy: 0.9254 - val_loss: 1.7371 - val_accuracy: 0.6211\n",
            "Epoch 16/100\n",
            "57/57 [==============================] - 1s 25ms/step - loss: 0.2018 - accuracy: 0.9221 - val_loss: 1.8045 - val_accuracy: 0.5895\n",
            "Epoch 17/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.2174 - accuracy: 0.9199 - val_loss: 1.9662 - val_accuracy: 0.5789\n",
            "Epoch 18/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.1952 - accuracy: 0.9282 - val_loss: 1.9726 - val_accuracy: 0.5895\n",
            "Epoch 19/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.1859 - accuracy: 0.9287 - val_loss: 2.1283 - val_accuracy: 0.5474\n",
            "Epoch 20/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1708 - accuracy: 0.9348 - val_loss: 2.1827 - val_accuracy: 0.5579\n",
            "Epoch 21/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1872 - accuracy: 0.9326 - val_loss: 1.6460 - val_accuracy: 0.5895\n",
            "Epoch 22/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.1777 - accuracy: 0.9331 - val_loss: 2.1309 - val_accuracy: 0.5474\n",
            "Epoch 23/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.1664 - accuracy: 0.9387 - val_loss: 2.3910 - val_accuracy: 0.5789\n",
            "Epoch 24/100\n",
            "57/57 [==============================] - 1s 25ms/step - loss: 0.1791 - accuracy: 0.9298 - val_loss: 2.0067 - val_accuracy: 0.5895\n",
            "Epoch 25/100\n",
            "57/57 [==============================] - 1s 23ms/step - loss: 0.1602 - accuracy: 0.9337 - val_loss: 2.2817 - val_accuracy: 0.5579\n",
            "Epoch 26/100\n",
            "57/57 [==============================] - 2s 28ms/step - loss: 0.1503 - accuracy: 0.9381 - val_loss: 2.4373 - val_accuracy: 0.5579\n",
            "Epoch 27/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.1488 - accuracy: 0.9392 - val_loss: 2.3857 - val_accuracy: 0.5684\n",
            "Epoch 28/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.1552 - accuracy: 0.9387 - val_loss: 2.4404 - val_accuracy: 0.5579\n",
            "Epoch 29/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1576 - accuracy: 0.9414 - val_loss: 2.1383 - val_accuracy: 0.5684\n",
            "Epoch 30/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1632 - accuracy: 0.9365 - val_loss: 2.4456 - val_accuracy: 0.5789\n",
            "Epoch 31/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.1543 - accuracy: 0.9403 - val_loss: 2.2936 - val_accuracy: 0.5684\n",
            "Epoch 32/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1583 - accuracy: 0.9403 - val_loss: 2.5965 - val_accuracy: 0.5579\n",
            "Epoch 33/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1701 - accuracy: 0.9348 - val_loss: 2.4670 - val_accuracy: 0.5474\n",
            "Epoch 34/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1436 - accuracy: 0.9420 - val_loss: 2.7533 - val_accuracy: 0.5474\n",
            "Epoch 35/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1315 - accuracy: 0.9442 - val_loss: 2.7794 - val_accuracy: 0.5474\n",
            "Epoch 36/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1280 - accuracy: 0.9492 - val_loss: 2.8587 - val_accuracy: 0.5579\n",
            "Epoch 37/100\n",
            "57/57 [==============================] - 2s 28ms/step - loss: 0.1383 - accuracy: 0.9459 - val_loss: 2.9735 - val_accuracy: 0.5368\n",
            "Epoch 38/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.1395 - accuracy: 0.9442 - val_loss: 2.8251 - val_accuracy: 0.5263\n",
            "Epoch 39/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.1452 - accuracy: 0.9442 - val_loss: 2.9821 - val_accuracy: 0.5684\n",
            "Epoch 40/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.1274 - accuracy: 0.9459 - val_loss: 2.7749 - val_accuracy: 0.5579\n",
            "Epoch 41/100\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 0.1183 - accuracy: 0.9530 - val_loss: 2.9702 - val_accuracy: 0.5684\n",
            "Epoch 42/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1108 - accuracy: 0.9530 - val_loss: 2.9789 - val_accuracy: 0.5579\n",
            "Epoch 43/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.1317 - accuracy: 0.9453 - val_loss: 3.0192 - val_accuracy: 0.5684\n",
            "Epoch 44/100\n",
            "57/57 [==============================] - 1s 23ms/step - loss: 0.1344 - accuracy: 0.9436 - val_loss: 2.7493 - val_accuracy: 0.5579\n",
            "Epoch 45/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1572 - accuracy: 0.9409 - val_loss: 2.3087 - val_accuracy: 0.5684\n",
            "Epoch 46/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.1431 - accuracy: 0.9387 - val_loss: 2.7091 - val_accuracy: 0.5474\n",
            "Epoch 47/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.1321 - accuracy: 0.9442 - val_loss: 2.7517 - val_accuracy: 0.5579\n",
            "Epoch 48/100\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 0.1161 - accuracy: 0.9497 - val_loss: 2.9927 - val_accuracy: 0.5579\n",
            "Epoch 49/100\n",
            "57/57 [==============================] - 1s 23ms/step - loss: 0.1067 - accuracy: 0.9519 - val_loss: 3.0518 - val_accuracy: 0.5474\n",
            "Epoch 50/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.1026 - accuracy: 0.9552 - val_loss: 3.1135 - val_accuracy: 0.5579\n",
            "Epoch 51/100\n",
            "57/57 [==============================] - 3s 46ms/step - loss: 0.0985 - accuracy: 0.9547 - val_loss: 3.1815 - val_accuracy: 0.5579\n",
            "Epoch 52/100\n",
            "57/57 [==============================] - 2s 34ms/step - loss: 0.1100 - accuracy: 0.9519 - val_loss: 3.1671 - val_accuracy: 0.5368\n",
            "Epoch 53/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1032 - accuracy: 0.9541 - val_loss: 3.2805 - val_accuracy: 0.5368\n",
            "Epoch 54/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0970 - accuracy: 0.9608 - val_loss: 3.3886 - val_accuracy: 0.5368\n",
            "Epoch 55/100\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 0.0916 - accuracy: 0.9608 - val_loss: 3.4264 - val_accuracy: 0.5368\n",
            "Epoch 56/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0916 - accuracy: 0.9602 - val_loss: 3.5701 - val_accuracy: 0.5579\n",
            "Epoch 57/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0924 - accuracy: 0.9591 - val_loss: 3.2856 - val_accuracy: 0.5474\n",
            "Epoch 58/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0984 - accuracy: 0.9541 - val_loss: 3.4734 - val_accuracy: 0.5474\n",
            "Epoch 59/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0922 - accuracy: 0.9602 - val_loss: 3.7301 - val_accuracy: 0.5263\n",
            "Epoch 60/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1107 - accuracy: 0.9552 - val_loss: 3.4580 - val_accuracy: 0.5368\n",
            "Epoch 61/100\n",
            "57/57 [==============================] - 1s 21ms/step - loss: 0.0947 - accuracy: 0.9580 - val_loss: 3.6169 - val_accuracy: 0.5368\n",
            "Epoch 62/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0887 - accuracy: 0.9575 - val_loss: 3.3906 - val_accuracy: 0.5053\n",
            "Epoch 63/100\n",
            "57/57 [==============================] - 1s 26ms/step - loss: 0.1816 - accuracy: 0.9265 - val_loss: 2.8061 - val_accuracy: 0.5474\n",
            "Epoch 64/100\n",
            "57/57 [==============================] - 1s 14ms/step - loss: 0.1139 - accuracy: 0.9486 - val_loss: 3.3366 - val_accuracy: 0.5684\n",
            "Epoch 65/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1100 - accuracy: 0.9536 - val_loss: 3.0188 - val_accuracy: 0.5474\n",
            "Epoch 66/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1038 - accuracy: 0.9580 - val_loss: 3.1760 - val_accuracy: 0.5263\n",
            "Epoch 67/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.1091 - accuracy: 0.9536 - val_loss: 3.1228 - val_accuracy: 0.5684\n",
            "Epoch 68/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0803 - accuracy: 0.9635 - val_loss: 3.3333 - val_accuracy: 0.5368\n",
            "Epoch 69/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0796 - accuracy: 0.9641 - val_loss: 3.5056 - val_accuracy: 0.5474\n",
            "Epoch 70/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0747 - accuracy: 0.9641 - val_loss: 3.5055 - val_accuracy: 0.5474\n",
            "Epoch 71/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0711 - accuracy: 0.9608 - val_loss: 3.6822 - val_accuracy: 0.5368\n",
            "Epoch 72/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0755 - accuracy: 0.9602 - val_loss: 3.7715 - val_accuracy: 0.5579\n",
            "Epoch 73/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0685 - accuracy: 0.9669 - val_loss: 3.9605 - val_accuracy: 0.5684\n",
            "Epoch 74/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.0760 - accuracy: 0.9652 - val_loss: 3.8613 - val_accuracy: 0.5684\n",
            "Epoch 75/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0829 - accuracy: 0.9624 - val_loss: 3.8295 - val_accuracy: 0.5684\n",
            "Epoch 76/100\n",
            "57/57 [==============================] - 2s 27ms/step - loss: 0.0735 - accuracy: 0.9669 - val_loss: 3.9269 - val_accuracy: 0.5579\n",
            "Epoch 77/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.0691 - accuracy: 0.9680 - val_loss: 4.0273 - val_accuracy: 0.5684\n",
            "Epoch 78/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0678 - accuracy: 0.9696 - val_loss: 4.0413 - val_accuracy: 0.5474\n",
            "Epoch 79/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0626 - accuracy: 0.9674 - val_loss: 4.1339 - val_accuracy: 0.5474\n",
            "Epoch 80/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0667 - accuracy: 0.9663 - val_loss: 4.2648 - val_accuracy: 0.5474\n",
            "Epoch 81/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.0811 - accuracy: 0.9630 - val_loss: 4.1255 - val_accuracy: 0.5789\n",
            "Epoch 82/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0912 - accuracy: 0.9602 - val_loss: 4.0892 - val_accuracy: 0.5474\n",
            "Epoch 83/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0992 - accuracy: 0.9575 - val_loss: 3.8753 - val_accuracy: 0.5263\n",
            "Epoch 84/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0903 - accuracy: 0.9575 - val_loss: 3.8020 - val_accuracy: 0.5474\n",
            "Epoch 85/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0740 - accuracy: 0.9680 - val_loss: 3.8294 - val_accuracy: 0.5263\n",
            "Epoch 86/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0651 - accuracy: 0.9674 - val_loss: 3.8476 - val_accuracy: 0.5474\n",
            "Epoch 87/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0633 - accuracy: 0.9669 - val_loss: 4.0622 - val_accuracy: 0.5158\n",
            "Epoch 88/100\n",
            "57/57 [==============================] - 1s 19ms/step - loss: 0.0625 - accuracy: 0.9674 - val_loss: 4.1459 - val_accuracy: 0.5263\n",
            "Epoch 89/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0572 - accuracy: 0.9685 - val_loss: 4.2513 - val_accuracy: 0.5368\n",
            "Epoch 90/100\n",
            "57/57 [==============================] - 1s 22ms/step - loss: 0.0604 - accuracy: 0.9680 - val_loss: 4.2748 - val_accuracy: 0.5474\n",
            "Epoch 91/100\n",
            "57/57 [==============================] - 1s 18ms/step - loss: 0.0587 - accuracy: 0.9685 - val_loss: 4.3650 - val_accuracy: 0.5053\n",
            "Epoch 92/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.0569 - accuracy: 0.9702 - val_loss: 4.5076 - val_accuracy: 0.5368\n",
            "Epoch 93/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.0666 - accuracy: 0.9646 - val_loss: 3.8900 - val_accuracy: 0.5789\n",
            "Epoch 94/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0694 - accuracy: 0.9641 - val_loss: 4.2771 - val_accuracy: 0.5368\n",
            "Epoch 95/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0642 - accuracy: 0.9674 - val_loss: 3.9907 - val_accuracy: 0.5579\n",
            "Epoch 96/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0580 - accuracy: 0.9669 - val_loss: 4.4223 - val_accuracy: 0.5368\n",
            "Epoch 97/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0578 - accuracy: 0.9696 - val_loss: 4.7462 - val_accuracy: 0.5263\n",
            "Epoch 98/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0567 - accuracy: 0.9674 - val_loss: 4.8438 - val_accuracy: 0.5053\n",
            "Epoch 99/100\n",
            "57/57 [==============================] - 1s 15ms/step - loss: 0.0537 - accuracy: 0.9680 - val_loss: 4.7873 - val_accuracy: 0.5263\n",
            "Epoch 100/100\n",
            "57/57 [==============================] - 1s 16ms/step - loss: 0.0519 - accuracy: 0.9696 - val_loss: 4.7467 - val_accuracy: 0.5263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Bert Model"
      ],
      "metadata": {
        "id": "m7jeYhgtDTlM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install transformers"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "30a-r13uD1cb",
        "outputId": "a9800216-85ab-407c-cbed-a9b6dafe89b8"
      },
      "execution_count": 94,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.29.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Requirement already satisfied: tokenizers!=0.11.3,<0.14,>=0.11.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.13.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.5.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tensorflow as tf\n",
        "# from transformers import TFBertModel, BertTokenizer\n",
        "\n",
        "# # Load pre-trained BERT model and tokenizer\n",
        "# bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# # Example data\n",
        "# split = round(len(df)*0.8)\n",
        "# train_reviews = df['reviews'][:split]\n",
        "# train_label = df['sentiment'][:split]\n",
        "# test_reviews = df['reviews'][split:]\n",
        "# test_label = df['sentiment'][split:]\n",
        "\n",
        "# # Set the maximum sequence length\n",
        "# max_length = 128\n",
        "\n",
        "# # Tokenize and encode the training sentences\n",
        "# train_input_ids = tokenizer.batch_encode_plus(\n",
        "#     train_reviews.tolist(),\n",
        "#     padding='max_length',\n",
        "#     truncation=True,\n",
        "#     max_length=max_length,\n",
        "#     return_tensors='tf'\n",
        "# )\n",
        "\n",
        "# # Tokenize and encode the testing sentences\n",
        "# test_input_ids = tokenizer.batch_encode_plus(\n",
        "#     test_reviews.tolist(),\n",
        "#     padding='max_length',\n",
        "#     truncation=True,\n",
        "#     max_length=max_length,\n",
        "#     return_tensors='tf'\n",
        "# )\n",
        "\n",
        "# # Convert input data to TensorFlow datasets\n",
        "# train_dataset = tf.data.Dataset.from_tensor_slices((train_input_ids['input_ids'], train_label))\n",
        "# test_dataset = tf.data.Dataset.from_tensor_slices((test_input_ids['input_ids'], test_label))\n",
        "\n",
        "# # Define the BERT model architecture\n",
        "# input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "# bert_output = bert_model(input_ids)[0]\n",
        "# flatten = tf.keras.layers.Flatten()(bert_output)\n",
        "# dense = tf.keras.layers.Dense(32, activation='relu')(flatten)\n",
        "# output = tf.keras.layers.Dense(3, activation='softmax')(dense)\n",
        "# model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n",
        "\n",
        "# # Compile the model\n",
        "# model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# # Print the model summary\n",
        "# model.summary()\n",
        "\n",
        "# # Train the model\n",
        "# model.fit(train_dataset.batch(32), epochs=10)\n",
        "\n",
        "# # Evaluate the model on testing data\n",
        "# model.evaluate(test_dataset.batch(32))\n",
        "\n",
        "# # Perform any additional steps such as calculating accuracy or using the predictions for further analysis\n"
      ],
      "metadata": {
        "id": "19Ks_FivDVSa"
      },
      "execution_count": 95,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "from transformers import TFBertModel, BertTokenizer\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# Load pre-trained BERT model and tokenizer\n",
        "bert_model = TFBertModel.from_pretrained('bert-base-uncased')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "\n",
        "# Example data\n",
        "split = round(len(df)*0.8)\n",
        "train_reviews = df['reviews'][:split]\n",
        "train_label = df['sentiment'][:split]\n",
        "test_reviews = df['reviews'][split:]\n",
        "test_label = df['sentiment'][split:]\n",
        "\n",
        "# Set the maximum sequence length\n",
        "max_length = 128\n",
        "\n",
        "# Create a stemmer\n",
        "stemmer = PorterStemmer()\n",
        "\n",
        "# Tokenize, stem, and encode the training sentences\n",
        "train_input_ids = tokenizer.batch_encode_plus(\n",
        "    [stemmer.stem(review) for review in train_reviews.tolist()],\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# Tokenize, stem, and encode the testing sentences\n",
        "test_input_ids = tokenizer.batch_encode_plus(\n",
        "    [stemmer.stem(review) for review in test_reviews.tolist()],\n",
        "    padding='max_length',\n",
        "    truncation=True,\n",
        "    max_length=max_length,\n",
        "    return_tensors='tf'\n",
        ")\n",
        "\n",
        "# Convert input data to TensorFlow datasets\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((train_input_ids['input_ids'], train_label))\n",
        "test_dataset = tf.data.Dataset.from_tensor_slices((test_input_ids['input_ids'], test_label))\n",
        "\n",
        "# Define the BERT model architecture\n",
        "input_ids = tf.keras.layers.Input(shape=(128,), dtype=tf.int32)\n",
        "bert_output = bert_model(input_ids)[0]\n",
        "flatten = tf.keras.layers.Flatten()(bert_output)\n",
        "dense = tf.keras.layers.Dense(32, activation='relu')(flatten)\n",
        "output = tf.keras.layers.Dense(3, activation='softmax')(dense)\n",
        "model = tf.keras.models.Model(inputs=input_ids, outputs=output)\n",
        "\n",
        "# Compile the model\n",
        "model.compile(optimizer='rmsprop', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Print the model summary\n",
        "model.summary()\n",
        "\n",
        "# Train the model\n",
        "model.fit(train_dataset.batch(32), epochs=10)\n",
        "\n",
        "# Evaluate the model on testing data\n",
        "model.evaluate(test_dataset.batch(32))\n",
        "\n",
        "# Perform any additional steps such as calculating accuracy or using the predictions for further analysis\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "49VbLjsNDz4V",
        "outputId": "09a9f557-de74-47aa-ac65-42c04fc14ace"
      },
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some layers from the model checkpoint at bert-base-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls']\n",
            "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "All the layers of TFBertModel were initialized from the model checkpoint at bert-base-uncased.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use TFBertModel for predictions without further training.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"model_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_4 (InputLayer)        [(None, 128)]             0         \n",
            "                                                                 \n",
            " tf_bert_model_3 (TFBertMode  TFBaseModelOutputWithPoo  109482240\n",
            " l)                          lingAndCrossAttentions(l            \n",
            "                             ast_hidden_state=(None,             \n",
            "                             128, 768),                          \n",
            "                              pooler_output=(None, 76            \n",
            "                             8),                                 \n",
            "                              past_key_values=None, h            \n",
            "                             idden_states=None, atten            \n",
            "                             tions=None, cross_attent            \n",
            "                             ions=None)                          \n",
            "                                                                 \n",
            " flatten_5 (Flatten)         (None, 98304)             0         \n",
            "                                                                 \n",
            " dense_10 (Dense)            (None, 32)                3145760   \n",
            "                                                                 \n",
            " dense_11 (Dense)            (None, 3)                 99        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 112,628,099\n",
            "Trainable params: 112,628,099\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "Epoch 1/10\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n",
            "WARNING:tensorflow:Gradients do not exist for variables ['tf_bert_model_3/bert/pooler/dense/kernel:0', 'tf_bert_model_3/bert/pooler/dense/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss` argument?\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "48/48 [==============================] - 93s 770ms/step - loss: 15.3649 - accuracy: 0.4035\n",
            "Epoch 2/10\n",
            "48/48 [==============================] - 35s 735ms/step - loss: 1.0923 - accuracy: 0.4606\n",
            "Epoch 3/10\n",
            "48/48 [==============================] - 35s 734ms/step - loss: 1.0879 - accuracy: 0.4606\n",
            "Epoch 4/10\n",
            "48/48 [==============================] - 36s 741ms/step - loss: 1.0840 - accuracy: 0.4606\n",
            "Epoch 5/10\n",
            "48/48 [==============================] - 35s 737ms/step - loss: 1.0806 - accuracy: 0.4606\n",
            "Epoch 6/10\n",
            "48/48 [==============================] - 35s 738ms/step - loss: 1.0777 - accuracy: 0.4606\n",
            "Epoch 7/10\n",
            "48/48 [==============================] - 36s 741ms/step - loss: 1.0752 - accuracy: 0.4606\n",
            "Epoch 8/10\n",
            "48/48 [==============================] - 35s 735ms/step - loss: 1.0731 - accuracy: 0.4606\n",
            "Epoch 9/10\n",
            "48/48 [==============================] - 36s 740ms/step - loss: 1.0712 - accuracy: 0.4606\n",
            "Epoch 10/10\n",
            "48/48 [==============================] - 35s 736ms/step - loss: 1.0696 - accuracy: 0.4606\n",
            "12/12 [==============================] - 7s 263ms/step - loss: 1.0125 - accuracy: 0.6457\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[1.012540340423584, 0.6456692814826965]"
            ]
          },
          "metadata": {},
          "execution_count": 96
        }
      ]
    }
  ]
}