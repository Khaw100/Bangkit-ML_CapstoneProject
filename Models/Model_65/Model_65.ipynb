{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import linregress"
      ],
      "metadata": {
        "id": "_qfzq2wDZmkx"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ReviewsEN.csv')"
      ],
      "metadata": {
        "id": "Vd4os3NhZp5l"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-1 :\", df['sentiment'].value_counts()[-1])\n",
        "print(\"0 :\", df['sentiment'].value_counts()[0])\n",
        "print(\"1 :\", df['sentiment'].value_counts()[1])"
      ],
      "metadata": {
        "id": "7uQq0VQTZ9iF",
        "outputId": "70cdf4ae-e6aa-4072-88ad-fe8eed09d943",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 : 870\n",
            "0 : 639\n",
            "1 : 1518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace values in pandas DataFrame.\n",
        "df['sentiment'] = df['sentiment'].replace([1], 2)\n",
        "df['sentiment'] = df['sentiment'].replace([0], 1)\n",
        "df['sentiment'] = df['sentiment'].replace([-1], 0)"
      ],
      "metadata": {
        "id": "DsSXQdDXZ-0R"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply lower function\n",
        "df['reviews'] = df['reviews'].apply(str.lower)\n",
        "print(df)"
      ],
      "metadata": {
        "id": "RayqsORYaBXV",
        "outputId": "6014a6aa-06ac-4231-e163-69fe78a73ce9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "      sentiment                                            reviews\n",
            "0             1  teacher are punctual but they should also give...\n",
            "1             2                                               good\n",
            "2             2  excellent lectures are delivered by teachers a...\n",
            "3             2  teachers give us all the information required ...\n",
            "4             2                                                yes\n",
            "...         ...                                                ...\n",
            "3022          2        lecturers provide clear enough explanations\n",
            "3023          2            lecturer's assessment is very objective\n",
            "3024          0              lecturers give very good explanations\n",
            "3025          0        lecturers often provoke discussion in class\n",
            "3026          2         lecturers provide material in a boring way\n",
            "\n",
            "[3027 rows x 2 columns]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "MAXLEN = 16\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "MAX_EXAMPLES = 3000\n",
        "TRAINING_SPLIT = 0.9"
      ],
      "metadata": {
        "id": "xSDvqvHSeqFr"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Get the indices of the DataFrame\n",
        "indices = df.index.tolist()\n",
        "\n",
        "# Perform random sampling on the indices\n",
        "selected_indices = random.sample(indices, MAX_EXAMPLES)\n",
        "\n",
        "# Select the corresponding sentences and labels based on the sampled indices\n",
        "sentences = df.loc[selected_indices, 'reviews']\n",
        "labels = df.loc[selected_indices, 'sentiment']\n",
        "\n",
        "print(f\"There are {len(sentences)} sentences and {len(labels)} labels after random sampling\\n\")\n"
      ],
      "metadata": {
        "id": "Lm_QQl4setsh",
        "outputId": "326a29d9-c3bc-449a-f84b-f82567991af1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3000 sentences and 3000 labels after random sampling\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training - Validation Split"
      ],
      "metadata": {
        "id": "XBbquN0qf6Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(sentences, labels, training_split):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Compute the number of sentences that will be used for training (should be an integer)\n",
        "    train_size = int(len(sentences)*training_split)\n",
        "\n",
        "    # Split the sentences and labels into train/validation splits\n",
        "    train_sentences = sentences[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_sentences = sentences[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return train_sentences, validation_sentences, train_labels, validation_labels"
      ],
      "metadata": {
        "id": "BSptFFxJf8hw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
        "\n",
        "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
        "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
        "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
        "print(f\"There are {len(val_labels)} labels for validation.\")"
      ],
      "metadata": {
        "id": "QbdiZp4LgHwo",
        "outputId": "8084392a-93b4-4152-dad9-574ea59bbef4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2700 sentences for training.\n",
            "\n",
            "There are 2700 labels for training.\n",
            "\n",
            "There are 300 sentences for validation.\n",
            "\n",
            "There are 300 labels for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization - Sequences, Truncating, and Padding"
      ],
      "metadata": {
        "id": "Hkha78ZLhD8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: fit_tokenizer\n",
        "def fit_tokenizer(train_sentences, oov_token):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class, passing in the correct values for oov_token\n",
        "    tokenizer = Tokenizer(oov_token = OOV_TOKEN)\n",
        "    \n",
        "    # Fit the tokenizer to the training sentences\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "Ysf5VZ_hhITo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "tokenizer = fit_tokenizer(train_sentences, OOV_TOKEN)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index)\n",
        "\n",
        "print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n",
        "print(f\"\\nindex of word 'i' should be {word_index['i']}\")\n",
        "word_index"
      ],
      "metadata": {
        "id": "6Hy5fTzvhMMm",
        "outputId": "6a66f853-2a4a-4538-91f0-e43206365956",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 3691 words\n",
            "\n",
            "<OOV> token included in vocabulary\n",
            "\n",
            "index of word 'i' should be 1139\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'class': 2,\n",
              " 'professor': 3,\n",
              " 'students': 4,\n",
              " 'course': 5,\n",
              " 'material': 6,\n",
              " 'easy': 7,\n",
              " 'teacher': 8,\n",
              " 'understand': 9,\n",
              " 'learning': 10,\n",
              " 'lectures': 11,\n",
              " 'help': 12,\n",
              " 'teaching': 13,\n",
              " 'hard': 14,\n",
              " 'helpful': 15,\n",
              " 'lecturers': 16,\n",
              " 'questions': 17,\n",
              " 'lot': 18,\n",
              " 'assignments': 19,\n",
              " 'difficult': 20,\n",
              " 'professors': 21,\n",
              " 'subject': 22,\n",
              " 'time': 23,\n",
              " 'exams': 24,\n",
              " 'makes': 25,\n",
              " 'exam': 26,\n",
              " 'tests': 27,\n",
              " 'learn': 28,\n",
              " 'lecture': 29,\n",
              " 'nice': 30,\n",
              " 'provide': 31,\n",
              " 'study': 32,\n",
              " 'knowledge': 33,\n",
              " 'recommend': 34,\n",
              " 'math': 35,\n",
              " 'classes': 36,\n",
              " 'sometimes': 37,\n",
              " 'understanding': 38,\n",
              " 'homework': 39,\n",
              " 'materials': 40,\n",
              " 'final': 41,\n",
              " 'test': 42,\n",
              " 'knowledgeable': 43,\n",
              " 'grading': 44,\n",
              " 'extra': 45,\n",
              " 'experience': 46,\n",
              " 'boring': 47,\n",
              " 'bad': 48,\n",
              " 'engaging': 49,\n",
              " 'jokes': 50,\n",
              " 'credit': 51,\n",
              " 'content': 52,\n",
              " 'university': 53,\n",
              " 'concepts': 54,\n",
              " 'read': 55,\n",
              " 'teachers': 56,\n",
              " 'pretty': 57,\n",
              " 'guy': 58,\n",
              " 'fun': 59,\n",
              " 'grade': 60,\n",
              " 'fair': 61,\n",
              " 'prof': 62,\n",
              " 'funny': 63,\n",
              " 'lab': 64,\n",
              " 'practical': 65,\n",
              " 'available': 66,\n",
              " 'teach': 67,\n",
              " 'learned': 68,\n",
              " 'feedback': 69,\n",
              " 'book': 70,\n",
              " 'resources': 71,\n",
              " 'taking': 72,\n",
              " 'overall': 73,\n",
              " 'willing': 74,\n",
              " 'excellent': 75,\n",
              " 'paper': 76,\n",
              " 'feel': 77,\n",
              " 'notes': 78,\n",
              " 'books': 79,\n",
              " 'answer': 80,\n",
              " 'grades': 81,\n",
              " 'provides': 82,\n",
              " 'environment': 83,\n",
              " 'semester': 84,\n",
              " 'examples': 85,\n",
              " 'matter': 86,\n",
              " 'quizzes': 87,\n",
              " 'midterm': 88,\n",
              " 'teaches': 89,\n",
              " 'little': 90,\n",
              " 'provided': 91,\n",
              " 'fine': 92,\n",
              " 'helped': 93,\n",
              " 'pass': 94,\n",
              " 'classroom': 95,\n",
              " 'courses': 96,\n",
              " 'cares': 97,\n",
              " 'definitely': 98,\n",
              " 'challenging': 99,\n",
              " 'activities': 100,\n",
              " 'instructor': 101,\n",
              " 'person': 102,\n",
              " 'accent': 103,\n",
              " 'library': 104,\n",
              " 'papers': 105,\n",
              " 'average': 106,\n",
              " 'skills': 107,\n",
              " 'bit': 108,\n",
              " 'worst': 109,\n",
              " 'youll': 110,\n",
              " 'pattern': 111,\n",
              " 'discussions': 112,\n",
              " 'attention': 113,\n",
              " 'hours': 114,\n",
              " 'follow': 115,\n",
              " 'practice': 116,\n",
              " 'delivery': 117,\n",
              " 'style': 118,\n",
              " 'opportunities': 119,\n",
              " 'passionate': 120,\n",
              " 'topics': 121,\n",
              " 'relevant': 122,\n",
              " 'marks': 123,\n",
              " 'depth': 124,\n",
              " 'people': 125,\n",
              " 'taught': 126,\n",
              " 'reading': 127,\n",
              " 'lots': 128,\n",
              " 'amazing': 129,\n",
              " 'highly': 130,\n",
              " 'love': 131,\n",
              " 'super': 132,\n",
              " 'extremely': 133,\n",
              " 'pay': 134,\n",
              " 'write': 135,\n",
              " 'life': 136,\n",
              " 'encouraged': 137,\n",
              " 'interaction': 138,\n",
              " 'awesome': 139,\n",
              " 'labs': 140,\n",
              " 'times': 141,\n",
              " 'thinking': 142,\n",
              " 'getting': 143,\n",
              " 'confusing': 144,\n",
              " 'explain': 145,\n",
              " 'checking': 146,\n",
              " 'week': 147,\n",
              " 'cant': 148,\n",
              " 'means': 149,\n",
              " 'tough': 150,\n",
              " 'online': 151,\n",
              " 'sense': 152,\n",
              " 'im': 153,\n",
              " 'office': 154,\n",
              " 'information': 155,\n",
              " 'interactive': 156,\n",
              " 'stuff': 157,\n",
              " 'neither': 158,\n",
              " 'wasnt': 159,\n",
              " 'outside': 160,\n",
              " 'wish': 161,\n",
              " 'critical': 162,\n",
              " 'answers': 163,\n",
              " 'succeed': 164,\n",
              " 'try': 165,\n",
              " 'facilities': 166,\n",
              " 'evaluations': 167,\n",
              " 'question': 168,\n",
              " 'explanations': 169,\n",
              " 'punctuality': 170,\n",
              " 'projects': 171,\n",
              " 'helps': 172,\n",
              " 'participation': 173,\n",
              " 'explains': 174,\n",
              " 'explaining': 175,\n",
              " 'complex': 176,\n",
              " 'effectively': 177,\n",
              " 'personal': 178,\n",
              " 'system': 179,\n",
              " 'based': 180,\n",
              " 'friendly': 181,\n",
              " 'field': 182,\n",
              " 'takes': 183,\n",
              " 'youre': 184,\n",
              " 'approachable': 185,\n",
              " 'cool': 186,\n",
              " 'methods': 187,\n",
              " 'comments': 188,\n",
              " 'manner': 189,\n",
              " 'tries': 190,\n",
              " 'valuable': 191,\n",
              " 'attend': 192,\n",
              " 'research': 193,\n",
              " 'effort': 194,\n",
              " 'stories': 195,\n",
              " 'disorganized': 196,\n",
              " 'realworld': 197,\n",
              " 'talk': 198,\n",
              " 'terrible': 199,\n",
              " 'grader': 200,\n",
              " 'assessments': 201,\n",
              " 'prepared': 202,\n",
              " 'unclear': 203,\n",
              " 'day': 204,\n",
              " 'expectations': 205,\n",
              " 'tell': 206,\n",
              " 'easier': 207,\n",
              " 'actually': 208,\n",
              " 'support': 209,\n",
              " 'level': 210,\n",
              " 'organized': 211,\n",
              " 'effective': 212,\n",
              " 'terms': 213,\n",
              " 'timely': 214,\n",
              " 'strict': 215,\n",
              " 'encourages': 216,\n",
              " 'wont': 217,\n",
              " 'attendance': 218,\n",
              " 'presentation': 219,\n",
              " 'supportive': 220,\n",
              " 'easily': 221,\n",
              " 'decent': 222,\n",
              " 'comprehensive': 223,\n",
              " 'goes': 224,\n",
              " 'professional': 225,\n",
              " 'constructive': 226,\n",
              " 'simple': 227,\n",
              " 'covered': 228,\n",
              " 'proper': 229,\n",
              " 'required': 230,\n",
              " 'participate': 231,\n",
              " 'towards': 232,\n",
              " 'sweet': 233,\n",
              " 'informative': 234,\n",
              " 'talking': 235,\n",
              " 'discussion': 236,\n",
              " 'look': 237,\n",
              " 'enjoyable': 238,\n",
              " 'lack': 239,\n",
              " 'strong': 240,\n",
              " 'subjects': 241,\n",
              " 'heavy': 242,\n",
              " 'passion': 243,\n",
              " 'school': 244,\n",
              " 'writing': 245,\n",
              " 'found': 246,\n",
              " 'isnt': 247,\n",
              " 'project': 248,\n",
              " 'cover': 249,\n",
              " 'improvement': 250,\n",
              " 'care': 251,\n",
              " 'opinions': 252,\n",
              " 'rude': 253,\n",
              " 'ensuring': 254,\n",
              " 'stay': 255,\n",
              " 'business': 256,\n",
              " 'improve': 257,\n",
              " 'faculty': 258,\n",
              " 'neutral': 259,\n",
              " 'caring': 260,\n",
              " 'job': 261,\n",
              " 'sufficient': 262,\n",
              " 'topic': 263,\n",
              " 'okay': 264,\n",
              " 'explanation': 265,\n",
              " 'good': 266,\n",
              " 'loved': 267,\n",
              " 'additional': 268,\n",
              " 'real': 269,\n",
              " 'ok': 270,\n",
              " 'throughout': 271,\n",
              " 'textbook': 272,\n",
              " 'demonstrated': 273,\n",
              " 'english': 274,\n",
              " 'accessible': 275,\n",
              " 'positive': 276,\n",
              " 'worth': 277,\n",
              " 'leaving': 278,\n",
              " 'academic': 279,\n",
              " 'examination': 280,\n",
              " 'extracurricular': 281,\n",
              " 'fail': 282,\n",
              " 'allowing': 283,\n",
              " 'instructions': 284,\n",
              " 'assessment': 285,\n",
              " 'providing': 286,\n",
              " 'expects': 287,\n",
              " 'absolutely': 288,\n",
              " 'avoid': 289,\n",
              " 'humor': 290,\n",
              " 'guidance': 291,\n",
              " 'deep': 292,\n",
              " 'independent': 293,\n",
              " 'beyond': 294,\n",
              " 'guides': 295,\n",
              " 'lacks': 296,\n",
              " 'harder': 297,\n",
              " 'development': 298,\n",
              " 'process': 299,\n",
              " 'apply': 300,\n",
              " 'created': 301,\n",
              " 'readings': 302,\n",
              " 'enjoyed': 303,\n",
              " 'perspectives': 304,\n",
              " 'helping': 305,\n",
              " 'ideas': 306,\n",
              " 'adequate': 307,\n",
              " 'hell': 308,\n",
              " 'enthusiastic': 309,\n",
              " 'quiz': 310,\n",
              " 'communication': 311,\n",
              " 'criteria': 312,\n",
              " 'confusion': 313,\n",
              " 'applications': 314,\n",
              " 'complete': 315,\n",
              " 'guide': 316,\n",
              " 'approach': 317,\n",
              " 'choice': 318,\n",
              " 'truly': 319,\n",
              " 'syllabus': 320,\n",
              " 'review': 321,\n",
              " 'presenter': 322,\n",
              " 'useful': 323,\n",
              " 'unresponsive': 324,\n",
              " 'probably': 325,\n",
              " 'miss': 326,\n",
              " 'career': 327,\n",
              " 'encourage': 328,\n",
              " 'giving': 329,\n",
              " 'fast': 330,\n",
              " 'late': 331,\n",
              " 'concept': 332,\n",
              " 'outdated': 333,\n",
              " 'punctual': 334,\n",
              " 'create': 335,\n",
              " 'confused': 336,\n",
              " 'chapter': 337,\n",
              " 'grasp': 338,\n",
              " 'else': 339,\n",
              " 'genuinely': 340,\n",
              " 'distribution': 341,\n",
              " 'ready': 342,\n",
              " 'favorite': 343,\n",
              " 'smart': 344,\n",
              " 'due': 345,\n",
              " 'maintained': 346,\n",
              " 'studies': 347,\n",
              " 'inspiring': 348,\n",
              " 'textbooks': 349,\n",
              " 'fairly': 350,\n",
              " 'enthusiasm': 351,\n",
              " 'likes': 352,\n",
              " 'inclusive': 353,\n",
              " 'language': 354,\n",
              " 'comfortable': 355,\n",
              " 'tells': 356,\n",
              " 'causing': 357,\n",
              " 'lost': 358,\n",
              " 'posts': 359,\n",
              " 'unfair': 360,\n",
              " 'evaluation': 361,\n",
              " 'insights': 362,\n",
              " 'expected': 363,\n",
              " 'assignment': 364,\n",
              " 'wellorganized': 365,\n",
              " 'fostering': 366,\n",
              " 'slides': 367,\n",
              " 'world': 368,\n",
              " 'coming': 369,\n",
              " 'chance': 370,\n",
              " 'demonstrates': 371,\n",
              " 'current': 372,\n",
              " 'mark': 373,\n",
              " 'glad': 374,\n",
              " 'mean': 375,\n",
              " 'concise': 376,\n",
              " 'unhelpful': 377,\n",
              " 'emails': 378,\n",
              " 'industry': 379,\n",
              " 'low': 380,\n",
              " 'active': 381,\n",
              " 'comes': 382,\n",
              " 'listen': 383,\n",
              " 'looking': 384,\n",
              " 'inconsistent': 385,\n",
              " 'theory': 386,\n",
              " 'essays': 387,\n",
              " 'styles': 388,\n",
              " 'wonderful': 389,\n",
              " 'yes': 390,\n",
              " 'success': 391,\n",
              " 'mistakes': 392,\n",
              " 'enhance': 393,\n",
              " 'studying': 394,\n",
              " 'innovative': 395,\n",
              " 'perfect': 396,\n",
              " 'negative': 397,\n",
              " 'prepare': 398,\n",
              " 'poor': 399,\n",
              " 'chemistry': 400,\n",
              " 'diverse': 401,\n",
              " 'encouraging': 402,\n",
              " 'expect': 403,\n",
              " 'completely': 404,\n",
              " 'jazz': 405,\n",
              " 'multiple': 406,\n",
              " 'concerns': 407,\n",
              " 'respectful': 408,\n",
              " 'board': 409,\n",
              " 'memorize': 410,\n",
              " 'unorganized': 411,\n",
              " 'exactly': 412,\n",
              " 'regarding': 413,\n",
              " 'various': 414,\n",
              " 'trying': 415,\n",
              " 'clarity': 416,\n",
              " 'expertise': 417,\n",
              " 'etc': 418,\n",
              " 'curves': 419,\n",
              " 'plus': 420,\n",
              " 'arent': 421,\n",
              " 'entertaining': 422,\n",
              " 'develop': 423,\n",
              " 'objective': 424,\n",
              " 'lacked': 425,\n",
              " 'structured': 426,\n",
              " 'usually': 427,\n",
              " 'hour': 428,\n",
              " 'term': 429,\n",
              " 'incredibly': 430,\n",
              " 'consistent': 431,\n",
              " 'especially': 432,\n",
              " 'curve': 433,\n",
              " 'short': 434,\n",
              " 'reviews': 435,\n",
              " 'struggling': 436,\n",
              " 'college': 437,\n",
              " 'choose': 438,\n",
              " 'improved': 439,\n",
              " 'quality': 440,\n",
              " 'horrible': 441,\n",
              " 'understood': 442,\n",
              " 'held': 443,\n",
              " 'theres': 444,\n",
              " 'creating': 445,\n",
              " 'solid': 446,\n",
              " 'dry': 447,\n",
              " 'engage': 448,\n",
              " 'lessons': 449,\n",
              " 'mandatory': 450,\n",
              " 'promoting': 451,\n",
              " 'set': 452,\n",
              " 'computer': 453,\n",
              " 'wait': 454,\n",
              " 'role': 455,\n",
              " 'poorly': 456,\n",
              " 'wrong': 457,\n",
              " 'application': 458,\n",
              " 'satisfied': 459,\n",
              " 'werent': 460,\n",
              " 'weeks': 461,\n",
              " 'related': 462,\n",
              " 'issues': 463,\n",
              " 'intellectual': 464,\n",
              " 'practicals': 465,\n",
              " 'changes': 466,\n",
              " 'thanks': 467,\n",
              " 'chapters': 468,\n",
              " 'collaborative': 469,\n",
              " 'analysis': 470,\n",
              " 'resource': 471,\n",
              " 'focus': 472,\n",
              " 'handson': 473,\n",
              " 'stupid': 474,\n",
              " 'waste': 475,\n",
              " 'written': 476,\n",
              " 'blog': 477,\n",
              " 'interact': 478,\n",
              " 'irrelevant': 479,\n",
              " 'future': 480,\n",
              " 'theoretical': 481,\n",
              " 'instructors': 482,\n",
              " 'genuine': 483,\n",
              " 'watch': 484,\n",
              " 'detail': 485,\n",
              " 'beginning': 486,\n",
              " 'explore': 487,\n",
              " 'summer': 488,\n",
              " 'step': 489,\n",
              " 'quickly': 490,\n",
              " 'previous': 491,\n",
              " 'enjoy': 492,\n",
              " 'attitude': 493,\n",
              " 'progress': 494,\n",
              " 'allowed': 495,\n",
              " 'atmosphere': 496,\n",
              " 'technology': 497,\n",
              " 'reason': 498,\n",
              " 'critically': 499,\n",
              " 'midterms': 500,\n",
              " 'barrow': 501,\n",
              " 'fails': 502,\n",
              " 'requires': 503,\n",
              " 'offered': 504,\n",
              " 'seriously': 505,\n",
              " 'indepth': 506,\n",
              " 'tas': 507,\n",
              " 'solutions': 508,\n",
              " 'programming': 509,\n",
              " 'performance': 510,\n",
              " 'useless': 511,\n",
              " 'whats': 512,\n",
              " 'experienced': 513,\n",
              " 'speaking': 514,\n",
              " 'wellstructured': 515,\n",
              " 'techniques': 516,\n",
              " 'involved': 517,\n",
              " 'incomplete': 518,\n",
              " 'entire': 519,\n",
              " 'pace': 520,\n",
              " 'repeat': 521,\n",
              " 'responsive': 522,\n",
              " 'forward': 523,\n",
              " 'basically': 524,\n",
              " 'actively': 525,\n",
              " 'incorporated': 526,\n",
              " 'break': 527,\n",
              " 'focused': 528,\n",
              " 'unbiased': 529,\n",
              " 'growth': 530,\n",
              " 'days': 531,\n",
              " 'unique': 532,\n",
              " 'amount': 533,\n",
              " 'receive': 534,\n",
              " 'appropriate': 535,\n",
              " 'engaged': 536,\n",
              " 'workload': 537,\n",
              " 'form': 538,\n",
              " 'objectives': 539,\n",
              " 'properly': 540,\n",
              " 'motivated': 541,\n",
              " 'condescending': 542,\n",
              " 'management': 543,\n",
              " 'particular': 544,\n",
              " 'weekly': 545,\n",
              " 'chinese': 546,\n",
              " 'please': 547,\n",
              " 'thorough': 548,\n",
              " 'schedule': 549,\n",
              " 'flexible': 550,\n",
              " 'problemsolving': 551,\n",
              " 'honestly': 552,\n",
              " 'mind': 553,\n",
              " 'particularly': 554,\n",
              " 'covers': 555,\n",
              " 'rewarding': 556,\n",
              " 'music': 557,\n",
              " 'slow': 558,\n",
              " 'staff': 559,\n",
              " 'fall': 560,\n",
              " 'articles': 561,\n",
              " 'value': 562,\n",
              " 'fantastic': 563,\n",
              " 'simply': 564,\n",
              " 'text': 565,\n",
              " 'changed': 566,\n",
              " 'assigns': 567,\n",
              " 'words': 568,\n",
              " 'department': 569,\n",
              " 'policy': 570,\n",
              " 'straightforward': 571,\n",
              " 'difficulty': 572,\n",
              " 'opportunity': 573,\n",
              " 'cute': 574,\n",
              " 'increased': 575,\n",
              " 'half': 576,\n",
              " 'spend': 577,\n",
              " 'respect': 578,\n",
              " 'engagement': 579,\n",
              " 'tried': 580,\n",
              " 'graded': 581,\n",
              " 'unless': 582,\n",
              " 'structure': 583,\n",
              " 'track': 584,\n",
              " 'behavior': 585,\n",
              " 'ability': 586,\n",
              " 'trouble': 587,\n",
              " 'patient': 588,\n",
              " 'past': 589,\n",
              " 'analyze': 590,\n",
              " 'testing': 591,\n",
              " 'situations': 592,\n",
              " 'delivering': 593,\n",
              " 'passing': 594,\n",
              " 'leading': 595,\n",
              " 'mastery': 596,\n",
              " 'calculus': 597,\n",
              " '100': 598,\n",
              " 'gained': 599,\n",
              " 'reach': 600,\n",
              " 'platform': 601,\n",
              " 'breaks': 602,\n",
              " 'whatever': 603,\n",
              " 'ta': 604,\n",
              " 'gradesscores': 605,\n",
              " 'creativity': 606,\n",
              " 'insightful': 607,\n",
              " 'uninspiring': 608,\n",
              " 'sheet': 609,\n",
              " 'enhancing': 610,\n",
              " 'type': 611,\n",
              " 'lady': 612,\n",
              " 'biased': 613,\n",
              " 'agree': 614,\n",
              " 'inappropriate': 615,\n",
              " 'couldnt': 616,\n",
              " 'lives': 617,\n",
              " 'share': 618,\n",
              " 'relaxed': 619,\n",
              " 'ass': 620,\n",
              " 'is': 621,\n",
              " 'power': 622,\n",
              " 'confidence': 623,\n",
              " 'seek': 624,\n",
              " 'individual': 625,\n",
              " 'lacking': 626,\n",
              " 'consider': 627,\n",
              " 'single': 628,\n",
              " 'extensive': 629,\n",
              " 'actual': 630,\n",
              " 'section': 631,\n",
              " 'videos': 632,\n",
              " 'unprepared': 633,\n",
              " 'maintains': 634,\n",
              " 'liked': 635,\n",
              " 'songs': 636,\n",
              " 'left': 637,\n",
              " 'explained': 638,\n",
              " 'ge': 639,\n",
              " 'theories': 640,\n",
              " 'hate': 641,\n",
              " 'philosophy': 642,\n",
              " 'intimidating': 643,\n",
              " 'depending': 644,\n",
              " 'initially': 645,\n",
              " 'monotone': 646,\n",
              " 'mid': 647,\n",
              " 'change': 648,\n",
              " 'offers': 649,\n",
              " 'visual': 650,\n",
              " 'key': 651,\n",
              " 'efforts': 652,\n",
              " 'fostered': 653,\n",
              " 'collaboration': 654,\n",
              " 'visually': 655,\n",
              " 'answered': 656,\n",
              " 'suggest': 657,\n",
              " 'equations': 658,\n",
              " 'curriculum': 659,\n",
              " 'luck': 660,\n",
              " 'top': 661,\n",
              " 'allows': 662,\n",
              " 'model': 663,\n",
              " 'public': 664,\n",
              " 'remember': 665,\n",
              " 'gpa': 666,\n",
              " 'unapproachable': 667,\n",
              " 'hope': 668,\n",
              " 'figure': 669,\n",
              " 'word': 670,\n",
              " '15': 671,\n",
              " 'delivered': 672,\n",
              " 'variety': 673,\n",
              " 'keeping': 674,\n",
              " 'it': 675,\n",
              " 'loves': 676,\n",
              " 'otherwise': 677,\n",
              " 'deeper': 678,\n",
              " 'passed': 679,\n",
              " 'challenge': 680,\n",
              " 'opinion': 681,\n",
              " 'attending': 682,\n",
              " 'saying': 683,\n",
              " 'balanced': 684,\n",
              " 'unprofessional': 685,\n",
              " 'elements': 686,\n",
              " 'main': 687,\n",
              " 'according': 688,\n",
              " 'standards': 689,\n",
              " 'manageable': 690,\n",
              " 'dedication': 691,\n",
              " 'pop': 692,\n",
              " 'addressing': 693,\n",
              " 'tends': 694,\n",
              " 'heshe': 695,\n",
              " 'special': 696,\n",
              " 'education': 697,\n",
              " 'major': 698,\n",
              " 'sets': 699,\n",
              " 'clarification': 700,\n",
              " 'offer': 701,\n",
              " 'confident': 702,\n",
              " 'dynamic': 703,\n",
              " 'preparing': 704,\n",
              " 'standard': 705,\n",
              " 'aligned': 706,\n",
              " 'participating': 707,\n",
              " 'subjective': 708,\n",
              " 'ones': 709,\n",
              " 'lazy': 710,\n",
              " 'literally': 711,\n",
              " 'listening': 712,\n",
              " 'transparent': 713,\n",
              " 'frequently': 714,\n",
              " 'adequately': 715,\n",
              " 'huge': 716,\n",
              " 'powerpoint': 717,\n",
              " 'relate': 718,\n",
              " 'met': 719,\n",
              " 'struggle': 720,\n",
              " 'idea': 721,\n",
              " 'compared': 722,\n",
              " 'detailed': 723,\n",
              " 'favoritism': 724,\n",
              " 'email': 725,\n",
              " 'science': 726,\n",
              " 'pointless': 727,\n",
              " 'failed': 728,\n",
              " 'essay': 729,\n",
              " 'background': 730,\n",
              " 'scared': 731,\n",
              " 'worse': 732,\n",
              " 'reallife': 733,\n",
              " 'formula': 734,\n",
              " 'machine': 735,\n",
              " 'studied': 736,\n",
              " 'lower': 737,\n",
              " 'hated': 738,\n",
              " 'using': 739,\n",
              " 'formulas': 740,\n",
              " 'voice': 741,\n",
              " 'understandable': 742,\n",
              " 'respected': 743,\n",
              " 'basis': 744,\n",
              " 'understands': 745,\n",
              " 'disrespectful': 746,\n",
              " 'speak': 747,\n",
              " 'disabilities': 748,\n",
              " 'best': 749,\n",
              " 'software': 750,\n",
              " 'recommended': 751,\n",
              " 'community': 752,\n",
              " 'yeah': 753,\n",
              " 'introduction': 754,\n",
              " 'barrier': 755,\n",
              " 'knowledgable': 756,\n",
              " 'frustrated': 757,\n",
              " 'till': 758,\n",
              " 'tend': 759,\n",
              " 'align': 760,\n",
              " 'incorporating': 761,\n",
              " 'lowest': 762,\n",
              " 'issue': 763,\n",
              " 'specific': 764,\n",
              " 'evident': 765,\n",
              " 'drop': 766,\n",
              " '50': 767,\n",
              " 'history': 768,\n",
              " 'tasks': 769,\n",
              " 'require': 770,\n",
              " 'applicable': 771,\n",
              " 'complicated': 772,\n",
              " 'basic': 773,\n",
              " 'finished': 774,\n",
              " 'reliable': 775,\n",
              " 'move': 776,\n",
              " 'microbiology': 777,\n",
              " 'timeconsuming': 778,\n",
              " 'campus': 779,\n",
              " 'considering': 780,\n",
              " 'focusing': 781,\n",
              " 'check': 782,\n",
              " 'straight': 783,\n",
              " 'managed': 784,\n",
              " 'personality': 785,\n",
              " 'uptodate': 786,\n",
              " 'limited': 787,\n",
              " 'jerk': 788,\n",
              " 'quizes': 789,\n",
              " 'posted': 790,\n",
              " 'increase': 791,\n",
              " 'covering': 792,\n",
              " 'disappointing': 793,\n",
              " 'exact': 794,\n",
              " 'presentations': 795,\n",
              " 'examinations': 796,\n",
              " 'guys': 797,\n",
              " 'scenarios': 798,\n",
              " 'satisfactory': 799,\n",
              " 'method': 800,\n",
              " 'eyes': 801,\n",
              " 'creates': 802,\n",
              " 'responses': 803,\n",
              " 'tone': 804,\n",
              " 'utilized': 805,\n",
              " 'aids': 806,\n",
              " 'multimedia': 807,\n",
              " 'conducted': 808,\n",
              " 'appreciate': 809,\n",
              " 'cheating': 810,\n",
              " 'name': 811,\n",
              " 'true': 812,\n",
              " 'sharing': 813,\n",
              " 'vague': 814,\n",
              " 'woman': 815,\n",
              " 'ensure': 816,\n",
              " 'challenged': 817,\n",
              " 'inspired': 818,\n",
              " 'rest': 819,\n",
              " 'friends': 820,\n",
              " 'thick': 821,\n",
              " 'dialogue': 822,\n",
              " 'happy': 823,\n",
              " 'following': 824,\n",
              " 'child': 825,\n",
              " 'skip': 826,\n",
              " 'stop': 827,\n",
              " 'achieve': 828,\n",
              " 'stressful': 829,\n",
              " 'pages': 830,\n",
              " 'practices': 831,\n",
              " 'smile': 832,\n",
              " 'id': 833,\n",
              " 'immunology': 834,\n",
              " 'believe': 835,\n",
              " 'kept': 836,\n",
              " 'not': 837,\n",
              " '12': 838,\n",
              " 'consuming': 839,\n",
              " 'easiest': 840,\n",
              " 'south': 841,\n",
              " 'honest': 842,\n",
              " 'hardly': 843,\n",
              " 'memorable': 844,\n",
              " 'financial': 845,\n",
              " 'hindering': 846,\n",
              " 'impossible': 847,\n",
              " 'demeanor': 848,\n",
              " 'regular': 849,\n",
              " 'sports': 850,\n",
              " 'offering': 851,\n",
              " 'consistency': 852,\n",
              " 'looks': 853,\n",
              " 'crazy': 854,\n",
              " 'marker': 855,\n",
              " 'total': 856,\n",
              " 'methodical': 857,\n",
              " 'mention': 858,\n",
              " 'advanced': 859,\n",
              " 'angry': 860,\n",
              " 'organic': 861,\n",
              " 'tutorials': 862,\n",
              " 'attentive': 863,\n",
              " 'depends': 864,\n",
              " 'unengaging': 865,\n",
              " 'challenges': 866,\n",
              " 'obviously': 867,\n",
              " 'competent': 868,\n",
              " 'assigned': 869,\n",
              " 'thoroughly': 870,\n",
              " 'foster': 871,\n",
              " 'partial': 872,\n",
              " 'inquiries': 873,\n",
              " 'bring': 874,\n",
              " 'welcome': 875,\n",
              " 'return': 876,\n",
              " 'roberts': 877,\n",
              " 'solve': 878,\n",
              " 'seen': 879,\n",
              " 'mistake': 880,\n",
              " 'potential': 881,\n",
              " 'maybe': 882,\n",
              " 'page': 883,\n",
              " 'bio': 884,\n",
              " 'social': 885,\n",
              " 'start': 886,\n",
              " 'thoughtprovoking': 887,\n",
              " 'gis': 888,\n",
              " 'busy': 889,\n",
              " 'uncomfortable': 890,\n",
              " 'purpose': 891,\n",
              " 'command': 892,\n",
              " 'mile': 893,\n",
              " 'culture': 894,\n",
              " 'backgrounds': 895,\n",
              " '125': 896,\n",
              " 'clarify': 897,\n",
              " 'efficiently': 898,\n",
              " 'core': 899,\n",
              " 'received': 900,\n",
              " 'looked': 901,\n",
              " 'center': 902,\n",
              " 'plenty': 903,\n",
              " 'video': 904,\n",
              " 'stick': 905,\n",
              " 'essential': 906,\n",
              " 'oh': 907,\n",
              " 'strictly': 908,\n",
              " 'feeling': 909,\n",
              " 'cared': 910,\n",
              " 'marking': 911,\n",
              " 'homeworks': 912,\n",
              " 'deal': 913,\n",
              " 'deadlines': 914,\n",
              " 'sucks': 915,\n",
              " 'incredible': 916,\n",
              " 'monotonous': 917,\n",
              " 'appreciated': 918,\n",
              " 'overwhelming': 919,\n",
              " 'whenever': 920,\n",
              " 'asleep': 921,\n",
              " 'awful': 922,\n",
              " 'ample': 923,\n",
              " 'chill': 924,\n",
              " 'totally': 925,\n",
              " 'submit': 926,\n",
              " 'corrections': 927,\n",
              " 'intelligent': 928,\n",
              " 'literature': 929,\n",
              " 'frustrating': 930,\n",
              " 'fool': 931,\n",
              " 'graduate': 932,\n",
              " 'assistance': 933,\n",
              " 'advantage': 934,\n",
              " 'setting': 935,\n",
              " 'meet': 936,\n",
              " 'cancels': 937,\n",
              " 'assistants': 938,\n",
              " 'annoying': 939,\n",
              " 'theyre': 940,\n",
              " 'hilarious': 941,\n",
              " 'integrity': 942,\n",
              " 'sources': 943,\n",
              " 'weird': 944,\n",
              " 'call': 945,\n",
              " 'costs': 946,\n",
              " 'shell': 947,\n",
              " 'duration': 948,\n",
              " 'box': 949,\n",
              " 'directions': 950,\n",
              " 'economics': 951,\n",
              " 'told': 952,\n",
              " 'plan': 953,\n",
              " 'rote': 954,\n",
              " 'internet': 955,\n",
              " 'correct': 956,\n",
              " 'biology': 957,\n",
              " 'exposure': 958,\n",
              " 'dull': 959,\n",
              " 'live': 960,\n",
              " 'failing': 961,\n",
              " 'foundation': 962,\n",
              " 'random': 963,\n",
              " 'respond': 964,\n",
              " 'classmates': 965,\n",
              " 'nicest': 966,\n",
              " 'holds': 967,\n",
              " 'minutes': 968,\n",
              " 'tons': 969,\n",
              " 'calls': 970,\n",
              " 'pushing': 971,\n",
              " 'trust': 972,\n",
              " 'front': 973,\n",
              " 'absences': 974,\n",
              " 'advancements': 975,\n",
              " 'errors': 976,\n",
              " 'delays': 977,\n",
              " 'disinterested': 978,\n",
              " 'dedicated': 979,\n",
              " 'lucky': 980,\n",
              " 'viewpoints': 981,\n",
              " 'outline': 982,\n",
              " 'established': 983,\n",
              " 'night': 984,\n",
              " 'quick': 985,\n",
              " 'third': 986,\n",
              " 'deliver': 987,\n",
              " 'results': 988,\n",
              " 'strategic': 989,\n",
              " 'surprised': 990,\n",
              " 'touch': 991,\n",
              " 'save': 992,\n",
              " 'safe': 993,\n",
              " 'amy': 994,\n",
              " 'partiality': 995,\n",
              " 'north': 996,\n",
              " 'indians': 997,\n",
              " 'rules': 998,\n",
              " 'ramble': 999,\n",
              " 'empowering': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 56
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "    ### START CODE HERE\n",
        "       \n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the correct padding, truncating and maxlen\n",
        "    pad_trunc_sequences = pad_sequences(sequences, maxlen= MAXLEN, padding = PADDING, truncating = TRUNCATING)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return pad_trunc_sequences"
      ],
      "metadata": {
        "id": "xUkJFtiwh7C-"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "\n",
        "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
        "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
      ],
      "metadata": {
        "id": "biCCj27uh-Ve",
        "outputId": "da391694-e1cb-425d-a4b7-427bcc022c35",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded and truncated training sequences have shape: (2700, 16)\n",
            "\n",
            "Padded and truncated validation sequences have shape: (300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)"
      ],
      "metadata": {
        "id": "OR3wfthriCnN"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pre-defined Embeddings"
      ],
      "metadata": {
        "id": "j0j16iy7iI-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to file containing the embeddings\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "\n",
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ],
      "metadata": {
        "id": "tUHybcFMiE0d"
      },
      "execution_count": 47,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Represent the words in your vocabulary using the embeddings"
      ],
      "metadata": {
        "id": "ldaN0eawl5v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty numpy array with the appropriate size\n",
        "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
        "\n",
        "# Iterate all of the words in the vocabulary and if the vector representation for \n",
        "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        EMBEDDINGS_MATRIX[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Rokt-fwRl8NB"
      },
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a model that does not overfit"
      ],
      "metadata": {
        "id": "HZjjYxbzl8ga"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "    model = tf.keras.Sequential([ \n",
        "        # This is how you need to set the Embedding layer when using pre-trained embeddings\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
        "        tf.keras.layers.GlobalMaxPooling1D(),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(32, activation='relu'),\n",
        "        tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])\n",
        "    \n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer='adam',\n",
        "                  metrics=['accuracy']) \n",
        "    return model"
      ],
      "metadata": {
        "id": "HAWbMeF-mBg5"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)\n",
        "\n",
        "# Train the model and save the training history\n",
        "history = model.fit(train_pad_trunc_seq, train_labels, epochs=200, validation_data=(val_pad_trunc_seq, val_labels))"
      ],
      "metadata": {
        "id": "ArRf_nFcmEjZ",
        "outputId": "141e5846-5dd1-435d-91fc-a22963ce7f33",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "85/85 [==============================] - 2s 4ms/step - loss: 1.0282 - accuracy: 0.5096 - val_loss: 0.9825 - val_accuracy: 0.6033\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.9146 - accuracy: 0.5863 - val_loss: 0.8944 - val_accuracy: 0.6333\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.8277 - accuracy: 0.6370 - val_loss: 0.8445 - val_accuracy: 0.6400\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.7538 - accuracy: 0.6793 - val_loss: 0.8082 - val_accuracy: 0.6433\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6838 - accuracy: 0.7163 - val_loss: 0.7990 - val_accuracy: 0.6667\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.6372 - accuracy: 0.7315 - val_loss: 0.7995 - val_accuracy: 0.6567\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5712 - accuracy: 0.7678 - val_loss: 0.7903 - val_accuracy: 0.6767\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.5033 - accuracy: 0.7981 - val_loss: 0.8203 - val_accuracy: 0.6633\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4562 - accuracy: 0.8230 - val_loss: 0.8028 - val_accuracy: 0.6867\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.4147 - accuracy: 0.8415 - val_loss: 0.8108 - val_accuracy: 0.6833\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3835 - accuracy: 0.8526 - val_loss: 0.8341 - val_accuracy: 0.6767\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3609 - accuracy: 0.8607 - val_loss: 0.8581 - val_accuracy: 0.6967\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3420 - accuracy: 0.8707 - val_loss: 0.9123 - val_accuracy: 0.6900\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.3024 - accuracy: 0.8907 - val_loss: 0.8912 - val_accuracy: 0.6900\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2996 - accuracy: 0.8900 - val_loss: 0.9548 - val_accuracy: 0.6700\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2781 - accuracy: 0.8978 - val_loss: 1.0120 - val_accuracy: 0.6867\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2692 - accuracy: 0.9007 - val_loss: 1.0194 - val_accuracy: 0.6667\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2627 - accuracy: 0.9030 - val_loss: 1.0217 - val_accuracy: 0.6700\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2451 - accuracy: 0.9063 - val_loss: 1.0626 - val_accuracy: 0.6900\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2413 - accuracy: 0.9070 - val_loss: 1.0929 - val_accuracy: 0.6633\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2341 - accuracy: 0.9115 - val_loss: 1.1388 - val_accuracy: 0.6633\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2162 - accuracy: 0.9256 - val_loss: 1.1587 - val_accuracy: 0.6667\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2035 - accuracy: 0.9278 - val_loss: 1.1426 - val_accuracy: 0.6667\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2157 - accuracy: 0.9241 - val_loss: 1.2105 - val_accuracy: 0.6833\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1946 - accuracy: 0.9311 - val_loss: 1.2025 - val_accuracy: 0.6767\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.2057 - accuracy: 0.9278 - val_loss: 1.1974 - val_accuracy: 0.6733\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1895 - accuracy: 0.9293 - val_loss: 1.2277 - val_accuracy: 0.6900\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1887 - accuracy: 0.9326 - val_loss: 1.2663 - val_accuracy: 0.6767\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1841 - accuracy: 0.9356 - val_loss: 1.3368 - val_accuracy: 0.6733\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1863 - accuracy: 0.9341 - val_loss: 1.2767 - val_accuracy: 0.6900\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1755 - accuracy: 0.9322 - val_loss: 1.3373 - val_accuracy: 0.6833\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1680 - accuracy: 0.9393 - val_loss: 1.3238 - val_accuracy: 0.6800\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1641 - accuracy: 0.9426 - val_loss: 1.3090 - val_accuracy: 0.6667\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9419 - val_loss: 1.3268 - val_accuracy: 0.6667\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1679 - accuracy: 0.9452 - val_loss: 1.4475 - val_accuracy: 0.6500\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1712 - accuracy: 0.9367 - val_loss: 1.4152 - val_accuracy: 0.6700\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1487 - accuracy: 0.9452 - val_loss: 1.4286 - val_accuracy: 0.6733\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1489 - accuracy: 0.9452 - val_loss: 1.3023 - val_accuracy: 0.6767\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1545 - accuracy: 0.9422 - val_loss: 1.4483 - val_accuracy: 0.6567\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1606 - accuracy: 0.9448 - val_loss: 1.3900 - val_accuracy: 0.6833\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1381 - accuracy: 0.9500 - val_loss: 1.5159 - val_accuracy: 0.6733\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1661 - accuracy: 0.9396 - val_loss: 1.3711 - val_accuracy: 0.6633\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1427 - accuracy: 0.9463 - val_loss: 1.4895 - val_accuracy: 0.6867\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1395 - accuracy: 0.9500 - val_loss: 1.5163 - val_accuracy: 0.6667\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1337 - accuracy: 0.9515 - val_loss: 1.4828 - val_accuracy: 0.6833\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1230 - accuracy: 0.9556 - val_loss: 1.5510 - val_accuracy: 0.6800\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1443 - accuracy: 0.9459 - val_loss: 1.5317 - val_accuracy: 0.6733\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9563 - val_loss: 1.5002 - val_accuracy: 0.6900\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1276 - accuracy: 0.9567 - val_loss: 1.4788 - val_accuracy: 0.6867\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1343 - accuracy: 0.9541 - val_loss: 1.5490 - val_accuracy: 0.6633\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1274 - accuracy: 0.9519 - val_loss: 1.5979 - val_accuracy: 0.6600\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1339 - accuracy: 0.9556 - val_loss: 1.6728 - val_accuracy: 0.6833\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1352 - accuracy: 0.9504 - val_loss: 1.6483 - val_accuracy: 0.6567\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1271 - accuracy: 0.9541 - val_loss: 1.7121 - val_accuracy: 0.6667\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1472 - accuracy: 0.9467 - val_loss: 1.6980 - val_accuracy: 0.6700\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1251 - accuracy: 0.9559 - val_loss: 1.7182 - val_accuracy: 0.6700\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1434 - accuracy: 0.9493 - val_loss: 1.6724 - val_accuracy: 0.6800\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1275 - accuracy: 0.9563 - val_loss: 1.6326 - val_accuracy: 0.6800\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1265 - accuracy: 0.9548 - val_loss: 1.6655 - val_accuracy: 0.6400\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1214 - accuracy: 0.9552 - val_loss: 1.7612 - val_accuracy: 0.6700\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1203 - accuracy: 0.9556 - val_loss: 1.6809 - val_accuracy: 0.6700\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1213 - accuracy: 0.9567 - val_loss: 1.6040 - val_accuracy: 0.6633\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1281 - accuracy: 0.9548 - val_loss: 1.7829 - val_accuracy: 0.6700\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1173 - accuracy: 0.9585 - val_loss: 1.7480 - val_accuracy: 0.6633\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1320 - accuracy: 0.9552 - val_loss: 1.7415 - val_accuracy: 0.6667\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1295 - accuracy: 0.9522 - val_loss: 1.7130 - val_accuracy: 0.6500\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1102 - accuracy: 0.9596 - val_loss: 1.6921 - val_accuracy: 0.6767\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1330 - accuracy: 0.9526 - val_loss: 1.7298 - val_accuracy: 0.6900\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1206 - accuracy: 0.9530 - val_loss: 1.7811 - val_accuracy: 0.6800\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1200 - accuracy: 0.9548 - val_loss: 1.8175 - val_accuracy: 0.6500\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1037 - accuracy: 0.9615 - val_loss: 1.8246 - val_accuracy: 0.6600\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1104 - accuracy: 0.9585 - val_loss: 1.7454 - val_accuracy: 0.6567\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1146 - accuracy: 0.9559 - val_loss: 1.7877 - val_accuracy: 0.6733\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1080 - accuracy: 0.9593 - val_loss: 1.7934 - val_accuracy: 0.6900\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1155 - accuracy: 0.9611 - val_loss: 1.9244 - val_accuracy: 0.6667\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1089 - accuracy: 0.9600 - val_loss: 1.8995 - val_accuracy: 0.6567\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1072 - accuracy: 0.9596 - val_loss: 1.7791 - val_accuracy: 0.6567\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0999 - accuracy: 0.9574 - val_loss: 1.8972 - val_accuracy: 0.6367\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1011 - accuracy: 0.9626 - val_loss: 1.8862 - val_accuracy: 0.6467\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1134 - accuracy: 0.9581 - val_loss: 1.8025 - val_accuracy: 0.6500\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1034 - accuracy: 0.9630 - val_loss: 1.9514 - val_accuracy: 0.6600\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1073 - accuracy: 0.9615 - val_loss: 2.0040 - val_accuracy: 0.6533\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1106 - accuracy: 0.9593 - val_loss: 2.0016 - val_accuracy: 0.6600\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.1105 - accuracy: 0.9593 - val_loss: 1.8718 - val_accuracy: 0.6733\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1020 - accuracy: 0.9596 - val_loss: 1.8691 - val_accuracy: 0.6667\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1264 - accuracy: 0.9507 - val_loss: 1.8886 - val_accuracy: 0.6667\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0953 - accuracy: 0.9626 - val_loss: 1.9664 - val_accuracy: 0.6733\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1097 - accuracy: 0.9604 - val_loss: 1.9254 - val_accuracy: 0.6800\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1058 - accuracy: 0.9611 - val_loss: 1.9937 - val_accuracy: 0.6600\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9630 - val_loss: 1.8932 - val_accuracy: 0.6500\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1001 - accuracy: 0.9633 - val_loss: 1.8812 - val_accuracy: 0.6700\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1054 - accuracy: 0.9626 - val_loss: 1.8430 - val_accuracy: 0.6433\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1125 - accuracy: 0.9593 - val_loss: 1.9619 - val_accuracy: 0.6633\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9689 - val_loss: 1.9548 - val_accuracy: 0.6800\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9674 - val_loss: 2.0602 - val_accuracy: 0.6533\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1006 - accuracy: 0.9630 - val_loss: 2.1870 - val_accuracy: 0.6600\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1092 - accuracy: 0.9615 - val_loss: 2.0578 - val_accuracy: 0.6567\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0992 - accuracy: 0.9596 - val_loss: 2.0380 - val_accuracy: 0.6633\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0991 - accuracy: 0.9633 - val_loss: 2.0027 - val_accuracy: 0.6700\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1091 - accuracy: 0.9559 - val_loss: 1.8382 - val_accuracy: 0.6600\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1107 - accuracy: 0.9567 - val_loss: 1.8743 - val_accuracy: 0.6767\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1023 - accuracy: 0.9630 - val_loss: 1.9247 - val_accuracy: 0.6600\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0922 - accuracy: 0.9633 - val_loss: 1.8836 - val_accuracy: 0.6667\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1009 - accuracy: 0.9604 - val_loss: 1.9249 - val_accuracy: 0.6367\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0995 - accuracy: 0.9611 - val_loss: 1.8990 - val_accuracy: 0.6633\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9641 - val_loss: 1.8528 - val_accuracy: 0.6700\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0971 - accuracy: 0.9641 - val_loss: 1.9485 - val_accuracy: 0.6600\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0923 - accuracy: 0.9648 - val_loss: 1.9247 - val_accuracy: 0.6533\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0825 - accuracy: 0.9674 - val_loss: 2.0000 - val_accuracy: 0.6700\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1113 - accuracy: 0.9589 - val_loss: 2.0812 - val_accuracy: 0.6700\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1133 - accuracy: 0.9585 - val_loss: 1.9450 - val_accuracy: 0.6667\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0942 - accuracy: 0.9644 - val_loss: 1.9528 - val_accuracy: 0.6667\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9619 - val_loss: 1.9904 - val_accuracy: 0.6633\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9626 - val_loss: 1.9695 - val_accuracy: 0.6667\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0955 - accuracy: 0.9633 - val_loss: 1.8444 - val_accuracy: 0.6867\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0948 - accuracy: 0.9611 - val_loss: 2.0001 - val_accuracy: 0.6767\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9641 - val_loss: 2.0119 - val_accuracy: 0.6900\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9656 - val_loss: 2.0309 - val_accuracy: 0.6800\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0906 - accuracy: 0.9667 - val_loss: 2.0273 - val_accuracy: 0.6667\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0883 - accuracy: 0.9652 - val_loss: 2.0688 - val_accuracy: 0.6767\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0938 - accuracy: 0.9637 - val_loss: 1.9425 - val_accuracy: 0.6600\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0932 - accuracy: 0.9644 - val_loss: 1.9233 - val_accuracy: 0.6700\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0836 - accuracy: 0.9674 - val_loss: 1.9239 - val_accuracy: 0.6733\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.1009 - accuracy: 0.9600 - val_loss: 2.0738 - val_accuracy: 0.6700\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0976 - accuracy: 0.9633 - val_loss: 1.8662 - val_accuracy: 0.6567\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 0s 5ms/step - loss: 0.0974 - accuracy: 0.9589 - val_loss: 2.0393 - val_accuracy: 0.6800\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0887 - accuracy: 0.9622 - val_loss: 1.9489 - val_accuracy: 0.6767\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0912 - accuracy: 0.9648 - val_loss: 1.9745 - val_accuracy: 0.6633\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1000 - accuracy: 0.9585 - val_loss: 1.9047 - val_accuracy: 0.6733\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0908 - accuracy: 0.9644 - val_loss: 1.9768 - val_accuracy: 0.6600\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0914 - accuracy: 0.9615 - val_loss: 2.2239 - val_accuracy: 0.6467\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0865 - accuracy: 0.9652 - val_loss: 2.0109 - val_accuracy: 0.6567\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0931 - accuracy: 0.9633 - val_loss: 1.9978 - val_accuracy: 0.6500\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0969 - accuracy: 0.9611 - val_loss: 2.1026 - val_accuracy: 0.6767\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0960 - accuracy: 0.9652 - val_loss: 2.1488 - val_accuracy: 0.6700\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0841 - accuracy: 0.9663 - val_loss: 2.0586 - val_accuracy: 0.6667\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0928 - accuracy: 0.9637 - val_loss: 2.1202 - val_accuracy: 0.6700\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0773 - accuracy: 0.9711 - val_loss: 2.2261 - val_accuracy: 0.6467\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0945 - accuracy: 0.9622 - val_loss: 2.1418 - val_accuracy: 0.6467\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0957 - accuracy: 0.9630 - val_loss: 2.0664 - val_accuracy: 0.6633\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9689 - val_loss: 2.0689 - val_accuracy: 0.6667\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0854 - accuracy: 0.9644 - val_loss: 2.1144 - val_accuracy: 0.6600\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0843 - accuracy: 0.9652 - val_loss: 1.9193 - val_accuracy: 0.6733\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0858 - accuracy: 0.9670 - val_loss: 2.1460 - val_accuracy: 0.6400\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0802 - accuracy: 0.9663 - val_loss: 2.1011 - val_accuracy: 0.6467\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0815 - accuracy: 0.9667 - val_loss: 2.2914 - val_accuracy: 0.6733\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0853 - accuracy: 0.9637 - val_loss: 2.1245 - val_accuracy: 0.6333\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9693 - val_loss: 2.2727 - val_accuracy: 0.6533\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0890 - accuracy: 0.9648 - val_loss: 2.2380 - val_accuracy: 0.6433\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0978 - accuracy: 0.9630 - val_loss: 2.1815 - val_accuracy: 0.6567\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0831 - accuracy: 0.9674 - val_loss: 2.0728 - val_accuracy: 0.6400\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0769 - accuracy: 0.9696 - val_loss: 2.1607 - val_accuracy: 0.6567\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0867 - accuracy: 0.9678 - val_loss: 2.0016 - val_accuracy: 0.6467\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0730 - accuracy: 0.9674 - val_loss: 2.2917 - val_accuracy: 0.6433\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0817 - accuracy: 0.9674 - val_loss: 2.3347 - val_accuracy: 0.6200\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0805 - accuracy: 0.9678 - val_loss: 2.3670 - val_accuracy: 0.6400\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1007 - accuracy: 0.9596 - val_loss: 2.2927 - val_accuracy: 0.6400\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0838 - accuracy: 0.9648 - val_loss: 2.2382 - val_accuracy: 0.6633\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0767 - accuracy: 0.9704 - val_loss: 2.1728 - val_accuracy: 0.6300\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9667 - val_loss: 2.1803 - val_accuracy: 0.6567\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0920 - accuracy: 0.9607 - val_loss: 2.2282 - val_accuracy: 0.6633\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0880 - accuracy: 0.9607 - val_loss: 2.2520 - val_accuracy: 0.6467\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0827 - accuracy: 0.9674 - val_loss: 2.1545 - val_accuracy: 0.6633\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0846 - accuracy: 0.9670 - val_loss: 2.1433 - val_accuracy: 0.6600\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0806 - accuracy: 0.9644 - val_loss: 2.1815 - val_accuracy: 0.6533\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0796 - accuracy: 0.9693 - val_loss: 2.1466 - val_accuracy: 0.6400\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0777 - accuracy: 0.9681 - val_loss: 2.2279 - val_accuracy: 0.6500\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0845 - accuracy: 0.9615 - val_loss: 2.1099 - val_accuracy: 0.6533\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0859 - accuracy: 0.9641 - val_loss: 2.2289 - val_accuracy: 0.6367\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0736 - accuracy: 0.9689 - val_loss: 2.4400 - val_accuracy: 0.6633\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0872 - accuracy: 0.9648 - val_loss: 2.2397 - val_accuracy: 0.6500\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0909 - accuracy: 0.9674 - val_loss: 2.2226 - val_accuracy: 0.6333\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0861 - accuracy: 0.9659 - val_loss: 2.2787 - val_accuracy: 0.6300\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0775 - accuracy: 0.9704 - val_loss: 2.3425 - val_accuracy: 0.6467\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0868 - accuracy: 0.9648 - val_loss: 2.2318 - val_accuracy: 0.6467\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0989 - accuracy: 0.9559 - val_loss: 2.1277 - val_accuracy: 0.6267\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0855 - accuracy: 0.9656 - val_loss: 2.1876 - val_accuracy: 0.6400\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0729 - accuracy: 0.9689 - val_loss: 2.2397 - val_accuracy: 0.6567\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0777 - accuracy: 0.9685 - val_loss: 2.2158 - val_accuracy: 0.6467\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0770 - accuracy: 0.9663 - val_loss: 2.2396 - val_accuracy: 0.6133\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0806 - accuracy: 0.9674 - val_loss: 2.4927 - val_accuracy: 0.6167\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0748 - accuracy: 0.9667 - val_loss: 2.3842 - val_accuracy: 0.6167\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0835 - accuracy: 0.9648 - val_loss: 2.3642 - val_accuracy: 0.6467\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0743 - accuracy: 0.9652 - val_loss: 2.5867 - val_accuracy: 0.6400\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0754 - accuracy: 0.9659 - val_loss: 2.3274 - val_accuracy: 0.6333\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0706 - accuracy: 0.9693 - val_loss: 2.4592 - val_accuracy: 0.6367\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0780 - accuracy: 0.9685 - val_loss: 2.5201 - val_accuracy: 0.6467\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0654 - accuracy: 0.9730 - val_loss: 2.4769 - val_accuracy: 0.6400\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0764 - accuracy: 0.9667 - val_loss: 2.5501 - val_accuracy: 0.6467\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0793 - accuracy: 0.9670 - val_loss: 2.3258 - val_accuracy: 0.6667\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0760 - accuracy: 0.9681 - val_loss: 2.5803 - val_accuracy: 0.6467\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0734 - accuracy: 0.9689 - val_loss: 2.4732 - val_accuracy: 0.6500\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0849 - accuracy: 0.9670 - val_loss: 2.4830 - val_accuracy: 0.6267\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.1003 - accuracy: 0.9611 - val_loss: 2.4093 - val_accuracy: 0.6533\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0905 - accuracy: 0.9641 - val_loss: 2.4616 - val_accuracy: 0.6500\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0679 - accuracy: 0.9689 - val_loss: 2.5937 - val_accuracy: 0.6633\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0774 - accuracy: 0.9685 - val_loss: 2.6271 - val_accuracy: 0.6633\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0681 - accuracy: 0.9707 - val_loss: 2.6831 - val_accuracy: 0.6633\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 0s 3ms/step - loss: 0.0783 - accuracy: 0.9648 - val_loss: 2.5957 - val_accuracy: 0.6600\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 0s 4ms/step - loss: 0.0760 - accuracy: 0.9670 - val_loss: 2.5376 - val_accuracy: 0.6500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = model.predict(val_pad_trunc_seq)\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(val_labels, val_predicted_labels, average='weighted')\n",
        "recall = recall_score(val_labels, val_predicted_labels, average='weighted')\n",
        "f1 = f1_score(val_labels, val_predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n"
      ],
      "metadata": {
        "id": "icfjRpZh0Pao",
        "outputId": "10ed600a-e491-4bb1-935f-e542993378ab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 0s 2ms/step\n",
            "Precision:  0.6397891558654842\n",
            "Recall:  0.65\n",
            "F1-score:  0.6428606440581361\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metrics, our model performs relatively well with relatively high values of precision, recall, and F1-score"
      ],
      "metadata": {
        "id": "Z8LTrwH3183K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "tzYcQXQFaKgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file in read mode\n",
        "my_file = open(\"stopwords.txt\", \"r\")\n",
        "  \n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "  \n",
        "# replacing end splitting the text \n",
        "# when newline ('\\n') is seen.\n",
        "stopwords_data = data.split(\"\\n\")\n",
        "print(stopwords_data)\n",
        "my_file.close()"
      ],
      "metadata": {
        "id": "yoi7tX7ZaFcK",
        "outputId": "a268d478-fa34-496c-bb31-d16f90d1cea6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['a', 'about', 'above', 'across', 'after', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'among', 'an', 'and', 'another', 'any', 'anybody', 'anyone', 'anything', 'anywhere', 'are', 'area', 'areas', 'around', 'as', 'ask', 'asked', 'asking', 'asks', 'at', 'away', 'b', 'back', 'backed', 'backing', 'backs', 'be', 'became', 'because', 'become', 'becomes', 'been', 'before', 'began', 'behind', 'being', 'beings', 'best', 'better', 'between', 'big', 'both', 'but', 'by', 'c', 'came', 'can', 'cannot', 'case', 'cases', 'certain', 'certainly', 'clear', 'clearly', 'come', 'could', 'd', 'did', 'differ', 'different', 'differently', 'do', 'does', 'done', 'down', 'down', 'downed', 'downing', 'downs', 'during', 'e', 'each', 'early', 'either', 'end', 'ended', 'ending', 'ends', 'enough', 'even', 'evenly', 'ever', 'every', 'everybody', 'everyone', 'everything', 'everywhere', 'f', 'face', 'faces', 'fact', 'facts', 'far', 'felt', 'few', 'find', 'finds', 'first', 'for', 'four', 'from', 'full', 'fully', 'further', 'furthered', 'furthering', 'furthers', 'g', 'gave', 'general', 'generally', 'get', 'gets', 'give', 'given', 'gives', 'go', 'going', 'good', 'goods', 'got', 'great', 'greater', 'greatest', 'group', 'grouped', 'grouping', 'groups', 'h', 'had', 'has', 'have', 'having', 'he', 'her', 'here', 'herself', 'high', 'high', 'high', 'higher', 'highest', 'him', 'himself', 'his', 'how', 'however', 'i', 'if', 'important', 'in', 'interest', 'interested', 'interesting', 'interests', 'into', 'is', 'it', 'its', 'itself', 'j', 'just', 'k', 'keep', 'keeps', 'kind', 'knew', 'know', 'known', 'knows', 'l', 'large', 'largely', 'last', 'later', 'latest', 'least', 'less', 'let', 'lets', 'like', 'likely', 'long', 'longer', 'longest', 'm', 'made', 'make', 'making', 'man', 'many', 'may', 'me', 'member', 'members', 'men', 'might', 'more', 'most', 'mostly', 'mr', 'mrs', 'much', 'must', 'my', 'myself', 'n', 'necessary', 'need', 'needed', 'needing', 'needs', 'never', 'new', 'new', 'newer', 'newest', 'next', 'no', 'nobody', 'non', 'noone', 'not', 'nothing', 'now', 'nowhere', 'number', 'numbers', 'o', 'of', 'off', 'often', 'old', 'older', 'oldest', 'on', 'once', 'one', 'only', 'open', 'opened', 'opening', 'opens', 'or', 'order', 'ordered', 'ordering', 'orders', 'other', 'others', 'our', 'out', 'over', 'p', 'part', 'parted', 'parting', 'parts', 'per', 'perhaps', 'place', 'places', 'point', 'pointed', 'pointing', 'points', 'possible', 'present', 'presented', 'presenting', 'presents', 'problem', 'problems', 'put', 'puts', 'q', 'quite', 'r', 'rather', 'really', 'right', 'right', 'room', 'rooms', 's', 'said', 'same', 'saw', 'say', 'says', 'second', 'seconds', 'see', 'seem', 'seemed', 'seeming', 'seems', 'sees', 'several', 'shall', 'she', 'should', 'show', 'showed', 'showing', 'shows', 'side', 'sides', 'since', 'small', 'smaller', 'smallest', 'so', 'some', 'somebody', 'someone', 'something', 'somewhere', 'state', 'states', 'still', 'still', 'such', 'sure', 't', 'take', 'taken', 'than', 'that', 'the', 'their', 'them', 'then', 'there', 'therefore', 'these', 'they', 'thing', 'things', 'think', 'thinks', 'this', 'those', 'though', 'thought', 'thoughts', 'three', 'through', 'thus', 'to', 'today', 'together', 'too', 'took', 'toward', 'turn', 'turned', 'turning', 'turns', 'two', 'u', 'under', 'until', 'up', 'upon', 'us', 'use', 'used', 'uses', 'v', 'very', 'w', 'want', 'wanted', 'wanting', 'wants', 'was', 'way', 'ways', 'we', 'well', 'wells', 'went', 'were', 'what', 'when', 'where', 'whether', 'which', 'while', 'who', 'whole', 'whose', 'why', 'will', 'with', 'within', 'without', 'work', 'worked', 'working', 'works', 'would', 'x', 'y', 'year', 'years', 'yet', 'you', 'young', 'younger', 'youngest', 'your', 'yours', 'z', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Remove punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "    return text_without_punctuation"
      ],
      "metadata": {
        "id": "Lp2wK3_7aNgJ"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "for i in range(len(df)):\n",
        "  cleaned_text.append(remove_punctuation(df['reviews'][i]))\n",
        "\n",
        "df['reviews'] = cleaned_text\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "id": "6VGvjRW5aPAX",
        "outputId": "9cddaac0-f3b3-42c2-dee2-d59169ca9231",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['teacher are punctual but they should also give us the some practical knowledge other than theortical',\n",
              "       'good',\n",
              "       'excellent lectures are delivered by teachers and all teachers are very punctual',\n",
              "       ..., 'lecturers give very good explanations',\n",
              "       'lecturers often provoke discussion in class',\n",
              "       'lecturers provide material in a boring way'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(sentence, data):\n",
        "    # List of stopwords\n",
        "\n",
        "    stopwords = data + [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    numbers_stopwords = [\"1\", \"2\", \"3\", \"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n",
        "                         \"one\", \"two\",\"three\",\"four\",\"five\",\"098\"]\n",
        "    more_words = [\"didn't\", \"don't\", \"dont\", \"didnt\", \"it\", \"doesnt\", \"doesn't\", \"hw\",\"won't\",\"lpu\",\"weren't\",\"mr\",\"mcq\",\"shes\",\n",
        "                  \"shes\",\"india\",\"in\",\"hes\",\"shes\",\"me\", \"dr\", \"nlandu\", \"ko\",\"it\",\"1st\", \"omr\", \"ha\", \"upto\",\"ca\", \"soo\", \"cd\", \"ive\",\"po\",\"cse\", \"chem\", \"un\",\"of\",\n",
        "                  \"mte\", \"omr\",\"mte's\",\"ca's\",\"ete's\",\"jnv\",\"ip\",\"sir\",\"its\",\"wks\",\"prob\",\"python\",\"java\",\"lattc\",\"ol\",\"ived\",\"elsewhere\", \"mother\",\"wouldnt\",\"car\",\n",
        "                  \"si\", \"sat\",\"we\",\"home\",\"hot\",\"god\",\"ice\",\"money's\",\"money\",\"even\",\"about\",\"thats\", \"wks\", \"thurs\", \"months\", \"sir\", \"go\", \"jnv\", \"ip\", \"today\", \"today's\", \"linux\", \"github\",\n",
        "                  \"lt\", \"ums\", \"superb\", \"at\", \"cgpa\",\"ques\", \"brain's\", \"mcqs\", \"ve\", \"say\", \"pc\", \"viva\", \"after\", \"before\", \"draw\", \"asst\", \"only\", \"rich\", \"never\", \"went\", \"pcs\", \"gk\", \"one's\",\n",
        "                  \"co\", \"duty\", \"gona\", \"attendnce\",\"same\", \"that's\", \"hahahah\", \"ad's\", \"university's\", \"relly\", \"build\", \"cricket\", \"said\", \"hall\", \"profs\", \"guy's\", \"can\", \"along\", \"archieve\", \"bag\",\n",
        "                  \"part\", \"master\", \"push\", \"or\", \"add\", \"were\", \"virginia\",\"human\", \"bless\", \"clean\", \"count\", \"onlineopen\", \"ounce\", \"brushing\", \"zero\", \"mail\", \"fys\", \"lowell\", \"stets\", \"untill\", \"until\",\n",
        "                  \"prep\", \"appears\", \"giulia\", \"yuk\", \"memo\", \"ton\", \"110q\", \"unit\", \"80\",\"re\",\"by\",\"order\",\"fob\", \"sit\", \"from\",\"art\", \"org\", \"4d\", \"3d\", \"cinema\", \"iii\", \"cal\", \"both\", \"sundays\", \"todays\", \"ad\",\n",
        "                  \"yoursel\",\"yourself\", \"kiss\", \"it'll\", \"obayani's\", \"anal\", \"pgs\", \"csci\", \"hw\", \"more\", \"able\", \"lecturer\", \"lecturer's\", \"student\", \"stundet's\", \"it\", \"want\", \"you\",\"he's\", \"she's\"]\n",
        "    more =  [\n",
        "    'a', 'about', 'above', 'after', 'again', 'against', \"ain't\", 'all', 'am', 'an', 'and', 'any', 'are', 'aren\\'t', 'as', \n",
        "    'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'can\\'t', 'cannot', \n",
        "    'could', 'couldn\\'t', 'did', 'didn\\'t', 'do', 'does', 'doesn\\'t', 'doing', 'don\\'t', 'down', 'during', 'each', 'few', \n",
        "    'for', 'from', 'further', 'had', 'hadn\\'t', 'has', 'hasn\\'t', 'have', 'haven\\'t', 'having', 'he', 'he\\'d', 'he\\'ll', \n",
        "    'he\\'s', 'her', 'here', 'here\\'s', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'how\\'s', 'i', 'i\\'d', 'i\\'ll', \n",
        "    'i\\'m', 'i\\'ve', 'if', 'in', 'into', 'is', 'isn\\'t', 'it', 'it\\'s', 'its', 'itself', 'let\\'s', 'me', 'more', 'most', \n",
        "    'mustn\\'t', 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', \n",
        "    'ours', 'ourselves', 'out', 'over', 'own', 'same', 'shan\\'t', 'she', 'she\\'d', 'she\\'ll', 'she\\'s', 'should', \n",
        "    'shouldn\\'t', 'so', 'some', 'such', 'than', 'that', 'that\\'s', 'the', 'their', 'theirs', 'them', 'themselves', \n",
        "    'then', 'there', 'there\\'s', 'these', 'they', 'they\\'d', 'they\\'ll', 'they\\'re', 'they\\'ve', 'this', 'those', \n",
        "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'wasn\\'t', 'we', 'we\\'d', 'we\\'ll', 'we\\'re', \n",
        "    'we\\'ve', 'were', 'weren\\'t', 'what', 'what\\'s', 'when', 'when\\'s', 'where', 'where\\'s', 'which', 'while', 'who', \n",
        "    'who\\'s', 'whom', 'why', 'why\\'s', 'with', 'won\\'t', 'would', 'wouldn\\'t', 'you', 'you\\'d', 'you\\'ll', 'you\\'re', \n",
        "    'you\\'ve', 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "    final_stopwords = stopwords + numbers_stopwords + more_words + more\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    words = sentence.split()\n",
        "    tempWords = []\n",
        "    for i in words:\n",
        "        if i not in final_stopwords:\n",
        "            tempWords.append(i)\n",
        "            sentence = ' '.join(tempWords)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "rg8BJ24RaQF5"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "id": "5Eu9CPSHaSv0",
        "outputId": "020eaa25-6fdd-43b1-d742-3d570b61525d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['teacher are punctual but they should also give us the some practical knowledge other than theortical',\n",
              "       'good',\n",
              "       'excellent lectures are delivered by teachers and all teachers are very punctual',\n",
              "       ..., 'lecturers give very good explanations',\n",
              "       'lecturers often provoke discussion in class',\n",
              "       'lecturers provide material in a boring way'], dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cKomen = []\n",
        "for i in range(len(df)):\n",
        "  cKomen.append(remove_stopwords(df['reviews'][i], stopwords_data))\n",
        "\n",
        "df['reviews'] = cKomen"
      ],
      "metadata": {
        "id": "usgdW83Haj3y"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameter\n",
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 64\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post'"
      ],
      "metadata": {
        "id": "d2qSifujbDEs"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "_HJ6k07AbIxI"
      },
      "execution_count": 17,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "Selamat Datang di Colaboratory",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}