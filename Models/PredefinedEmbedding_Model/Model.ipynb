{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import linregress"
      ],
      "metadata": {
        "id": "_qfzq2wDZmkx"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('ReviewsEN.csv')"
      ],
      "metadata": {
        "id": "Vd4os3NhZp5l"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"-1 :\", df['sentiment'].value_counts()[-1])\n",
        "print(\"0 :\", df['sentiment'].value_counts()[0])\n",
        "print(\"1 :\", df['sentiment'].value_counts()[1])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uQq0VQTZ9iF",
        "outputId": "4660b8e8-f87f-42f5-86fc-8039ec24a046"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-1 : 870\n",
            "0 : 639\n",
            "1 : 1518\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace values in pandas DataFrame.\n",
        "df['sentiment'] = df['sentiment'].replace([1], 2)\n",
        "df['sentiment'] = df['sentiment'].replace([0], 1)\n",
        "df['sentiment'] = df['sentiment'].replace([-1], 0)"
      ],
      "metadata": {
        "id": "DsSXQdDXZ-0R"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# # Apply lower function\n",
        "# df['reviews'] = df['reviews'].apply(str.lower)\n",
        "# print(df)"
      ],
      "metadata": {
        "id": "RayqsORYaBXV"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Global Variables\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "MAXLEN = 16\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "MAX_EXAMPLES = 3000\n",
        "TRAINING_SPLIT = 0.9"
      ],
      "metadata": {
        "id": "xSDvqvHSeqFr"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Get the indices of the DataFrame\n",
        "indices = df.index.tolist()\n",
        "\n",
        "# Perform random sampling on the indices\n",
        "selected_indices = random.sample(indices, MAX_EXAMPLES)\n",
        "\n",
        "# Select the corresponding sentences and labels based on the sampled indices\n",
        "sentences = df.loc[selected_indices, 'reviews']\n",
        "labels = df.loc[selected_indices, 'sentiment']\n",
        "\n",
        "print(f\"There are {len(sentences)} sentences and {len(labels)} labels after random sampling\\n\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_QQl4setsh",
        "outputId": "fcee6982-3bc6-4075-f4e2-ba6221d54367"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 3000 sentences and 3000 labels after random sampling\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training - Validation Split"
      ],
      "metadata": {
        "id": "XBbquN0qf6Og"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_val_split(sentences, labels, training_split):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Compute the number of sentences that will be used for training (should be an integer)\n",
        "    train_size = int(len(sentences)*training_split)\n",
        "\n",
        "    # Split the sentences and labels into train/validation splits\n",
        "    train_sentences = sentences[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_sentences = sentences[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return train_sentences, validation_sentences, train_labels, validation_labels"
      ],
      "metadata": {
        "id": "BSptFFxJf8hw"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
        "\n",
        "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
        "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
        "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
        "print(f\"There are {len(val_labels)} labels for validation.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbdiZp4LgHwo",
        "outputId": "6e9af5f3-9b3c-495c-c35d-d26c1447b470"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 2700 sentences for training.\n",
            "\n",
            "There are 2700 labels for training.\n",
            "\n",
            "There are 300 sentences for validation.\n",
            "\n",
            "There are 300 labels for validation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tokenization - Sequences, Truncating, and Padding"
      ],
      "metadata": {
        "id": "Hkha78ZLhD8n"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# GRADED FUNCTION: fit_tokenizer\n",
        "def fit_tokenizer(train_sentences, oov_token):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class, passing in the correct values for oov_token\n",
        "    tokenizer = Tokenizer(oov_token = OOV_TOKEN)\n",
        "    \n",
        "    # Fit the tokenizer to the training sentences\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return tokenizer"
      ],
      "metadata": {
        "id": "Ysf5VZ_hhITo"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Test your function\n",
        "tokenizer = fit_tokenizer(train_sentences, OOV_TOKEN)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index)\n",
        "\n",
        "print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n",
        "print(f\"\\nindex of word 'i' should be {word_index['i']}\")\n",
        "word_index"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hy5fTzvhMMm",
        "outputId": "e20d7359-9282-4d59-f733-aa0fc4ee8d06"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary contains 3956 words\n",
            "\n",
            "<OOV> token included in vocabulary\n",
            "\n",
            "index of word 'i' should be 9\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'to': 4,\n",
              " 'is': 5,\n",
              " 'a': 6,\n",
              " 'he': 7,\n",
              " 'you': 8,\n",
              " 'i': 9,\n",
              " 'class': 10,\n",
              " 'of': 11,\n",
              " 'in': 12,\n",
              " 'very': 13,\n",
              " 'professor': 14,\n",
              " 'but': 15,\n",
              " 'not': 16,\n",
              " 'was': 17,\n",
              " 'are': 18,\n",
              " 'for': 19,\n",
              " 'it': 20,\n",
              " 'she': 21,\n",
              " 'good': 22,\n",
              " 'students': 23,\n",
              " 'his': 24,\n",
              " 'this': 25,\n",
              " 'on': 26,\n",
              " 'with': 27,\n",
              " 'if': 28,\n",
              " 'that': 29,\n",
              " 'course': 30,\n",
              " 'be': 31,\n",
              " 'or': 32,\n",
              " 'have': 33,\n",
              " 'her': 34,\n",
              " 'material': 35,\n",
              " 'easy': 36,\n",
              " 'as': 37,\n",
              " 'all': 38,\n",
              " 'were': 39,\n",
              " 'teacher': 40,\n",
              " 'about': 41,\n",
              " 'so': 42,\n",
              " 'really': 43,\n",
              " 'take': 44,\n",
              " 'will': 45,\n",
              " 'do': 46,\n",
              " 'an': 47,\n",
              " 'great': 48,\n",
              " 'understand': 49,\n",
              " 'lecturer': 50,\n",
              " 'can': 51,\n",
              " 'learning': 52,\n",
              " 'him': 53,\n",
              " 'lectures': 54,\n",
              " 'at': 55,\n",
              " 'get': 56,\n",
              " 'help': 57,\n",
              " 'helpful': 58,\n",
              " 'hard': 59,\n",
              " 'teaching': 60,\n",
              " 'had': 61,\n",
              " 'well': 62,\n",
              " 'my': 63,\n",
              " 'more': 64,\n",
              " 'questions': 65,\n",
              " 'they': 66,\n",
              " 'just': 67,\n",
              " 'work': 68,\n",
              " 'assignments': 69,\n",
              " 'lot': 70,\n",
              " 'difficult': 71,\n",
              " 'like': 72,\n",
              " 'there': 73,\n",
              " 'your': 74,\n",
              " 'what': 75,\n",
              " 'best': 76,\n",
              " 'one': 77,\n",
              " 'always': 78,\n",
              " 'time': 79,\n",
              " 'would': 80,\n",
              " \"don't\": 81,\n",
              " 'no': 82,\n",
              " 'also': 83,\n",
              " 'from': 84,\n",
              " 'subject': 85,\n",
              " 'too': 86,\n",
              " 'their': 87,\n",
              " 'exams': 88,\n",
              " 'up': 89,\n",
              " 'makes': 90,\n",
              " 'has': 91,\n",
              " 'tests': 92,\n",
              " 'did': 93,\n",
              " 'me': 94,\n",
              " 'exam': 95,\n",
              " 'clear': 96,\n",
              " 'which': 97,\n",
              " 'lecture': 98,\n",
              " 'learn': 99,\n",
              " 'much': 100,\n",
              " 'make': 101,\n",
              " 'because': 102,\n",
              " 'nice': 103,\n",
              " 'know': 104,\n",
              " 'some': 105,\n",
              " 'provide': 106,\n",
              " 'does': 107,\n",
              " 'made': 108,\n",
              " 'interesting': 109,\n",
              " \"lecturer's\": 110,\n",
              " 'them': 111,\n",
              " 'study': 112,\n",
              " 'we': 113,\n",
              " 'when': 114,\n",
              " \"he's\": 115,\n",
              " 'knowledge': 116,\n",
              " 'gives': 117,\n",
              " 'recommend': 118,\n",
              " 'every': 119,\n",
              " 'classes': 120,\n",
              " 'math': 121,\n",
              " 'us': 122,\n",
              " 'sometimes': 123,\n",
              " 'homework': 124,\n",
              " 'understanding': 125,\n",
              " 'way': 126,\n",
              " 'out': 127,\n",
              " \"it's\": 128,\n",
              " 'materials': 129,\n",
              " 'go': 130,\n",
              " \"professor's\": 131,\n",
              " 'by': 132,\n",
              " 'only': 133,\n",
              " 'test': 134,\n",
              " 'final': 135,\n",
              " 'ever': 136,\n",
              " 'knowledgeable': 137,\n",
              " 'who': 138,\n",
              " 'extra': 139,\n",
              " 'experience': 140,\n",
              " 'student': 141,\n",
              " 'grading': 142,\n",
              " 'everything': 143,\n",
              " 'even': 144,\n",
              " 'bad': 145,\n",
              " 'boring': 146,\n",
              " 'need': 147,\n",
              " 'engaging': 148,\n",
              " 'credit': 149,\n",
              " 'jokes': 150,\n",
              " 'how': 151,\n",
              " 'should': 152,\n",
              " 'university': 153,\n",
              " 'concepts': 154,\n",
              " 'than': 155,\n",
              " 'content': 156,\n",
              " 'read': 157,\n",
              " 'guy': 158,\n",
              " 'think': 159,\n",
              " \"doesn't\": 160,\n",
              " 'fun': 161,\n",
              " 'ask': 162,\n",
              " 'most': 163,\n",
              " 'pretty': 164,\n",
              " 'any': 165,\n",
              " 'grade': 166,\n",
              " 'fair': 167,\n",
              " 'long': 168,\n",
              " 'prof': 169,\n",
              " 'could': 170,\n",
              " 'lab': 171,\n",
              " 'sure': 172,\n",
              " 'funny': 173,\n",
              " 'book': 174,\n",
              " 'teachers': 175,\n",
              " 'lecturers': 176,\n",
              " 'practical': 177,\n",
              " 'other': 178,\n",
              " 'took': 179,\n",
              " 'never': 180,\n",
              " 'grades': 181,\n",
              " 'teach': 182,\n",
              " 'making': 183,\n",
              " \"didn't\": 184,\n",
              " 'learned': 185,\n",
              " 'available': 186,\n",
              " 'knows': 187,\n",
              " 'give': 188,\n",
              " 'feedback': 189,\n",
              " 'again': 190,\n",
              " 'excellent': 191,\n",
              " 'over': 192,\n",
              " 'resources': 193,\n",
              " 'taking': 194,\n",
              " 'overall': 195,\n",
              " 'willing': 196,\n",
              " 'books': 197,\n",
              " 'paper': 198,\n",
              " 'feel': 199,\n",
              " 'notes': 200,\n",
              " 'answer': 201,\n",
              " 'quizzes': 202,\n",
              " 'semester': 203,\n",
              " 'provides': 204,\n",
              " 'our': 205,\n",
              " 'want': 206,\n",
              " 'matter': 207,\n",
              " 'different': 208,\n",
              " 'environment': 209,\n",
              " \"students'\": 210,\n",
              " 'problems': 211,\n",
              " 'during': 212,\n",
              " 'enough': 213,\n",
              " 'without': 214,\n",
              " 'real': 215,\n",
              " 'examples': 216,\n",
              " 'midterm': 217,\n",
              " 'first': 218,\n",
              " 'teaches': 219,\n",
              " 'got': 220,\n",
              " 'little': 221,\n",
              " 'provided': 222,\n",
              " 'fine': 223,\n",
              " 'often': 224,\n",
              " 'helped': 225,\n",
              " 'pass': 226,\n",
              " 'classroom': 227,\n",
              " 'depth': 228,\n",
              " 'person': 229,\n",
              " 'going': 230,\n",
              " 'accent': 231,\n",
              " 'cares': 232,\n",
              " 'definitely': 233,\n",
              " 'things': 234,\n",
              " 'challenging': 235,\n",
              " 'anything': 236,\n",
              " \"she's\": 237,\n",
              " 'professors': 238,\n",
              " 'papers': 239,\n",
              " 'activities': 240,\n",
              " 'instructor': 241,\n",
              " 'courses': 242,\n",
              " 'library': 243,\n",
              " 'average': 244,\n",
              " 'better': 245,\n",
              " 'skills': 246,\n",
              " '2': 247,\n",
              " 'life': 248,\n",
              " 'into': 249,\n",
              " 'bit': 250,\n",
              " 'worst': 251,\n",
              " 'hours': 252,\n",
              " 'follow': 253,\n",
              " 'pattern': 254,\n",
              " 'am': 255,\n",
              " 'discussions': 256,\n",
              " 'then': 257,\n",
              " 'must': 258,\n",
              " 'attention': 259,\n",
              " 'felt': 260,\n",
              " 'practice': 261,\n",
              " 'able': 262,\n",
              " 'off': 263,\n",
              " 'use': 264,\n",
              " 'delivery': 265,\n",
              " 'still': 266,\n",
              " 'through': 267,\n",
              " 'after': 268,\n",
              " 'style': 269,\n",
              " \"you'll\": 270,\n",
              " 'opportunities': 271,\n",
              " 'wants': 272,\n",
              " 'passionate': 273,\n",
              " 'topics': 274,\n",
              " 'relevant': 275,\n",
              " 'before': 276,\n",
              " 'world': 277,\n",
              " 'marks': 278,\n",
              " 'organized': 279,\n",
              " 'however': 280,\n",
              " 'people': 281,\n",
              " 'taught': 282,\n",
              " 'used': 283,\n",
              " 'come': 284,\n",
              " 'reading': 285,\n",
              " 'own': 286,\n",
              " 'lots': 287,\n",
              " 'its': 288,\n",
              " 'amazing': 289,\n",
              " 'less': 290,\n",
              " 'write': 291,\n",
              " 'many': 292,\n",
              " \"i've\": 293,\n",
              " 'highly': 294,\n",
              " 'keep': 295,\n",
              " 'problem': 296,\n",
              " 'love': 297,\n",
              " 'encouraged': 298,\n",
              " 'super': 299,\n",
              " 'awesome': 300,\n",
              " 'extremely': 301,\n",
              " 'labs': 302,\n",
              " 'down': 303,\n",
              " 'pay': 304,\n",
              " 'doing': 305,\n",
              " 'everyone': 306,\n",
              " 'week': 307,\n",
              " 'interaction': 308,\n",
              " 'thinking': 309,\n",
              " 'getting': 310,\n",
              " 'kind': 311,\n",
              " 'dont': 312,\n",
              " 'confusing': 313,\n",
              " 'explain': 314,\n",
              " 'though': 315,\n",
              " 'checking': 316,\n",
              " 'each': 317,\n",
              " 'based': 318,\n",
              " 'sense': 319,\n",
              " 'thought': 320,\n",
              " 'means': 321,\n",
              " 'tough': 322,\n",
              " 'online': 323,\n",
              " 'group': 324,\n",
              " 'nor': 325,\n",
              " 'participation': 326,\n",
              " 'times': 327,\n",
              " \"can't\": 328,\n",
              " 'office': 329,\n",
              " 'being': 330,\n",
              " 'stuff': 331,\n",
              " '3': 332,\n",
              " 'thing': 333,\n",
              " 'information': 334,\n",
              " 'find': 335,\n",
              " 'been': 336,\n",
              " 'interactive': 337,\n",
              " 'answers': 338,\n",
              " 'neither': 339,\n",
              " 'question': 340,\n",
              " 'anyone': 341,\n",
              " \"i'm\": 342,\n",
              " 'outside': 343,\n",
              " 'points': 344,\n",
              " 'say': 345,\n",
              " 'show': 346,\n",
              " 'wish': 347,\n",
              " 'critical': 348,\n",
              " 'put': 349,\n",
              " 'succeed': 350,\n",
              " 'cool': 351,\n",
              " 'try': 352,\n",
              " 'facilities': 353,\n",
              " 'evaluations': 354,\n",
              " 'explanations': 355,\n",
              " 'important': 356,\n",
              " 'needs': 357,\n",
              " 'b': 358,\n",
              " '1': 359,\n",
              " 'punctuality': 360,\n",
              " 'projects': 361,\n",
              " 'takes': 362,\n",
              " \"wasn't\": 363,\n",
              " 'clearly': 364,\n",
              " 'taken': 365,\n",
              " 'helps': 366,\n",
              " 'prepared': 367,\n",
              " 'explains': 368,\n",
              " 'explaining': 369,\n",
              " 'complex': 370,\n",
              " 'effectively': 371,\n",
              " 'personal': 372,\n",
              " 'system': 373,\n",
              " 'friendly': 374,\n",
              " 'field': 375,\n",
              " 'among': 376,\n",
              " 'such': 377,\n",
              " 'approachable': 378,\n",
              " 'part': 379,\n",
              " 'methods': 380,\n",
              " 'comments': 381,\n",
              " 'manner': 382,\n",
              " 'tries': 383,\n",
              " 'went': 384,\n",
              " 'high': 385,\n",
              " 'valuable': 386,\n",
              " 'unclear': 387,\n",
              " 'attend': 388,\n",
              " 'something': 389,\n",
              " 'research': 390,\n",
              " 'effort': 391,\n",
              " 'stories': 392,\n",
              " 'disorganized': 393,\n",
              " 'talk': 394,\n",
              " 'terrible': 395,\n",
              " 'nothing': 396,\n",
              " 'grader': 397,\n",
              " 'assessments': 398,\n",
              " 'level': 399,\n",
              " \"you're\": 400,\n",
              " 'point': 401,\n",
              " 'far': 402,\n",
              " 'day': 403,\n",
              " 'presentation': 404,\n",
              " 'others': 405,\n",
              " 'expectations': 406,\n",
              " '5': 407,\n",
              " 'tell': 408,\n",
              " 'easier': 409,\n",
              " 'while': 410,\n",
              " 'quite': 411,\n",
              " 'actually': 412,\n",
              " 'support': 413,\n",
              " 'two': 414,\n",
              " 'goes': 415,\n",
              " 'effective': 416,\n",
              " 'professional': 417,\n",
              " 'terms': 418,\n",
              " 'timely': 419,\n",
              " 'strict': 420,\n",
              " 'encourages': 421,\n",
              " 'attendance': 422,\n",
              " 'topic': 423,\n",
              " 'informative': 424,\n",
              " 'supportive': 425,\n",
              " 'new': 426,\n",
              " 'end': 427,\n",
              " 'heavy': 428,\n",
              " 'easily': 429,\n",
              " 'decent': 430,\n",
              " 'these': 431,\n",
              " 'discussion': 432,\n",
              " 'interested': 433,\n",
              " 'those': 434,\n",
              " 'subjects': 435,\n",
              " 'comprehensive': 436,\n",
              " 'constructive': 437,\n",
              " 'simple': 438,\n",
              " 'stay': 439,\n",
              " 'covered': 440,\n",
              " 'proper': 441,\n",
              " 'required': 442,\n",
              " 'participate': 443,\n",
              " 'towards': 444,\n",
              " 'sweet': 445,\n",
              " 'talking': 446,\n",
              " 'look': 447,\n",
              " 'enjoyable': 448,\n",
              " 'lack': 449,\n",
              " 'possible': 450,\n",
              " 'strong': 451,\n",
              " 'few': 452,\n",
              " 'passion': 453,\n",
              " 'school': 454,\n",
              " 'writing': 455,\n",
              " 'found': 456,\n",
              " 'project': 457,\n",
              " 'cover': 458,\n",
              " 'improvement': 459,\n",
              " 'structured': 460,\n",
              " 'given': 461,\n",
              " 'care': 462,\n",
              " 'gave': 463,\n",
              " 'opinions': 464,\n",
              " 'rude': 465,\n",
              " 'ensuring': 466,\n",
              " 'business': 467,\n",
              " 'improve': 468,\n",
              " 'faculty': 469,\n",
              " 'neutral': 470,\n",
              " 'caring': 471,\n",
              " 'job': 472,\n",
              " 'sufficient': 473,\n",
              " 'okay': 474,\n",
              " 'explanation': 475,\n",
              " 'may': 476,\n",
              " 'loved': 477,\n",
              " 'examination': 478,\n",
              " 'additional': 479,\n",
              " 'ok': 480,\n",
              " 'needed': 481,\n",
              " 'throughout': 482,\n",
              " 'now': 483,\n",
              " 'textbook': 484,\n",
              " 'demonstrated': 485,\n",
              " 'said': 486,\n",
              " 'expects': 487,\n",
              " 'english': 488,\n",
              " 'open': 489,\n",
              " 'right': 490,\n",
              " 'interest': 491,\n",
              " 'yourself': 492,\n",
              " 'man': 493,\n",
              " 'accessible': 494,\n",
              " \"isn't\": 495,\n",
              " 'seems': 496,\n",
              " 'positive': 497,\n",
              " 'worth': 498,\n",
              " 'leaving': 499,\n",
              " 'academic': 500,\n",
              " 'extracurricular': 501,\n",
              " 'fail': 502,\n",
              " 'ideas': 503,\n",
              " 'allowing': 504,\n",
              " 'instructions': 505,\n",
              " 'assessment': 506,\n",
              " 'providing': 507,\n",
              " 'once': 508,\n",
              " 'absolutely': 509,\n",
              " 'avoid': 510,\n",
              " 'humor': 511,\n",
              " 'away': 512,\n",
              " 'almost': 513,\n",
              " 'u': 514,\n",
              " 'guidance': 515,\n",
              " 'deep': 516,\n",
              " 'independent': 517,\n",
              " 'beyond': 518,\n",
              " 'until': 519,\n",
              " 'guides': 520,\n",
              " 'lacks': 521,\n",
              " 'where': 522,\n",
              " 'necessary': 523,\n",
              " 'harder': 524,\n",
              " 'development': 525,\n",
              " 'process': 526,\n",
              " '10': 527,\n",
              " 'another': 528,\n",
              " 'apply': 529,\n",
              " 'created': 530,\n",
              " 'readings': 531,\n",
              " 'enjoyed': 532,\n",
              " 'perspectives': 533,\n",
              " 'helping': 534,\n",
              " 'syllabus': 535,\n",
              " 'back': 536,\n",
              " 'adequate': 537,\n",
              " 'between': 538,\n",
              " 'case': 539,\n",
              " 'same': 540,\n",
              " \"won't\": 541,\n",
              " 'fast': 542,\n",
              " 'miss': 543,\n",
              " 'enthusiastic': 544,\n",
              " 'language': 545,\n",
              " 'quiz': 546,\n",
              " 'communication': 547,\n",
              " 'criteria': 548,\n",
              " 'confusion': 549,\n",
              " 'applications': 550,\n",
              " 'yet': 551,\n",
              " 'complete': 552,\n",
              " 'guide': 553,\n",
              " 'approach': 554,\n",
              " 'next': 555,\n",
              " 'choice': 556,\n",
              " 'truly': 557,\n",
              " 'review': 558,\n",
              " 'else': 559,\n",
              " 'see': 560,\n",
              " 'presenter': 561,\n",
              " 'useful': 562,\n",
              " 'says': 563,\n",
              " 'fairly': 564,\n",
              " 'unresponsive': 565,\n",
              " 'probably': 566,\n",
              " 'career': 567,\n",
              " 'encourage': 568,\n",
              " 'giving': 569,\n",
              " 'late': 570,\n",
              " 'concept': 571,\n",
              " 'outdated': 572,\n",
              " \"that's\": 573,\n",
              " 'punctual': 574,\n",
              " 'uses': 575,\n",
              " 'create': 576,\n",
              " 'confused': 577,\n",
              " 'chapter': 578,\n",
              " 'posts': 579,\n",
              " 'grasp': 580,\n",
              " 'genuinely': 581,\n",
              " 'since': 582,\n",
              " 'assignment': 583,\n",
              " 'distribution': 584,\n",
              " 'ready': 585,\n",
              " 'slides': 586,\n",
              " 'favorite': 587,\n",
              " 'step': 588,\n",
              " 'smart': 589,\n",
              " 'due': 590,\n",
              " 'maintained': 591,\n",
              " 'studies': 592,\n",
              " 'number': 593,\n",
              " 'inspiring': 594,\n",
              " 'both': 595,\n",
              " 'big': 596,\n",
              " 'textbooks': 597,\n",
              " 'unhelpful': 598,\n",
              " 'ended': 599,\n",
              " 'enthusiasm': 600,\n",
              " 'likes': 601,\n",
              " 'inclusive': 602,\n",
              " 'comfortable': 603,\n",
              " 'tells': 604,\n",
              " 'either': 605,\n",
              " 'causing': 606,\n",
              " '4': 607,\n",
              " 'essays': 608,\n",
              " 'lost': 609,\n",
              " 'working': 610,\n",
              " 'hands': 611,\n",
              " 'chemistry': 612,\n",
              " 'rather': 613,\n",
              " 'unfair': 614,\n",
              " 'evaluation': 615,\n",
              " 'insights': 616,\n",
              " 'expected': 617,\n",
              " 'let': 618,\n",
              " 'poor': 619,\n",
              " 'fostering': 620,\n",
              " 'coming': 621,\n",
              " 'chance': 622,\n",
              " 'whole': 623,\n",
              " 'doesnt': 624,\n",
              " 'demonstrates': 625,\n",
              " 'current': 626,\n",
              " 'mark': 627,\n",
              " 'quality': 628,\n",
              " 'glad': 629,\n",
              " 'myself': 630,\n",
              " 'memorize': 631,\n",
              " 'mean': 632,\n",
              " 'seem': 633,\n",
              " 'concise': 634,\n",
              " 'unorganized': 635,\n",
              " 'emails': 636,\n",
              " 'industry': 637,\n",
              " 'low': 638,\n",
              " 'active': 639,\n",
              " 'comes': 640,\n",
              " 'listen': 641,\n",
              " 'looking': 642,\n",
              " 'clarity': 643,\n",
              " 'certain': 644,\n",
              " 'having': 645,\n",
              " 'inconsistent': 646,\n",
              " 'solving': 647,\n",
              " 'theory': 648,\n",
              " 'styles': 649,\n",
              " 'wonderful': 650,\n",
              " 'consuming': 651,\n",
              " 'yes': 652,\n",
              " 'success': 653,\n",
              " 'mistakes': 654,\n",
              " 'enhance': 655,\n",
              " 'studying': 656,\n",
              " 'innovative': 657,\n",
              " 'perfect': 658,\n",
              " 'negative': 659,\n",
              " 'shes': 660,\n",
              " 'prepare': 661,\n",
              " 'diverse': 662,\n",
              " 'encouraging': 663,\n",
              " 'expect': 664,\n",
              " 'seemed': 665,\n",
              " 'completely': 666,\n",
              " 'written': 667,\n",
              " 'short': 668,\n",
              " 'gets': 669,\n",
              " 'jazz': 670,\n",
              " 'multiple': 671,\n",
              " 'concerns': 672,\n",
              " 'c': 673,\n",
              " 'although': 674,\n",
              " 'respectful': 675,\n",
              " 'here': 676,\n",
              " 'year': 677,\n",
              " 'board': 678,\n",
              " 'done': 679,\n",
              " 'exactly': 680,\n",
              " 'regarding': 681,\n",
              " 'works': 682,\n",
              " 'various': 683,\n",
              " 'trying': 684,\n",
              " 'expertise': 685,\n",
              " 'etc': 686,\n",
              " 'old': 687,\n",
              " 'themselves': 688,\n",
              " 'curves': 689,\n",
              " 'plus': 690,\n",
              " 'turn': 691,\n",
              " 'entertaining': 692,\n",
              " 'develop': 693,\n",
              " 'objective': 694,\n",
              " 'lacked': 695,\n",
              " 'usually': 696,\n",
              " 'hour': 697,\n",
              " 'term': 698,\n",
              " 'incredibly': 699,\n",
              " '6': 700,\n",
              " 'consistent': 701,\n",
              " 'especially': 702,\n",
              " 'curve': 703,\n",
              " 'reviews': 704,\n",
              " 'asked': 705,\n",
              " 'struggling': 706,\n",
              " 'detail': 707,\n",
              " 'interests': 708,\n",
              " 'college': 709,\n",
              " 'choose': 710,\n",
              " 'improved': 711,\n",
              " 'horrible': 712,\n",
              " 'understood': 713,\n",
              " 'held': 714,\n",
              " \"there's\": 715,\n",
              " 'reason': 716,\n",
              " 'why': 717,\n",
              " 'creating': 718,\n",
              " 'dr': 719,\n",
              " 'solid': 720,\n",
              " 'midterms': 721,\n",
              " 'dry': 722,\n",
              " 'engage': 723,\n",
              " 'lessons': 724,\n",
              " 'mandatory': 725,\n",
              " 'promoting': 726,\n",
              " 'set': 727,\n",
              " 'showed': 728,\n",
              " 'computer': 729,\n",
              " 'wait': 730,\n",
              " 'role': 731,\n",
              " 'speaking': 732,\n",
              " 'poorly': 733,\n",
              " 'wrong': 734,\n",
              " 'application': 735,\n",
              " 'satisfied': 736,\n",
              " \"weren't\": 737,\n",
              " 'weeks': 738,\n",
              " 'around': 739,\n",
              " 'related': 740,\n",
              " 'issues': 741,\n",
              " 'intellectual': 742,\n",
              " 'practicals': 743,\n",
              " 'changes': 744,\n",
              " 'shows': 745,\n",
              " \"aren't\": 746,\n",
              " 'date': 747,\n",
              " 'scores': 748,\n",
              " 'thanks': 749,\n",
              " 'chapters': 750,\n",
              " 'collaborative': 751,\n",
              " 'analysis': 752,\n",
              " 'resource': 753,\n",
              " 'focus': 754,\n",
              " 'along': 755,\n",
              " 'stupid': 756,\n",
              " 'money': 757,\n",
              " 'waste': 758,\n",
              " 'blog': 759,\n",
              " 'properly': 760,\n",
              " 'interact': 761,\n",
              " 'irrelevant': 762,\n",
              " 'future': 763,\n",
              " 'theoretical': 764,\n",
              " 'instructors': 765,\n",
              " 'genuine': 766,\n",
              " 'watch': 767,\n",
              " 'beginning': 768,\n",
              " 'explore': 769,\n",
              " 'summer': 770,\n",
              " 'quickly': 771,\n",
              " 'previous': 772,\n",
              " 'enjoy': 773,\n",
              " 'attitude': 774,\n",
              " 'progress': 775,\n",
              " 'allowed': 776,\n",
              " 'atmosphere': 777,\n",
              " 'technology': 778,\n",
              " 'critically': 779,\n",
              " 'place': 780,\n",
              " 'barrow': 781,\n",
              " 'hell': 782,\n",
              " 'text': 783,\n",
              " 'fails': 784,\n",
              " 'cannot': 785,\n",
              " 'mid': 786,\n",
              " 'human': 787,\n",
              " 'requires': 788,\n",
              " 'offered': 789,\n",
              " 'seriously': 790,\n",
              " 'half': 791,\n",
              " 'solutions': 792,\n",
              " 'programming': 793,\n",
              " 'performance': 794,\n",
              " 'useless': 795,\n",
              " \"what's\": 796,\n",
              " 'experienced': 797,\n",
              " 'top': 798,\n",
              " 'techniques': 799,\n",
              " 'presents': 800,\n",
              " 'involved': 801,\n",
              " 'incomplete': 802,\n",
              " 'entire': 803,\n",
              " 'pace': 804,\n",
              " 'repeat': 805,\n",
              " 'responsive': 806,\n",
              " 'forward': 807,\n",
              " 'basically': 808,\n",
              " 'actively': 809,\n",
              " 'showing': 810,\n",
              " 'incorporated': 811,\n",
              " 'small': 812,\n",
              " 'break': 813,\n",
              " 'focused': 814,\n",
              " 'unbiased': 815,\n",
              " 'growth': 816,\n",
              " 'days': 817,\n",
              " 'pop': 818,\n",
              " 'unique': 819,\n",
              " 'amount': 820,\n",
              " 'type': 821,\n",
              " 'receive': 822,\n",
              " 'appropriate': 823,\n",
              " 'engaged': 824,\n",
              " 'workload': 825,\n",
              " 'self': 826,\n",
              " 'form': 827,\n",
              " 'someone': 828,\n",
              " 'asking': 829,\n",
              " 'objectives': 830,\n",
              " 'motivated': 831,\n",
              " 'whether': 832,\n",
              " 'condescending': 833,\n",
              " 'management': 834,\n",
              " 'particular': 835,\n",
              " 'weekly': 836,\n",
              " 'power': 837,\n",
              " 'chinese': 838,\n",
              " 'please': 839,\n",
              " 'thorough': 840,\n",
              " 'schedule': 841,\n",
              " 'flexible': 842,\n",
              " 'honestly': 843,\n",
              " 'met': 844,\n",
              " 'videos': 845,\n",
              " 'mind': 846,\n",
              " 'side': 847,\n",
              " 'particularly': 848,\n",
              " 'covers': 849,\n",
              " 'rewarding': 850,\n",
              " 'music': 851,\n",
              " 'slow': 852,\n",
              " 'staff': 853,\n",
              " 'fall': 854,\n",
              " 'general': 855,\n",
              " '50': 856,\n",
              " 'articles': 857,\n",
              " 'value': 858,\n",
              " 'fantastic': 859,\n",
              " 'simply': 860,\n",
              " 'changed': 861,\n",
              " 'assigns': 862,\n",
              " 'words': 863,\n",
              " 'department': 864,\n",
              " 'policy': 865,\n",
              " 'straightforward': 866,\n",
              " 'difficulty': 867,\n",
              " 'opportunity': 868,\n",
              " 'cute': 869,\n",
              " 'increased': 870,\n",
              " 'spend': 871,\n",
              " 'respect': 872,\n",
              " 'engagement': 873,\n",
              " 'tried': 874,\n",
              " 'luck': 875,\n",
              " 'graded': 876,\n",
              " 'gpa': 877,\n",
              " 'unless': 878,\n",
              " 'structure': 879,\n",
              " 'track': 880,\n",
              " 'behavior': 881,\n",
              " 'ability': 882,\n",
              " 'presented': 883,\n",
              " 'trouble': 884,\n",
              " 'patient': 885,\n",
              " 'past': 886,\n",
              " 'word': 887,\n",
              " 'analyze': 888,\n",
              " 'testing': 889,\n",
              " 'situations': 890,\n",
              " 'delivering': 891,\n",
              " 'passing': 892,\n",
              " 'leading': 893,\n",
              " 'mastery': 894,\n",
              " 'otherwise': 895,\n",
              " 'calculus': 896,\n",
              " '100': 897,\n",
              " 'gained': 898,\n",
              " 'reach': 899,\n",
              " 'platform': 900,\n",
              " 'breaks': 901,\n",
              " 'whatever': 902,\n",
              " 'ta': 903,\n",
              " 'creativity': 904,\n",
              " 'insightful': 905,\n",
              " 'uninspiring': 906,\n",
              " 'areas': 907,\n",
              " 'sheet': 908,\n",
              " 'enhancing': 909,\n",
              " 'lady': 910,\n",
              " 'biased': 911,\n",
              " 'agree': 912,\n",
              " 'inappropriate': 913,\n",
              " \"couldn't\": 914,\n",
              " 'lives': 915,\n",
              " 'share': 916,\n",
              " 'relaxed': 917,\n",
              " 'ass': 918,\n",
              " \"he'll\": 919,\n",
              " 'confidence': 920,\n",
              " 'seek': 921,\n",
              " 'individual': 922,\n",
              " 'lacking': 923,\n",
              " 'consider': 924,\n",
              " 'single': 925,\n",
              " 'extensive': 926,\n",
              " 'actual': 927,\n",
              " 'section': 928,\n",
              " 'unprepared': 929,\n",
              " 'maintains': 930,\n",
              " 'second': 931,\n",
              " 'liked': 932,\n",
              " 'songs': 933,\n",
              " 'left': 934,\n",
              " 'explained': 935,\n",
              " \"wouldn't\": 936,\n",
              " 'theories': 937,\n",
              " 'hate': 938,\n",
              " 'mostly': 939,\n",
              " 'years': 940,\n",
              " 'above': 941,\n",
              " 'generally': 942,\n",
              " 'philosophy': 943,\n",
              " 'intimidating': 944,\n",
              " 'depending': 945,\n",
              " 'initially': 946,\n",
              " 'monotone': 947,\n",
              " 'understandable': 948,\n",
              " 'change': 949,\n",
              " 'offers': 950,\n",
              " 'visual': 951,\n",
              " 'key': 952,\n",
              " 'efforts': 953,\n",
              " 'wanted': 954,\n",
              " 'fostered': 955,\n",
              " 'collaboration': 956,\n",
              " 'visually': 957,\n",
              " 'answered': 958,\n",
              " 'suggest': 959,\n",
              " 'equations': 960,\n",
              " 'curriculum': 961,\n",
              " 'last': 962,\n",
              " 's': 963,\n",
              " 'allows': 964,\n",
              " 'keeps': 965,\n",
              " 'model': 966,\n",
              " 'public': 967,\n",
              " 'remember': 968,\n",
              " 'drop': 969,\n",
              " 'unapproachable': 970,\n",
              " 'hope': 971,\n",
              " 'figure': 972,\n",
              " '15': 973,\n",
              " 'herself': 974,\n",
              " 'state': 975,\n",
              " 'delivered': 976,\n",
              " 'variety': 977,\n",
              " 'keeping': 978,\n",
              " 'loves': 979,\n",
              " 'deeper': 980,\n",
              " 'passed': 981,\n",
              " 'challenge': 982,\n",
              " 'limited': 983,\n",
              " 'opinion': 984,\n",
              " 'attending': 985,\n",
              " 'saying': 986,\n",
              " 'balanced': 987,\n",
              " 'full': 988,\n",
              " 'elements': 989,\n",
              " 'least': 990,\n",
              " 'main': 991,\n",
              " 'according': 992,\n",
              " 'standards': 993,\n",
              " 'thinks': 994,\n",
              " 'manageable': 995,\n",
              " 'dedication': 996,\n",
              " 'himself': 997,\n",
              " 'appears': 998,\n",
              " 'addressing': 999,\n",
              " 'true': 1000,\n",
              " ...}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "    ### START CODE HERE\n",
        "       \n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the correct padding, truncating and maxlen\n",
        "    pad_trunc_sequences = pad_sequences(sequences, maxlen= MAXLEN, padding = PADDING, truncating = TRUNCATING)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return pad_trunc_sequences"
      ],
      "metadata": {
        "id": "xUkJFtiwh7C-"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "\n",
        "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
        "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biCCj27uh-Ve",
        "outputId": "69db4283-07d2-401c-8799-cdcb109af86b"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Padded and truncated training sequences have shape: (2700, 16)\n",
            "\n",
            "Padded and truncated validation sequences have shape: (300, 16)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)"
      ],
      "metadata": {
        "id": "OR3wfthriCnN"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Using pre-defined Embeddings"
      ],
      "metadata": {
        "id": "j0j16iy7iI-m"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define path to file containing the embeddings\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "\n",
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ],
      "metadata": {
        "id": "tUHybcFMiE0d"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Represent the words in your vocabulary using the embeddings"
      ],
      "metadata": {
        "id": "ldaN0eawl5v5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize an empty numpy array with the appropriate size\n",
        "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
        "\n",
        "# Iterate all of the words in the vocabulary and if the vector representation for \n",
        "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        EMBEDDINGS_MATRIX[i] = embedding_vector"
      ],
      "metadata": {
        "id": "Rokt-fwRl8NB"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define a model that does not overfit"
      ],
      "metadata": {
        "id": "HZjjYxbzl8ga"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with 0.001 learning rate"
      ],
      "metadata": {
        "id": "QPn--pNsDhEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "#     model = tf.keras.Sequential([ \n",
        "#         # This is how you need to set the Embedding layer when using pre-trained embeddings\n",
        "#         tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "#         tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
        "#         tf.keras.layers.GlobalMaxPooling1D(),\n",
        "#         tf.keras.layers.Dropout(0.2),\n",
        "#         tf.keras.layers.Dense(32, activation='relu'),\n",
        "#         tf.keras.layers.Dense(3, activation='softmax'),\n",
        "#     ])\n",
        "    \n",
        "#     model.compile(loss='sparse_categorical_crossentropy',\n",
        "#                   optimizer='adam',\n",
        "#                   metrics=['accuracy']) \n",
        "#     return model"
      ],
      "metadata": {
        "id": "HAWbMeF-mBg5"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with 0.002 learning rate"
      ],
      "metadata": {
        "id": "_iIk4pY1DcZd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])\n",
        "    optimizer = optimizers.Adam(learning_rate = 0.002)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy']) \n",
        "    return model\n"
      ],
      "metadata": {
        "id": "eLywL_aR9KSf"
      },
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model with 0.0025 learning rate"
      ],
      "metadata": {
        "id": "ewJcvwnoIwCh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# from tensorflow.keras import optimizers\n",
        "\n",
        "# def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "#         tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "#         tf.keras.layers.Dropout(0.2),\n",
        "#         tf.keras.layers.Dense(3, activation='softmax'),\n",
        "#     ])\n",
        "#     optimizer = optimizers.Adam(learning_rate = 0.0025)\n",
        "#     model.compile(loss='sparse_categorical_crossentropy',\n",
        "#                   optimizer=optimizer,\n",
        "#                   metrics=['accuracy']) \n",
        "#     return model\n"
      ],
      "metadata": {
        "id": "BB31XCglDbsW"
      },
      "execution_count": 42,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)"
      ],
      "metadata": {
        "id": "ePMfJY5UKPB4"
      },
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZnbOCPTQ_QK",
        "outputId": "554dcb06-7052-40bf-d863-8e3894f4f1e0"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding (Embedding)       (None, 16, 100)           395700    \n",
            "                                                                 \n",
            " conv1d (Conv1D)             (None, 14, 64)            19264     \n",
            "                                                                 \n",
            " max_pooling1d (MaxPooling1D  (None, 7, 64)            0         \n",
            " )                                                               \n",
            "                                                                 \n",
            " bidirectional (Bidirectiona  (None, 7, 128)           49920     \n",
            " l)                                                              \n",
            "                                                                 \n",
            " bidirectional_1 (Bidirectio  (None, 64)               31104     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense (Dense)               (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout (Dropout)           (None, 64)                0         \n",
            "                                                                 \n",
            " dense_1 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 500,343\n",
            "Trainable params: 104,643\n",
            "Non-trainable params: 395,700\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Train the model and save the training history\n",
        "history = model.fit(train_pad_trunc_seq, train_labels, epochs=200, validation_data=(val_pad_trunc_seq, val_labels))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArRf_nFcmEjZ",
        "outputId": "bdb07760-6d59-4e26-889a-ce73022480b2"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "85/85 [==============================] - 13s 43ms/step - loss: 0.9287 - accuracy: 0.5774 - val_loss: 0.8106 - val_accuracy: 0.6567\n",
            "Epoch 2/200\n",
            "85/85 [==============================] - 2s 27ms/step - loss: 0.7305 - accuracy: 0.6844 - val_loss: 0.7573 - val_accuracy: 0.6667\n",
            "Epoch 3/200\n",
            "85/85 [==============================] - 3s 38ms/step - loss: 0.6150 - accuracy: 0.7541 - val_loss: 0.6918 - val_accuracy: 0.7233\n",
            "Epoch 4/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.5117 - accuracy: 0.7974 - val_loss: 0.7070 - val_accuracy: 0.7200\n",
            "Epoch 5/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.4139 - accuracy: 0.8485 - val_loss: 0.8057 - val_accuracy: 0.7100\n",
            "Epoch 6/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.3303 - accuracy: 0.8796 - val_loss: 0.7954 - val_accuracy: 0.7233\n",
            "Epoch 7/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.2474 - accuracy: 0.9104 - val_loss: 0.9859 - val_accuracy: 0.7167\n",
            "Epoch 8/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.1910 - accuracy: 0.9293 - val_loss: 1.0171 - val_accuracy: 0.7400\n",
            "Epoch 9/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.1770 - accuracy: 0.9370 - val_loss: 1.2214 - val_accuracy: 0.7167\n",
            "Epoch 10/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.1432 - accuracy: 0.9511 - val_loss: 1.1763 - val_accuracy: 0.7033\n",
            "Epoch 11/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.1216 - accuracy: 0.9585 - val_loss: 1.2041 - val_accuracy: 0.7133\n",
            "Epoch 12/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0924 - accuracy: 0.9689 - val_loss: 1.3812 - val_accuracy: 0.7233\n",
            "Epoch 13/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0730 - accuracy: 0.9748 - val_loss: 1.3977 - val_accuracy: 0.7233\n",
            "Epoch 14/200\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.1065 - accuracy: 0.9670 - val_loss: 1.4917 - val_accuracy: 0.6533\n",
            "Epoch 15/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0852 - accuracy: 0.9689 - val_loss: 1.3554 - val_accuracy: 0.7067\n",
            "Epoch 16/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0560 - accuracy: 0.9830 - val_loss: 1.4275 - val_accuracy: 0.7067\n",
            "Epoch 17/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0352 - accuracy: 0.9870 - val_loss: 2.0228 - val_accuracy: 0.6700\n",
            "Epoch 18/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0463 - accuracy: 0.9841 - val_loss: 1.6808 - val_accuracy: 0.7000\n",
            "Epoch 19/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0588 - accuracy: 0.9804 - val_loss: 1.6396 - val_accuracy: 0.7167\n",
            "Epoch 20/200\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.0251 - accuracy: 0.9926 - val_loss: 1.9702 - val_accuracy: 0.7067\n",
            "Epoch 21/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0221 - accuracy: 0.9922 - val_loss: 1.9893 - val_accuracy: 0.7000\n",
            "Epoch 22/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0260 - accuracy: 0.9926 - val_loss: 1.9207 - val_accuracy: 0.7200\n",
            "Epoch 23/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0732 - accuracy: 0.9744 - val_loss: 1.7340 - val_accuracy: 0.7067\n",
            "Epoch 24/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.1186 - accuracy: 0.9589 - val_loss: 1.4678 - val_accuracy: 0.7433\n",
            "Epoch 25/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0596 - accuracy: 0.9793 - val_loss: 1.6083 - val_accuracy: 0.7333\n",
            "Epoch 26/200\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.0674 - accuracy: 0.9793 - val_loss: 1.5029 - val_accuracy: 0.7467\n",
            "Epoch 27/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0164 - accuracy: 0.9926 - val_loss: 1.8046 - val_accuracy: 0.7200\n",
            "Epoch 28/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0156 - accuracy: 0.9933 - val_loss: 2.1451 - val_accuracy: 0.6967\n",
            "Epoch 29/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0344 - accuracy: 0.9893 - val_loss: 1.7118 - val_accuracy: 0.6767\n",
            "Epoch 30/200\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.0284 - accuracy: 0.9893 - val_loss: 1.7524 - val_accuracy: 0.7267\n",
            "Epoch 31/200\n",
            "85/85 [==============================] - 3s 38ms/step - loss: 0.0351 - accuracy: 0.9856 - val_loss: 1.6246 - val_accuracy: 0.7400\n",
            "Epoch 32/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0498 - accuracy: 0.9819 - val_loss: 1.6834 - val_accuracy: 0.7500\n",
            "Epoch 33/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0375 - accuracy: 0.9867 - val_loss: 1.9708 - val_accuracy: 0.6967\n",
            "Epoch 34/200\n",
            "85/85 [==============================] - 3s 29ms/step - loss: 0.0186 - accuracy: 0.9937 - val_loss: 1.7625 - val_accuracy: 0.7467\n",
            "Epoch 35/200\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.0103 - accuracy: 0.9952 - val_loss: 1.8532 - val_accuracy: 0.7367\n",
            "Epoch 36/200\n",
            "85/85 [==============================] - 3s 38ms/step - loss: 0.0091 - accuracy: 0.9959 - val_loss: 1.9943 - val_accuracy: 0.7267\n",
            "Epoch 37/200\n",
            "85/85 [==============================] - 2s 27ms/step - loss: 0.0075 - accuracy: 0.9956 - val_loss: 2.0084 - val_accuracy: 0.7467\n",
            "Epoch 38/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0077 - accuracy: 0.9941 - val_loss: 2.0572 - val_accuracy: 0.7433\n",
            "Epoch 39/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 2.1079 - val_accuracy: 0.7433\n",
            "Epoch 40/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0068 - accuracy: 0.9963 - val_loss: 2.1406 - val_accuracy: 0.7433\n",
            "Epoch 41/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0078 - accuracy: 0.9959 - val_loss: 2.1911 - val_accuracy: 0.7367\n",
            "Epoch 42/200\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.0070 - accuracy: 0.9944 - val_loss: 2.1768 - val_accuracy: 0.7400\n",
            "Epoch 43/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0062 - accuracy: 0.9956 - val_loss: 2.2094 - val_accuracy: 0.7367\n",
            "Epoch 44/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0068 - accuracy: 0.9952 - val_loss: 2.1906 - val_accuracy: 0.7467\n",
            "Epoch 45/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0070 - accuracy: 0.9941 - val_loss: 2.2424 - val_accuracy: 0.7333\n",
            "Epoch 46/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0071 - accuracy: 0.9941 - val_loss: 2.2439 - val_accuracy: 0.7333\n",
            "Epoch 47/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 2.2674 - val_accuracy: 0.7400\n",
            "Epoch 48/200\n",
            "85/85 [==============================] - 3s 33ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.2965 - val_accuracy: 0.7433\n",
            "Epoch 49/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 2.3157 - val_accuracy: 0.7367\n",
            "Epoch 50/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.3264 - val_accuracy: 0.7433\n",
            "Epoch 51/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.3545 - val_accuracy: 0.7433\n",
            "Epoch 52/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.3755 - val_accuracy: 0.7433\n",
            "Epoch 53/200\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.4057 - val_accuracy: 0.7367\n",
            "Epoch 54/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.4021 - val_accuracy: 0.7400\n",
            "Epoch 55/200\n",
            "85/85 [==============================] - 2s 28ms/step - loss: 0.0065 - accuracy: 0.9956 - val_loss: 2.4112 - val_accuracy: 0.7400\n",
            "Epoch 56/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.4317 - val_accuracy: 0.7433\n",
            "Epoch 57/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.4429 - val_accuracy: 0.7433\n",
            "Epoch 58/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.4384 - val_accuracy: 0.7467\n",
            "Epoch 59/200\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.4442 - val_accuracy: 0.7500\n",
            "Epoch 60/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9956 - val_loss: 2.4510 - val_accuracy: 0.7433\n",
            "Epoch 61/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9952 - val_loss: 2.4683 - val_accuracy: 0.7500\n",
            "Epoch 62/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9952 - val_loss: 2.5018 - val_accuracy: 0.7467\n",
            "Epoch 63/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.5183 - val_accuracy: 0.7500\n",
            "Epoch 64/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 2.5128 - val_accuracy: 0.7500\n",
            "Epoch 65/200\n",
            "85/85 [==============================] - 3s 34ms/step - loss: 0.0068 - accuracy: 0.9963 - val_loss: 2.4986 - val_accuracy: 0.7500\n",
            "Epoch 66/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.4994 - val_accuracy: 0.7433\n",
            "Epoch 67/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.5046 - val_accuracy: 0.7433\n",
            "Epoch 68/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.4934 - val_accuracy: 0.7500\n",
            "Epoch 69/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0071 - accuracy: 0.9959 - val_loss: 2.5368 - val_accuracy: 0.7300\n",
            "Epoch 70/200\n",
            "85/85 [==============================] - 3s 36ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.4976 - val_accuracy: 0.7400\n",
            "Epoch 71/200\n",
            "85/85 [==============================] - 2s 29ms/step - loss: 0.0082 - accuracy: 0.9956 - val_loss: 2.5054 - val_accuracy: 0.7267\n",
            "Epoch 72/200\n",
            "85/85 [==============================] - 2s 23ms/step - loss: 0.3665 - accuracy: 0.8752 - val_loss: 1.0715 - val_accuracy: 0.7100\n",
            "Epoch 73/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.2018 - accuracy: 0.9304 - val_loss: 1.2044 - val_accuracy: 0.7200\n",
            "Epoch 74/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.1059 - accuracy: 0.9633 - val_loss: 1.3268 - val_accuracy: 0.7000\n",
            "Epoch 75/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0853 - accuracy: 0.9689 - val_loss: 1.4565 - val_accuracy: 0.7000\n",
            "Epoch 76/200\n",
            "85/85 [==============================] - 4s 47ms/step - loss: 0.0305 - accuracy: 0.9889 - val_loss: 1.6140 - val_accuracy: 0.7200\n",
            "Epoch 77/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0222 - accuracy: 0.9896 - val_loss: 1.8164 - val_accuracy: 0.7133\n",
            "Epoch 78/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0300 - accuracy: 0.9893 - val_loss: 1.8241 - val_accuracy: 0.7200\n",
            "Epoch 79/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0210 - accuracy: 0.9919 - val_loss: 2.1567 - val_accuracy: 0.6967\n",
            "Epoch 80/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0294 - accuracy: 0.9889 - val_loss: 1.8988 - val_accuracy: 0.7133\n",
            "Epoch 81/200\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.0344 - accuracy: 0.9878 - val_loss: 1.8598 - val_accuracy: 0.7400\n",
            "Epoch 82/200\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.0173 - accuracy: 0.9885 - val_loss: 2.0617 - val_accuracy: 0.7300\n",
            "Epoch 83/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0192 - accuracy: 0.9907 - val_loss: 2.2114 - val_accuracy: 0.7267\n",
            "Epoch 84/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0433 - accuracy: 0.9863 - val_loss: 1.9104 - val_accuracy: 0.7067\n",
            "Epoch 85/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0344 - accuracy: 0.9881 - val_loss: 2.0295 - val_accuracy: 0.7267\n",
            "Epoch 86/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0122 - accuracy: 0.9941 - val_loss: 1.8706 - val_accuracy: 0.7267\n",
            "Epoch 87/200\n",
            "85/85 [==============================] - 4s 42ms/step - loss: 0.0090 - accuracy: 0.9948 - val_loss: 2.0139 - val_accuracy: 0.7200\n",
            "Epoch 88/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0072 - accuracy: 0.9959 - val_loss: 2.2597 - val_accuracy: 0.7267\n",
            "Epoch 89/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9959 - val_loss: 2.2268 - val_accuracy: 0.7333\n",
            "Epoch 90/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0061 - accuracy: 0.9959 - val_loss: 2.2994 - val_accuracy: 0.7300\n",
            "Epoch 91/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9963 - val_loss: 2.3084 - val_accuracy: 0.7300\n",
            "Epoch 92/200\n",
            "85/85 [==============================] - 3s 33ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.3403 - val_accuracy: 0.7300\n",
            "Epoch 93/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.0062 - accuracy: 0.9959 - val_loss: 2.3816 - val_accuracy: 0.7300\n",
            "Epoch 94/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9967 - val_loss: 2.4188 - val_accuracy: 0.7200\n",
            "Epoch 95/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0103 - accuracy: 0.9948 - val_loss: 2.1623 - val_accuracy: 0.7133\n",
            "Epoch 96/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9959 - val_loss: 2.3625 - val_accuracy: 0.7233\n",
            "Epoch 97/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.0062 - accuracy: 0.9952 - val_loss: 2.4009 - val_accuracy: 0.7267\n",
            "Epoch 98/200\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.0061 - accuracy: 0.9959 - val_loss: 2.4202 - val_accuracy: 0.7200\n",
            "Epoch 99/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.4579 - val_accuracy: 0.7167\n",
            "Epoch 100/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0059 - accuracy: 0.9963 - val_loss: 2.5018 - val_accuracy: 0.7233\n",
            "Epoch 101/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.4904 - val_accuracy: 0.7300\n",
            "Epoch 102/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0061 - accuracy: 0.9963 - val_loss: 2.5230 - val_accuracy: 0.7233\n",
            "Epoch 103/200\n",
            "85/85 [==============================] - 3s 34ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.5582 - val_accuracy: 0.7200\n",
            "Epoch 104/200\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.0064 - accuracy: 0.9967 - val_loss: 2.5947 - val_accuracy: 0.7167\n",
            "Epoch 105/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0060 - accuracy: 0.9952 - val_loss: 2.6139 - val_accuracy: 0.7200\n",
            "Epoch 106/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9952 - val_loss: 2.6043 - val_accuracy: 0.7233\n",
            "Epoch 107/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9963 - val_loss: 2.6337 - val_accuracy: 0.7167\n",
            "Epoch 108/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.6371 - val_accuracy: 0.7133\n",
            "Epoch 109/200\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.6471 - val_accuracy: 0.7167\n",
            "Epoch 110/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.6459 - val_accuracy: 0.7200\n",
            "Epoch 111/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.6083 - val_accuracy: 0.7133\n",
            "Epoch 112/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0071 - accuracy: 0.9959 - val_loss: 2.7070 - val_accuracy: 0.7200\n",
            "Epoch 113/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.6873 - val_accuracy: 0.7233\n",
            "Epoch 114/200\n",
            "85/85 [==============================] - 2s 28ms/step - loss: 0.0062 - accuracy: 0.9956 - val_loss: 2.7277 - val_accuracy: 0.7200\n",
            "Epoch 115/200\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.0063 - accuracy: 0.9970 - val_loss: 2.7301 - val_accuracy: 0.7233\n",
            "Epoch 116/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0066 - accuracy: 0.9967 - val_loss: 2.7451 - val_accuracy: 0.7233\n",
            "Epoch 117/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0074 - accuracy: 0.9963 - val_loss: 2.7166 - val_accuracy: 0.7200\n",
            "Epoch 118/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9956 - val_loss: 2.6073 - val_accuracy: 0.7467\n",
            "Epoch 119/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0109 - accuracy: 0.9956 - val_loss: 2.6887 - val_accuracy: 0.7133\n",
            "Epoch 120/200\n",
            "85/85 [==============================] - 3s 36ms/step - loss: 0.0073 - accuracy: 0.9956 - val_loss: 3.1216 - val_accuracy: 0.7033\n",
            "Epoch 121/200\n",
            "85/85 [==============================] - 2s 28ms/step - loss: 0.2136 - accuracy: 0.9319 - val_loss: 1.2136 - val_accuracy: 0.6967\n",
            "Epoch 122/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.1233 - accuracy: 0.9585 - val_loss: 1.6088 - val_accuracy: 0.7100\n",
            "Epoch 123/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0720 - accuracy: 0.9733 - val_loss: 1.8086 - val_accuracy: 0.7133\n",
            "Epoch 124/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0249 - accuracy: 0.9900 - val_loss: 1.8741 - val_accuracy: 0.7167\n",
            "Epoch 125/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0384 - accuracy: 0.9863 - val_loss: 1.9064 - val_accuracy: 0.7200\n",
            "Epoch 126/200\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.0124 - accuracy: 0.9941 - val_loss: 2.1575 - val_accuracy: 0.7167\n",
            "Epoch 127/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0089 - accuracy: 0.9941 - val_loss: 2.1980 - val_accuracy: 0.7033\n",
            "Epoch 128/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0080 - accuracy: 0.9959 - val_loss: 2.3019 - val_accuracy: 0.7167\n",
            "Epoch 129/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0069 - accuracy: 0.9959 - val_loss: 2.3464 - val_accuracy: 0.7133\n",
            "Epoch 130/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0070 - accuracy: 0.9956 - val_loss: 2.3169 - val_accuracy: 0.6933\n",
            "Epoch 131/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.0067 - accuracy: 0.9959 - val_loss: 2.3294 - val_accuracy: 0.6967\n",
            "Epoch 132/200\n",
            "85/85 [==============================] - 3s 33ms/step - loss: 0.0068 - accuracy: 0.9963 - val_loss: 2.3541 - val_accuracy: 0.6967\n",
            "Epoch 133/200\n",
            "85/85 [==============================] - 2s 27ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.3887 - val_accuracy: 0.6967\n",
            "Epoch 134/200\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.4110 - val_accuracy: 0.6933\n",
            "Epoch 135/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9952 - val_loss: 2.4322 - val_accuracy: 0.6933\n",
            "Epoch 136/200\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.0062 - accuracy: 0.9967 - val_loss: 2.4517 - val_accuracy: 0.6967\n",
            "Epoch 137/200\n",
            "85/85 [==============================] - 3s 31ms/step - loss: 0.0063 - accuracy: 0.9959 - val_loss: 2.4832 - val_accuracy: 0.6967\n",
            "Epoch 138/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9952 - val_loss: 2.4909 - val_accuracy: 0.6967\n",
            "Epoch 139/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 2.5277 - val_accuracy: 0.7000\n",
            "Epoch 140/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9967 - val_loss: 2.5568 - val_accuracy: 0.7067\n",
            "Epoch 141/200\n",
            "85/85 [==============================] - 2s 27ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.5695 - val_accuracy: 0.7000\n",
            "Epoch 142/200\n",
            "85/85 [==============================] - 3s 39ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.5890 - val_accuracy: 0.7167\n",
            "Epoch 143/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.5897 - val_accuracy: 0.7133\n",
            "Epoch 144/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.6160 - val_accuracy: 0.7133\n",
            "Epoch 145/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0063 - accuracy: 0.9948 - val_loss: 2.6436 - val_accuracy: 0.7133\n",
            "Epoch 146/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9963 - val_loss: 2.6237 - val_accuracy: 0.7067\n",
            "Epoch 147/200\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.6211 - val_accuracy: 0.7000\n",
            "Epoch 148/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.6494 - val_accuracy: 0.7000\n",
            "Epoch 149/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.6607 - val_accuracy: 0.7000\n",
            "Epoch 150/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.6952 - val_accuracy: 0.7000\n",
            "Epoch 151/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0062 - accuracy: 0.9959 - val_loss: 2.6983 - val_accuracy: 0.7000\n",
            "Epoch 152/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0066 - accuracy: 0.9959 - val_loss: 2.6994 - val_accuracy: 0.7067\n",
            "Epoch 153/200\n",
            "85/85 [==============================] - 3s 38ms/step - loss: 0.0062 - accuracy: 0.9956 - val_loss: 2.7298 - val_accuracy: 0.7100\n",
            "Epoch 154/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.7547 - val_accuracy: 0.7100\n",
            "Epoch 155/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.7746 - val_accuracy: 0.7100\n",
            "Epoch 156/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.7811 - val_accuracy: 0.7133\n",
            "Epoch 157/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.0061 - accuracy: 0.9967 - val_loss: 2.7886 - val_accuracy: 0.7133\n",
            "Epoch 158/200\n",
            "85/85 [==============================] - 3s 40ms/step - loss: 0.0071 - accuracy: 0.9963 - val_loss: 2.7531 - val_accuracy: 0.7100\n",
            "Epoch 159/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0078 - accuracy: 0.9959 - val_loss: 2.5221 - val_accuracy: 0.7167\n",
            "Epoch 160/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.1597 - accuracy: 0.9456 - val_loss: 1.2400 - val_accuracy: 0.6967\n",
            "Epoch 161/200\n",
            "85/85 [==============================] - 2s 26ms/step - loss: 0.1526 - accuracy: 0.9444 - val_loss: 1.3350 - val_accuracy: 0.7067\n",
            "Epoch 162/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0812 - accuracy: 0.9726 - val_loss: 1.5903 - val_accuracy: 0.7067\n",
            "Epoch 163/200\n",
            "85/85 [==============================] - 2s 29ms/step - loss: 0.0449 - accuracy: 0.9822 - val_loss: 1.5637 - val_accuracy: 0.7033\n",
            "Epoch 164/200\n",
            "85/85 [==============================] - 3s 37ms/step - loss: 0.0316 - accuracy: 0.9878 - val_loss: 1.8824 - val_accuracy: 0.7300\n",
            "Epoch 165/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0488 - accuracy: 0.9811 - val_loss: 1.8376 - val_accuracy: 0.7000\n",
            "Epoch 166/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0340 - accuracy: 0.9867 - val_loss: 2.0435 - val_accuracy: 0.7000\n",
            "Epoch 167/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0091 - accuracy: 0.9959 - val_loss: 2.0594 - val_accuracy: 0.7233\n",
            "Epoch 168/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0079 - accuracy: 0.9948 - val_loss: 2.1250 - val_accuracy: 0.7200\n",
            "Epoch 169/200\n",
            "85/85 [==============================] - 3s 35ms/step - loss: 0.0073 - accuracy: 0.9956 - val_loss: 2.1824 - val_accuracy: 0.7167\n",
            "Epoch 170/200\n",
            "85/85 [==============================] - 3s 30ms/step - loss: 0.0073 - accuracy: 0.9956 - val_loss: 2.2206 - val_accuracy: 0.7200\n",
            "Epoch 171/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9959 - val_loss: 2.2476 - val_accuracy: 0.7200\n",
            "Epoch 172/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.2764 - val_accuracy: 0.7167\n",
            "Epoch 173/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.3060 - val_accuracy: 0.7200\n",
            "Epoch 174/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9959 - val_loss: 2.3476 - val_accuracy: 0.7200\n",
            "Epoch 175/200\n",
            "85/85 [==============================] - 4s 41ms/step - loss: 0.0063 - accuracy: 0.9963 - val_loss: 2.3634 - val_accuracy: 0.7200\n",
            "Epoch 176/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9959 - val_loss: 2.3796 - val_accuracy: 0.7133\n",
            "Epoch 177/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9959 - val_loss: 2.3900 - val_accuracy: 0.7133\n",
            "Epoch 178/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.4091 - val_accuracy: 0.7133\n",
            "Epoch 179/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.4251 - val_accuracy: 0.7133\n",
            "Epoch 180/200\n",
            "85/85 [==============================] - 2s 29ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.4405 - val_accuracy: 0.7133\n",
            "Epoch 181/200\n",
            "85/85 [==============================] - 3s 36ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.4619 - val_accuracy: 0.7100\n",
            "Epoch 182/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.4793 - val_accuracy: 0.7100\n",
            "Epoch 183/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.4970 - val_accuracy: 0.7167\n",
            "Epoch 184/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.5122 - val_accuracy: 0.7067\n",
            "Epoch 185/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.5335 - val_accuracy: 0.7133\n",
            "Epoch 186/200\n",
            "85/85 [==============================] - 3s 36ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.5370 - val_accuracy: 0.7133\n",
            "Epoch 187/200\n",
            "85/85 [==============================] - 2s 29ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.5458 - val_accuracy: 0.7133\n",
            "Epoch 188/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9959 - val_loss: 2.5415 - val_accuracy: 0.7133\n",
            "Epoch 189/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9959 - val_loss: 2.5516 - val_accuracy: 0.7167\n",
            "Epoch 190/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.5460 - val_accuracy: 0.7133\n",
            "Epoch 191/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 2.5874 - val_accuracy: 0.7033\n",
            "Epoch 192/200\n",
            "85/85 [==============================] - 3s 41ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.5933 - val_accuracy: 0.7033\n",
            "Epoch 193/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0063 - accuracy: 0.9963 - val_loss: 2.6046 - val_accuracy: 0.7167\n",
            "Epoch 194/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.6168 - val_accuracy: 0.7133\n",
            "Epoch 195/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.6359 - val_accuracy: 0.7133\n",
            "Epoch 196/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.6583 - val_accuracy: 0.7067\n",
            "Epoch 197/200\n",
            "85/85 [==============================] - 3s 32ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.6790 - val_accuracy: 0.7100\n",
            "Epoch 198/200\n",
            "85/85 [==============================] - 3s 33ms/step - loss: 0.0062 - accuracy: 0.9956 - val_loss: 2.7211 - val_accuracy: 0.7033\n",
            "Epoch 199/200\n",
            "85/85 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.7166 - val_accuracy: 0.7067\n",
            "Epoch 200/200\n",
            "85/85 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9959 - val_loss: 2.7174 - val_accuracy: 0.7167\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = model.predict(val_pad_trunc_seq)\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(val_labels, val_predicted_labels, average='weighted')\n",
        "recall = recall_score(val_labels, val_predicted_labels, average='weighted')\n",
        "f1 = f1_score(val_labels, val_predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icfjRpZh0Pao",
        "outputId": "85d638fa-3189-4c26-a4e1-fe09771b70b5"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10/10 [==============================] - 2s 12ms/step\n",
            "Precision:  0.7151656513151581\n",
            "Recall:  0.7166666666666667\n",
            "F1-score:  0.7143859527664495\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Based on the evaluation metrics, our model performs relatively well with relatively high values of precision, recall, and F1-score"
      ],
      "metadata": {
        "id": "Z8LTrwH3183K"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Stopwords"
      ],
      "metadata": {
        "id": "tzYcQXQFaKgv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# opening the file in read mode\n",
        "my_file = open(\"stopwords.txt\", \"r\")\n",
        "  \n",
        "# reading the file\n",
        "data = my_file.read()\n",
        "  \n",
        "# replacing end splitting the text \n",
        "# when newline ('\\n') is seen.\n",
        "stopwords_data = data.split(\"\\n\")\n",
        "print(stopwords_data)\n",
        "my_file.close()"
      ],
      "metadata": {
        "id": "yoi7tX7ZaFcK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def remove_punctuation(text):\n",
        "    # Remove punctuation marks\n",
        "    translator = str.maketrans('', '', string.punctuation)\n",
        "    text_without_punctuation = text.translate(translator)\n",
        "    return text_without_punctuation"
      ],
      "metadata": {
        "id": "Lp2wK3_7aNgJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cleaned_text = []\n",
        "for i in range(len(df)):\n",
        "  cleaned_text.append(remove_punctuation(df['reviews'][i]))\n",
        "\n",
        "df['reviews'] = cleaned_text\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "id": "6VGvjRW5aPAX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def remove_stopwords(sentence, data):\n",
        "    # List of stopwords\n",
        "\n",
        "    stopwords = data + [\"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n",
        "    numbers_stopwords = [\"1\", \"2\", \"3\", \"4\",\"5\",\"6\",\"7\",\"8\",\"9\",\"10\",\n",
        "                         \"one\", \"two\",\"three\",\"four\",\"five\",\"098\"]\n",
        "    more_words = [\"didn't\", \"don't\", \"dont\", \"didnt\", \"it\", \"doesnt\", \"doesn't\", \"hw\",\"won't\",\"lpu\",\"weren't\",\"mr\",\"mcq\",\"shes\",\n",
        "                  \"shes\",\"india\",\"in\",\"hes\",\"shes\",\"me\", \"dr\", \"nlandu\", \"ko\",\"it\",\"1st\", \"omr\", \"ha\", \"upto\",\"ca\", \"soo\", \"cd\", \"ive\",\"po\",\"cse\", \"chem\", \"un\",\"of\",\n",
        "                  \"mte\", \"omr\",\"mte's\",\"ca's\",\"ete's\",\"jnv\",\"ip\",\"sir\",\"its\",\"wks\",\"prob\",\"python\",\"java\",\"lattc\",\"ol\",\"ived\",\"elsewhere\", \"mother\",\"wouldnt\",\"car\",\n",
        "                  \"si\", \"sat\",\"we\",\"home\",\"hot\",\"god\",\"ice\",\"money's\",\"money\",\"even\",\"about\",\"thats\", \"wks\", \"thurs\", \"months\", \"sir\", \"go\", \"jnv\", \"ip\", \"today\", \"today's\", \"linux\", \"github\",\n",
        "                  \"lt\", \"ums\", \"superb\", \"at\", \"cgpa\",\"ques\", \"brain's\", \"mcqs\", \"ve\", \"say\", \"pc\", \"viva\", \"after\", \"before\", \"draw\", \"asst\", \"only\", \"rich\", \"never\", \"went\", \"pcs\", \"gk\", \"one's\",\n",
        "                  \"co\", \"duty\", \"gona\", \"attendnce\",\"same\", \"that's\", \"hahahah\", \"ad's\", \"university's\", \"relly\", \"build\", \"cricket\", \"said\", \"hall\", \"profs\", \"guy's\", \"can\", \"along\", \"archieve\", \"bag\",\n",
        "                  \"part\", \"master\", \"push\", \"or\", \"add\", \"were\", \"virginia\",\"human\", \"bless\", \"clean\", \"count\", \"onlineopen\", \"ounce\", \"brushing\", \"zero\", \"mail\", \"fys\", \"lowell\", \"stets\", \"untill\", \"until\",\n",
        "                  \"prep\", \"appears\", \"giulia\", \"yuk\", \"memo\", \"ton\", \"110q\", \"unit\", \"80\",\"re\",\"by\",\"order\",\"fob\", \"sit\", \"from\",\"art\", \"org\", \"4d\", \"3d\", \"cinema\", \"iii\", \"cal\", \"both\", \"sundays\", \"todays\", \"ad\",\n",
        "                  \"yoursel\",\"yourself\", \"kiss\", \"it'll\", \"obayani's\", \"anal\", \"pgs\", \"csci\", \"hw\", \"more\", \"able\", \"lecturer\", \"lecturer's\", \"student\", \"stundet's\", \"it\", \"want\", \"you\",\"he's\", \"she's\"]\n",
        "    more =  [\n",
        "    'a', 'about', 'above', 'after', 'again', 'against', \"ain't\", 'all', 'am', 'an', 'and', 'any', 'are', 'aren\\'t', 'as', \n",
        "    'at', 'be', 'because', 'been', 'before', 'being', 'below', 'between', 'both', 'but', 'by', 'can', 'can\\'t', 'cannot', \n",
        "    'could', 'couldn\\'t', 'did', 'didn\\'t', 'do', 'does', 'doesn\\'t', 'doing', 'don\\'t', 'down', 'during', 'each', 'few', \n",
        "    'for', 'from', 'further', 'had', 'hadn\\'t', 'has', 'hasn\\'t', 'have', 'haven\\'t', 'having', 'he', 'he\\'d', 'he\\'ll', \n",
        "    'he\\'s', 'her', 'here', 'here\\'s', 'hers', 'herself', 'him', 'himself', 'his', 'how', 'how\\'s', 'i', 'i\\'d', 'i\\'ll', \n",
        "    'i\\'m', 'i\\'ve', 'if', 'in', 'into', 'is', 'isn\\'t', 'it', 'it\\'s', 'its', 'itself', 'let\\'s', 'me', 'more', 'most', \n",
        "    'mustn\\'t', 'my', 'myself', 'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'ought', 'our', \n",
        "    'ours', 'ourselves', 'out', 'over', 'own', 'same', 'shan\\'t', 'she', 'she\\'d', 'she\\'ll', 'she\\'s', 'should', \n",
        "    'shouldn\\'t', 'so', 'some', 'such', 'than', 'that', 'that\\'s', 'the', 'their', 'theirs', 'them', 'themselves', \n",
        "    'then', 'there', 'there\\'s', 'these', 'they', 'they\\'d', 'they\\'ll', 'they\\'re', 'they\\'ve', 'this', 'those', \n",
        "    'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', 'wasn\\'t', 'we', 'we\\'d', 'we\\'ll', 'we\\'re', \n",
        "    'we\\'ve', 'were', 'weren\\'t', 'what', 'what\\'s', 'when', 'when\\'s', 'where', 'where\\'s', 'which', 'while', 'who', \n",
        "    'who\\'s', 'whom', 'why', 'why\\'s', 'with', 'won\\'t', 'would', 'wouldn\\'t', 'you', 'you\\'d', 'you\\'ll', 'you\\'re', \n",
        "    'you\\'ve', 'your', 'yours', 'yourself', 'yourselves']\n",
        "\n",
        "    final_stopwords = stopwords + numbers_stopwords + more_words + more\n",
        "    # Sentence converted to lowercase-only\n",
        "    sentence = sentence.lower()\n",
        "    \n",
        "    ### START CODE HERE\n",
        "    words = sentence.split()\n",
        "    tempWords = []\n",
        "    for i in words:\n",
        "        if i not in final_stopwords:\n",
        "            tempWords.append(i)\n",
        "            sentence = ' '.join(tempWords)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    return sentence"
      ],
      "metadata": {
        "id": "rg8BJ24RaQF5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(df)\n",
        "df['reviews'].values"
      ],
      "metadata": {
        "id": "5Eu9CPSHaSv0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cKomen = []\n",
        "for i in range(len(df)):\n",
        "  cKomen.append(remove_stopwords(df['reviews'][i], stopwords_data))\n",
        "\n",
        "df['reviews'] = cKomen"
      ],
      "metadata": {
        "id": "usgdW83Haj3y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HyperParameter\n",
        "vocab_size = 1000\n",
        "embedding_dim = 32\n",
        "max_length = 64\n",
        "trunc_type = 'post'\n",
        "oov_tok = '<OOV>'\n",
        "padding_type = 'post'"
      ],
      "metadata": {
        "id": "d2qSifujbDEs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\n",
        "tokenizer.fit_on_texts(training_sentences)\n",
        "word_index = tokenizer.word_index\n",
        "sequences = tokenizer.texts_to_sequences(training_sentences)\n",
        "padded = pad_sequences(sequences, maxlen=max_length, truncating=trunc_type)\n",
        "testing_sentences = tokenizer.texts_to_sequences(testing_sentences)\n",
        "testing_padded = pad_sequences(testing_sentences, maxlen=max_length)"
      ],
      "metadata": {
        "id": "_HJ6k07AbIxI"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}