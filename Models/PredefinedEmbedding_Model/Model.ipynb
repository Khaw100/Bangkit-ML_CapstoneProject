{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 50,
      "metadata": {
        "id": "_qfzq2wDZmkx"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy.stats import linregress"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 51,
      "metadata": {
        "id": "Vd4os3NhZp5l"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv('ReviewsEN.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 52,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7uQq0VQTZ9iF",
        "outputId": "435b1498-f676-4c48-bb2e-3952cea7f332"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-1 : 870\n",
            "0 : 639\n",
            "1 : 1518\n"
          ]
        }
      ],
      "source": [
        "print(\"-1 :\", df['sentiment'].value_counts()[-1])\n",
        "print(\"0 :\", df['sentiment'].value_counts()[0])\n",
        "print(\"1 :\", df['sentiment'].value_counts()[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 53,
      "metadata": {
        "id": "DsSXQdDXZ-0R"
      },
      "outputs": [],
      "source": [
        "# Replace values in pandas DataFrame.\n",
        "df['sentiment'] = df['sentiment'].replace([1], 2)\n",
        "df['sentiment'] = df['sentiment'].replace([0], 1)\n",
        "df['sentiment'] = df['sentiment'].replace([-1], 0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 54,
      "metadata": {
        "id": "RayqsORYaBXV"
      },
      "outputs": [],
      "source": [
        "# # Apply lower function\n",
        "# df['reviews'] = df['reviews'].apply(str.lower)\n",
        "# print(df)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 55,
      "metadata": {
        "id": "xSDvqvHSeqFr"
      },
      "outputs": [],
      "source": [
        "# Global Variables\n",
        "\n",
        "EMBEDDING_DIM = 100\n",
        "MAXLEN = 16\n",
        "TRUNCATING = 'post'\n",
        "PADDING = 'post'\n",
        "OOV_TOKEN = \"<OOV>\"\n",
        "MAX_EXAMPLES = len(df)\n",
        "TRAINING_SPLIT = 0.9"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 56,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Lm_QQl4setsh",
        "outputId": "fb5e73af-4c03-4e5c-efde-7bb74b2acb64"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 3027 sentences and 3027 labels after random sampling\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import random\n",
        "\n",
        "random.seed(42)\n",
        "\n",
        "# Get the indices of the DataFrame\n",
        "indices = df.index.tolist()\n",
        "\n",
        "# Perform random sampling on the indices\n",
        "selected_indices = random.sample(indices, MAX_EXAMPLES)\n",
        "\n",
        "# Select the corresponding sentences and labels based on the sampled indices\n",
        "sentences = df.loc[selected_indices, 'reviews']\n",
        "labels = df.loc[selected_indices, 'sentiment']\n",
        "\n",
        "print(f\"There are {len(sentences)} sentences and {len(labels)} labels after random sampling\\n\")\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "XBbquN0qf6Og"
      },
      "source": [
        "# Training - Validation Split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 57,
      "metadata": {
        "id": "BSptFFxJf8hw"
      },
      "outputs": [],
      "source": [
        "def train_val_split(sentences, labels, training_split):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Compute the number of sentences that will be used for training (should be an integer)\n",
        "    train_size = int(len(sentences)*training_split)\n",
        "\n",
        "    # Split the sentences and labels into train/validation splits\n",
        "    train_sentences = sentences[:train_size]\n",
        "    train_labels = labels[:train_size]\n",
        "\n",
        "    validation_sentences = sentences[train_size:]\n",
        "    validation_labels = labels[train_size:]\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return train_sentences, validation_sentences, train_labels, validation_labels"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbdiZp4LgHwo",
        "outputId": "a385434d-db14-4dc3-a68a-57600617ba4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "There are 2724 sentences for training.\n",
            "\n",
            "There are 2724 labels for training.\n",
            "\n",
            "There are 303 sentences for validation.\n",
            "\n",
            "There are 303 labels for validation.\n"
          ]
        }
      ],
      "source": [
        "train_sentences, val_sentences, train_labels, val_labels = train_val_split(sentences, labels, TRAINING_SPLIT)\n",
        "\n",
        "print(f\"There are {len(train_sentences)} sentences for training.\\n\")\n",
        "print(f\"There are {len(train_labels)} labels for training.\\n\")\n",
        "print(f\"There are {len(val_sentences)} sentences for validation.\\n\")\n",
        "print(f\"There are {len(val_labels)} labels for validation.\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Hkha78ZLhD8n"
      },
      "source": [
        "# Tokenization - Sequences, Truncating, and Padding"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "Ysf5VZ_hhITo"
      },
      "outputs": [],
      "source": [
        "# GRADED FUNCTION: fit_tokenizer\n",
        "def fit_tokenizer(train_sentences, oov_token):\n",
        "    ### START CODE HERE\n",
        "    \n",
        "    # Instantiate the Tokenizer class, passing in the correct values for oov_token\n",
        "    tokenizer = Tokenizer(oov_token = OOV_TOKEN)\n",
        "    \n",
        "    # Fit the tokenizer to the training sentences\n",
        "    tokenizer.fit_on_texts(train_sentences)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 60,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Hy5fTzvhMMm",
        "outputId": "50883404-b9f2-45f1-a718-a9640a2512d0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Vocabulary contains 3969 words\n",
            "\n",
            "<OOV> token included in vocabulary\n",
            "\n",
            "index of word 'i' should be 9\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'<OOV>': 1,\n",
              " 'the': 2,\n",
              " 'and': 3,\n",
              " 'to': 4,\n",
              " 'is': 5,\n",
              " 'a': 6,\n",
              " 'he': 7,\n",
              " 'you': 8,\n",
              " 'i': 9,\n",
              " 'class': 10,\n",
              " 'of': 11,\n",
              " 'in': 12,\n",
              " 'very': 13,\n",
              " 'professor': 14,\n",
              " 'but': 15,\n",
              " 'not': 16,\n",
              " 'was': 17,\n",
              " 'are': 18,\n",
              " 'for': 19,\n",
              " 'it': 20,\n",
              " 'she': 21,\n",
              " 'good': 22,\n",
              " 'students': 23,\n",
              " 'his': 24,\n",
              " 'this': 25,\n",
              " 'on': 26,\n",
              " 'with': 27,\n",
              " 'that': 28,\n",
              " 'if': 29,\n",
              " 'course': 30,\n",
              " 'be': 31,\n",
              " 'or': 32,\n",
              " 'have': 33,\n",
              " 'her': 34,\n",
              " 'material': 35,\n",
              " 'easy': 36,\n",
              " 'as': 37,\n",
              " 'all': 38,\n",
              " 'were': 39,\n",
              " 'teacher': 40,\n",
              " 'about': 41,\n",
              " 'so': 42,\n",
              " 'really': 43,\n",
              " 'take': 44,\n",
              " 'will': 45,\n",
              " 'do': 46,\n",
              " 'an': 47,\n",
              " 'great': 48,\n",
              " 'understand': 49,\n",
              " 'lecturer': 50,\n",
              " 'can': 51,\n",
              " 'learning': 52,\n",
              " 'him': 53,\n",
              " 'lectures': 54,\n",
              " 'at': 55,\n",
              " 'get': 56,\n",
              " 'help': 57,\n",
              " 'helpful': 58,\n",
              " 'teaching': 59,\n",
              " 'hard': 60,\n",
              " 'well': 61,\n",
              " 'my': 62,\n",
              " 'had': 63,\n",
              " 'more': 64,\n",
              " 'questions': 65,\n",
              " 'they': 66,\n",
              " 'work': 67,\n",
              " 'assignments': 68,\n",
              " 'just': 69,\n",
              " 'difficult': 70,\n",
              " 'like': 71,\n",
              " 'lot': 72,\n",
              " 'there': 73,\n",
              " 'best': 74,\n",
              " 'your': 75,\n",
              " 'what': 76,\n",
              " 'one': 77,\n",
              " 'always': 78,\n",
              " 'time': 79,\n",
              " 'no': 80,\n",
              " 'also': 81,\n",
              " 'would': 82,\n",
              " \"don't\": 83,\n",
              " 'subject': 84,\n",
              " 'from': 85,\n",
              " 'too': 86,\n",
              " 'their': 87,\n",
              " 'exams': 88,\n",
              " 'up': 89,\n",
              " 'makes': 90,\n",
              " 'has': 91,\n",
              " 'clear': 92,\n",
              " 'lecture': 93,\n",
              " 'tests': 94,\n",
              " 'me': 95,\n",
              " 'exam': 96,\n",
              " 'did': 97,\n",
              " 'which': 98,\n",
              " 'learn': 99,\n",
              " 'much': 100,\n",
              " 'make': 101,\n",
              " 'because': 102,\n",
              " 'nice': 103,\n",
              " 'know': 104,\n",
              " 'some': 105,\n",
              " 'provide': 106,\n",
              " 'made': 107,\n",
              " 'interesting': 108,\n",
              " 'does': 109,\n",
              " \"lecturer's\": 110,\n",
              " 'we': 111,\n",
              " 'them': 112,\n",
              " 'study': 113,\n",
              " 'when': 114,\n",
              " \"he's\": 115,\n",
              " 'knowledge': 116,\n",
              " 'math': 117,\n",
              " 'gives': 118,\n",
              " 'recommend': 119,\n",
              " 'every': 120,\n",
              " 'classes': 121,\n",
              " 'us': 122,\n",
              " 'understanding': 123,\n",
              " 'sometimes': 124,\n",
              " 'homework': 125,\n",
              " 'way': 126,\n",
              " 'out': 127,\n",
              " \"professor's\": 128,\n",
              " \"it's\": 129,\n",
              " 'by': 130,\n",
              " 'materials': 131,\n",
              " 'go': 132,\n",
              " 'only': 133,\n",
              " 'final': 134,\n",
              " 'test': 135,\n",
              " 'ever': 136,\n",
              " 'knowledgeable': 137,\n",
              " 'student': 138,\n",
              " 'grading': 139,\n",
              " 'everything': 140,\n",
              " 'who': 141,\n",
              " 'extra': 142,\n",
              " 'experience': 143,\n",
              " 'even': 144,\n",
              " 'bad': 145,\n",
              " 'boring': 146,\n",
              " 'should': 147,\n",
              " 'university': 148,\n",
              " 'engaging': 149,\n",
              " 'need': 150,\n",
              " 'jokes': 151,\n",
              " 'credit': 152,\n",
              " 'content': 153,\n",
              " 'how': 154,\n",
              " 'read': 155,\n",
              " 'concepts': 156,\n",
              " 'than': 157,\n",
              " 'guy': 158,\n",
              " 'think': 159,\n",
              " 'ask': 160,\n",
              " 'any': 161,\n",
              " \"doesn't\": 162,\n",
              " 'fun': 163,\n",
              " 'grade': 164,\n",
              " 'fair': 165,\n",
              " 'most': 166,\n",
              " 'pretty': 167,\n",
              " 'prof': 168,\n",
              " 'could': 169,\n",
              " 'long': 170,\n",
              " 'teachers': 171,\n",
              " 'lab': 172,\n",
              " 'sure': 173,\n",
              " 'funny': 174,\n",
              " 'book': 175,\n",
              " 'lecturers': 176,\n",
              " 'making': 177,\n",
              " 'practical': 178,\n",
              " 'other': 179,\n",
              " 'took': 180,\n",
              " 'never': 181,\n",
              " 'grades': 182,\n",
              " 'teach': 183,\n",
              " \"didn't\": 184,\n",
              " 'learned': 185,\n",
              " 'available': 186,\n",
              " 'knows': 187,\n",
              " 'give': 188,\n",
              " 'feedback': 189,\n",
              " 'again': 190,\n",
              " 'paper': 191,\n",
              " 'excellent': 192,\n",
              " 'over': 193,\n",
              " 'resources': 194,\n",
              " 'taking': 195,\n",
              " 'overall': 196,\n",
              " 'willing': 197,\n",
              " 'feel': 198,\n",
              " 'notes': 199,\n",
              " 'books': 200,\n",
              " 'answer': 201,\n",
              " 'semester': 202,\n",
              " 'matter': 203,\n",
              " 'quizzes': 204,\n",
              " 'provides': 205,\n",
              " 'our': 206,\n",
              " 'without': 207,\n",
              " 'teaches': 208,\n",
              " 'midterm': 209,\n",
              " 'want': 210,\n",
              " 'different': 211,\n",
              " 'environment': 212,\n",
              " \"students'\": 213,\n",
              " 'problems': 214,\n",
              " 'during': 215,\n",
              " 'enough': 216,\n",
              " 'real': 217,\n",
              " 'examples': 218,\n",
              " 'provided': 219,\n",
              " 'first': 220,\n",
              " 'helped': 221,\n",
              " 'got': 222,\n",
              " 'little': 223,\n",
              " 'depth': 224,\n",
              " 'fine': 225,\n",
              " 'challenging': 226,\n",
              " 'often': 227,\n",
              " 'pass': 228,\n",
              " 'classroom': 229,\n",
              " 'courses': 230,\n",
              " 'person': 231,\n",
              " 'going': 232,\n",
              " 'accent': 233,\n",
              " 'cares': 234,\n",
              " 'definitely': 235,\n",
              " 'things': 236,\n",
              " 'anything': 237,\n",
              " \"she's\": 238,\n",
              " 'professors': 239,\n",
              " 'papers': 240,\n",
              " 'activities': 241,\n",
              " 'instructor': 242,\n",
              " 'average': 243,\n",
              " 'library': 244,\n",
              " 'better': 245,\n",
              " '2': 246,\n",
              " 'hours': 247,\n",
              " 'skills': 248,\n",
              " 'style': 249,\n",
              " 'life': 250,\n",
              " 'into': 251,\n",
              " 'bit': 252,\n",
              " 'then': 253,\n",
              " 'must': 254,\n",
              " 'worst': 255,\n",
              " 'follow': 256,\n",
              " 'pattern': 257,\n",
              " 'use': 258,\n",
              " 'am': 259,\n",
              " 'discussions': 260,\n",
              " 'attention': 261,\n",
              " \"you'll\": 262,\n",
              " 'felt': 263,\n",
              " 'opportunities': 264,\n",
              " 'practice': 265,\n",
              " 'able': 266,\n",
              " 'wants': 267,\n",
              " 'off': 268,\n",
              " 'organized': 269,\n",
              " 'delivery': 270,\n",
              " 'still': 271,\n",
              " 'through': 272,\n",
              " 'after': 273,\n",
              " 'world': 274,\n",
              " 'passionate': 275,\n",
              " 'topics': 276,\n",
              " 'however': 277,\n",
              " 'relevant': 278,\n",
              " 'before': 279,\n",
              " 'marks': 280,\n",
              " 'used': 281,\n",
              " 'people': 282,\n",
              " 'taught': 283,\n",
              " 'come': 284,\n",
              " 'reading': 285,\n",
              " 'own': 286,\n",
              " 'lots': 287,\n",
              " 'its': 288,\n",
              " 'love': 289,\n",
              " 'amazing': 290,\n",
              " 'less': 291,\n",
              " 'write': 292,\n",
              " 'many': 293,\n",
              " \"i've\": 294,\n",
              " 'highly': 295,\n",
              " 'keep': 296,\n",
              " 'problem': 297,\n",
              " 'encouraged': 298,\n",
              " 'super': 299,\n",
              " 'awesome': 300,\n",
              " 'extremely': 301,\n",
              " 'labs': 302,\n",
              " 'down': 303,\n",
              " 'pay': 304,\n",
              " 'doing': 305,\n",
              " 'online': 306,\n",
              " 'everyone': 307,\n",
              " 'explain': 308,\n",
              " 'nor': 309,\n",
              " 'week': 310,\n",
              " 'interaction': 311,\n",
              " 'thinking': 312,\n",
              " 'getting': 313,\n",
              " 'kind': 314,\n",
              " 'dont': 315,\n",
              " 'group': 316,\n",
              " 'confusing': 317,\n",
              " 'though': 318,\n",
              " 'checking': 319,\n",
              " 'each': 320,\n",
              " 'based': 321,\n",
              " 'sense': 322,\n",
              " 'thought': 323,\n",
              " 'office': 324,\n",
              " 'means': 325,\n",
              " 'tough': 326,\n",
              " 'being': 327,\n",
              " '1': 328,\n",
              " 'information': 329,\n",
              " 'participation': 330,\n",
              " 'times': 331,\n",
              " \"can't\": 332,\n",
              " 'stuff': 333,\n",
              " '3': 334,\n",
              " 'thing': 335,\n",
              " 'neither': 336,\n",
              " 'find': 337,\n",
              " 'been': 338,\n",
              " 'interactive': 339,\n",
              " 'answers': 340,\n",
              " 'evaluations': 341,\n",
              " 'question': 342,\n",
              " 'explanations': 343,\n",
              " 'anyone': 344,\n",
              " \"i'm\": 345,\n",
              " 'clearly': 346,\n",
              " 'outside': 347,\n",
              " 'points': 348,\n",
              " 'explains': 349,\n",
              " 'say': 350,\n",
              " 'show': 351,\n",
              " 'wish': 352,\n",
              " 'critical': 353,\n",
              " 'put': 354,\n",
              " 'succeed': 355,\n",
              " 'cool': 356,\n",
              " 'try': 357,\n",
              " 'facilities': 358,\n",
              " 'important': 359,\n",
              " 'personal': 360,\n",
              " 'needs': 361,\n",
              " 'b': 362,\n",
              " 'punctuality': 363,\n",
              " 'projects': 364,\n",
              " 'takes': 365,\n",
              " \"wasn't\": 366,\n",
              " 'taken': 367,\n",
              " 'helps': 368,\n",
              " 'prepared': 369,\n",
              " 'explaining': 370,\n",
              " 'complex': 371,\n",
              " 'part': 372,\n",
              " 'effectively': 373,\n",
              " 'comments': 374,\n",
              " 'system': 375,\n",
              " 'friendly': 376,\n",
              " 'field': 377,\n",
              " 'high': 378,\n",
              " 'among': 379,\n",
              " 'such': 380,\n",
              " 'attend': 381,\n",
              " 'approachable': 382,\n",
              " 'presentation': 383,\n",
              " 'methods': 384,\n",
              " 'terrible': 385,\n",
              " 'manner': 386,\n",
              " 'tries': 387,\n",
              " 'assessments': 388,\n",
              " 'went': 389,\n",
              " 'valuable': 390,\n",
              " 'unclear': 391,\n",
              " 'something': 392,\n",
              " 'research': 393,\n",
              " 'effort': 394,\n",
              " 'stories': 395,\n",
              " 'disorganized': 396,\n",
              " 'talk': 397,\n",
              " 'while': 398,\n",
              " 'nothing': 399,\n",
              " 'grader': 400,\n",
              " 'level': 401,\n",
              " 'effective': 402,\n",
              " 'encourages': 403,\n",
              " \"you're\": 404,\n",
              " 'point': 405,\n",
              " 'far': 406,\n",
              " 'day': 407,\n",
              " 'others': 408,\n",
              " 'topic': 409,\n",
              " 'expectations': 410,\n",
              " '5': 411,\n",
              " 'tell': 412,\n",
              " 'easier': 413,\n",
              " 'quite': 414,\n",
              " 'actually': 415,\n",
              " 'support': 416,\n",
              " 'two': 417,\n",
              " 'goes': 418,\n",
              " 'professional': 419,\n",
              " 'terms': 420,\n",
              " 'timely': 421,\n",
              " 'strict': 422,\n",
              " 'attendance': 423,\n",
              " 'participate': 424,\n",
              " 'informative': 425,\n",
              " 'supportive': 426,\n",
              " 'new': 427,\n",
              " 'end': 428,\n",
              " 'heavy': 429,\n",
              " 'easily': 430,\n",
              " 'decent': 431,\n",
              " 'these': 432,\n",
              " 'discussion': 433,\n",
              " 'look': 434,\n",
              " 'interested': 435,\n",
              " 'those': 436,\n",
              " 'subjects': 437,\n",
              " 'comprehensive': 438,\n",
              " 'school': 439,\n",
              " 'constructive': 440,\n",
              " 'simple': 441,\n",
              " 'stay': 442,\n",
              " 'covered': 443,\n",
              " 'proper': 444,\n",
              " 'required': 445,\n",
              " 'towards': 446,\n",
              " 'sweet': 447,\n",
              " 'talking': 448,\n",
              " 'enjoyable': 449,\n",
              " 'lack': 450,\n",
              " 'possible': 451,\n",
              " 'given': 452,\n",
              " 'strong': 453,\n",
              " 'gave': 454,\n",
              " 'few': 455,\n",
              " 'improve': 456,\n",
              " 'passion': 457,\n",
              " 'writing': 458,\n",
              " 'found': 459,\n",
              " 'project': 460,\n",
              " 'cover': 461,\n",
              " 'improvement': 462,\n",
              " 'structured': 463,\n",
              " 'may': 464,\n",
              " 'additional': 465,\n",
              " 'care': 466,\n",
              " 'opinions': 467,\n",
              " 'rude': 468,\n",
              " 'ensuring': 469,\n",
              " 'business': 470,\n",
              " 'faculty': 471,\n",
              " 'neutral': 472,\n",
              " 'textbook': 473,\n",
              " 'caring': 474,\n",
              " 'man': 475,\n",
              " 'job': 476,\n",
              " 'sufficient': 477,\n",
              " 'okay': 478,\n",
              " 'explanation': 479,\n",
              " 'loved': 480,\n",
              " 'examination': 481,\n",
              " 'extracurricular': 482,\n",
              " 'ok': 483,\n",
              " 'needed': 484,\n",
              " 'throughout': 485,\n",
              " 'now': 486,\n",
              " 'demonstrated': 487,\n",
              " 'said': 488,\n",
              " 'expects': 489,\n",
              " 'english': 490,\n",
              " 'open': 491,\n",
              " 'right': 492,\n",
              " 'interest': 493,\n",
              " 'yourself': 494,\n",
              " 'accessible': 495,\n",
              " \"isn't\": 496,\n",
              " 'seems': 497,\n",
              " 'positive': 498,\n",
              " 'until': 499,\n",
              " 'worth': 500,\n",
              " 'leaving': 501,\n",
              " 'where': 502,\n",
              " '10': 503,\n",
              " 'academic': 504,\n",
              " 'fail': 505,\n",
              " 'ideas': 506,\n",
              " 'allowing': 507,\n",
              " 'instructions': 508,\n",
              " 'assessment': 509,\n",
              " 'providing': 510,\n",
              " 'once': 511,\n",
              " 'absolutely': 512,\n",
              " 'avoid': 513,\n",
              " 'humor': 514,\n",
              " 'away': 515,\n",
              " 'almost': 516,\n",
              " 'u': 517,\n",
              " 'guidance': 518,\n",
              " 'deep': 519,\n",
              " 'independent': 520,\n",
              " 'beyond': 521,\n",
              " 'guides': 522,\n",
              " 'lacks': 523,\n",
              " 'communication': 524,\n",
              " 'criteria': 525,\n",
              " 'necessary': 526,\n",
              " 'harder': 527,\n",
              " 'development': 528,\n",
              " 'process': 529,\n",
              " 'another': 530,\n",
              " 'apply': 531,\n",
              " 'created': 532,\n",
              " 'readings': 533,\n",
              " 'enjoyed': 534,\n",
              " 'perspectives': 535,\n",
              " 'helping': 536,\n",
              " 'syllabus': 537,\n",
              " 'else': 538,\n",
              " 'assignment': 539,\n",
              " 'back': 540,\n",
              " 'step': 541,\n",
              " 'adequate': 542,\n",
              " 'between': 543,\n",
              " 'case': 544,\n",
              " 'same': 545,\n",
              " \"won't\": 546,\n",
              " 'fast': 547,\n",
              " 'miss': 548,\n",
              " 'enthusiastic': 549,\n",
              " 'language': 550,\n",
              " 'quiz': 551,\n",
              " 'confusion': 552,\n",
              " 'applications': 553,\n",
              " 'yet': 554,\n",
              " 'complete': 555,\n",
              " 'guide': 556,\n",
              " 'approach': 557,\n",
              " 'next': 558,\n",
              " 'choice': 559,\n",
              " 'truly': 560,\n",
              " 'chapter': 561,\n",
              " 'review': 562,\n",
              " 'see': 563,\n",
              " 'smart': 564,\n",
              " 'inspiring': 565,\n",
              " 'presenter': 566,\n",
              " 'useful': 567,\n",
              " 'says': 568,\n",
              " 'fairly': 569,\n",
              " 'unresponsive': 570,\n",
              " 'probably': 571,\n",
              " 'career': 572,\n",
              " 'encourage': 573,\n",
              " 'giving': 574,\n",
              " 'either': 575,\n",
              " 'late': 576,\n",
              " 'concept': 577,\n",
              " 'outdated': 578,\n",
              " \"that's\": 579,\n",
              " 'punctual': 580,\n",
              " 'uses': 581,\n",
              " 'create': 582,\n",
              " 'confused': 583,\n",
              " 'chemistry': 584,\n",
              " 'posts': 585,\n",
              " 'grasp': 586,\n",
              " 'genuinely': 587,\n",
              " 'since': 588,\n",
              " 'poor': 589,\n",
              " 'distribution': 590,\n",
              " 'ready': 591,\n",
              " 'slides': 592,\n",
              " 'favorite': 593,\n",
              " 'due': 594,\n",
              " 'whole': 595,\n",
              " 'maintained': 596,\n",
              " 'studies': 597,\n",
              " 'number': 598,\n",
              " 'both': 599,\n",
              " 'big': 600,\n",
              " 'textbooks': 601,\n",
              " 'concise': 602,\n",
              " 'unhelpful': 603,\n",
              " 'ended': 604,\n",
              " 'enthusiasm': 605,\n",
              " 'likes': 606,\n",
              " 'inclusive': 607,\n",
              " 'comfortable': 608,\n",
              " 'tells': 609,\n",
              " 'causing': 610,\n",
              " '4': 611,\n",
              " 'essays': 612,\n",
              " 'lost': 613,\n",
              " 'working': 614,\n",
              " 'hands': 615,\n",
              " 'rather': 616,\n",
              " 'unfair': 617,\n",
              " 'evaluation': 618,\n",
              " 'insights': 619,\n",
              " 'expected': 620,\n",
              " 'let': 621,\n",
              " 'diverse': 622,\n",
              " 'fostering': 623,\n",
              " 'short': 624,\n",
              " 'coming': 625,\n",
              " 'chance': 626,\n",
              " 'doesnt': 627,\n",
              " 'although': 628,\n",
              " 'demonstrates': 629,\n",
              " 'current': 630,\n",
              " 'mark': 631,\n",
              " 'quality': 632,\n",
              " 'glad': 633,\n",
              " 'myself': 634,\n",
              " 'memorize': 635,\n",
              " 'mean': 636,\n",
              " 'seem': 637,\n",
              " 'unorganized': 638,\n",
              " 'emails': 639,\n",
              " 'industry': 640,\n",
              " 'low': 641,\n",
              " 'active': 642,\n",
              " 'comes': 643,\n",
              " 'listen': 644,\n",
              " 'looking': 645,\n",
              " 'clarity': 646,\n",
              " 'certain': 647,\n",
              " 'having': 648,\n",
              " 'inconsistent': 649,\n",
              " 'solving': 650,\n",
              " 'theory': 651,\n",
              " 'styles': 652,\n",
              " 'wonderful': 653,\n",
              " 'consuming': 654,\n",
              " 'yes': 655,\n",
              " 'success': 656,\n",
              " 'mistakes': 657,\n",
              " 'enhance': 658,\n",
              " 'studying': 659,\n",
              " 'innovative': 660,\n",
              " 'perfect': 661,\n",
              " 'negative': 662,\n",
              " 'shes': 663,\n",
              " 'prepare': 664,\n",
              " 'encouraging': 665,\n",
              " 'expect': 666,\n",
              " 'seemed': 667,\n",
              " 'completely': 668,\n",
              " 'written': 669,\n",
              " 'gets': 670,\n",
              " 'jazz': 671,\n",
              " 'multiple': 672,\n",
              " 'concerns': 673,\n",
              " 'c': 674,\n",
              " 'held': 675,\n",
              " 'respectful': 676,\n",
              " 'here': 677,\n",
              " 'year': 678,\n",
              " 'board': 679,\n",
              " 'done': 680,\n",
              " 'exactly': 681,\n",
              " 'regarding': 682,\n",
              " 'works': 683,\n",
              " 'various': 684,\n",
              " 'trying': 685,\n",
              " 'expertise': 686,\n",
              " 'etc': 687,\n",
              " 'old': 688,\n",
              " 'themselves': 689,\n",
              " 'curves': 690,\n",
              " 'plus': 691,\n",
              " 'turn': 692,\n",
              " 'entertaining': 693,\n",
              " 'develop': 694,\n",
              " 'objective': 695,\n",
              " 'lacked': 696,\n",
              " 'usually': 697,\n",
              " 'hour': 698,\n",
              " 'term': 699,\n",
              " 'incredibly': 700,\n",
              " '6': 701,\n",
              " 'consistent': 702,\n",
              " 'especially': 703,\n",
              " 'properly': 704,\n",
              " 'curve': 705,\n",
              " 'reviews': 706,\n",
              " 'asked': 707,\n",
              " 'struggling': 708,\n",
              " 'instructors': 709,\n",
              " 'watch': 710,\n",
              " 'detail': 711,\n",
              " 'interests': 712,\n",
              " 'summer': 713,\n",
              " 'college': 714,\n",
              " 'choose': 715,\n",
              " 'enjoy': 716,\n",
              " 'improved': 717,\n",
              " 'horrible': 718,\n",
              " 'understood': 719,\n",
              " \"there's\": 720,\n",
              " 'reason': 721,\n",
              " 'why': 722,\n",
              " 'creating': 723,\n",
              " 'dr': 724,\n",
              " 'solid': 725,\n",
              " 'midterms': 726,\n",
              " 'dry': 727,\n",
              " 'engage': 728,\n",
              " 'lessons': 729,\n",
              " 'mandatory': 730,\n",
              " 'promoting': 731,\n",
              " 'set': 732,\n",
              " 'showed': 733,\n",
              " 'computer': 734,\n",
              " 'wait': 735,\n",
              " 'role': 736,\n",
              " 'speaking': 737,\n",
              " 'poorly': 738,\n",
              " 'wrong': 739,\n",
              " 'application': 740,\n",
              " 'satisfied': 741,\n",
              " \"weren't\": 742,\n",
              " 'weeks': 743,\n",
              " 'around': 744,\n",
              " 'related': 745,\n",
              " 'issues': 746,\n",
              " 'intellectual': 747,\n",
              " 'practicals': 748,\n",
              " 'changes': 749,\n",
              " 'shows': 750,\n",
              " \"aren't\": 751,\n",
              " 'date': 752,\n",
              " 'scores': 753,\n",
              " 'thanks': 754,\n",
              " 'chapters': 755,\n",
              " 'collaborative': 756,\n",
              " 'analysis': 757,\n",
              " 'resource': 758,\n",
              " 'focus': 759,\n",
              " 'along': 760,\n",
              " 'stupid': 761,\n",
              " 'money': 762,\n",
              " 'waste': 763,\n",
              " 'blog': 764,\n",
              " 'interact': 765,\n",
              " 'irrelevant': 766,\n",
              " 'future': 767,\n",
              " 'theoretical': 768,\n",
              " 'genuine': 769,\n",
              " 'beginning': 770,\n",
              " 'explore': 771,\n",
              " 'quickly': 772,\n",
              " 'previous': 773,\n",
              " 'particularly': 774,\n",
              " 'attitude': 775,\n",
              " 'progress': 776,\n",
              " 'allowed': 777,\n",
              " 'atmosphere': 778,\n",
              " 'technology': 779,\n",
              " 'critically': 780,\n",
              " 'place': 781,\n",
              " 'barrow': 782,\n",
              " 'hell': 783,\n",
              " 'text': 784,\n",
              " 'fails': 785,\n",
              " 'cannot': 786,\n",
              " 'mid': 787,\n",
              " 'human': 788,\n",
              " 'requires': 789,\n",
              " 'straightforward': 790,\n",
              " 'offered': 791,\n",
              " 'seriously': 792,\n",
              " 'half': 793,\n",
              " 'solutions': 794,\n",
              " 'programming': 795,\n",
              " 'performance': 796,\n",
              " 'useless': 797,\n",
              " \"what's\": 798,\n",
              " 'experienced': 799,\n",
              " 'top': 800,\n",
              " 'presented': 801,\n",
              " 'techniques': 802,\n",
              " 'presents': 803,\n",
              " 'involved': 804,\n",
              " 'incomplete': 805,\n",
              " 'entire': 806,\n",
              " 'pace': 807,\n",
              " 'repeat': 808,\n",
              " 'responsive': 809,\n",
              " 'forward': 810,\n",
              " 'basically': 811,\n",
              " 'actively': 812,\n",
              " 'showing': 813,\n",
              " 'incorporated': 814,\n",
              " 'small': 815,\n",
              " 'break': 816,\n",
              " 'focused': 817,\n",
              " 'unbiased': 818,\n",
              " 'growth': 819,\n",
              " 'days': 820,\n",
              " 'pop': 821,\n",
              " 'unique': 822,\n",
              " 'amount': 823,\n",
              " 'type': 824,\n",
              " 'receive': 825,\n",
              " 'appropriate': 826,\n",
              " 'engaged': 827,\n",
              " 'workload': 828,\n",
              " 'self': 829,\n",
              " 'form': 830,\n",
              " 'someone': 831,\n",
              " 'asking': 832,\n",
              " 'objectives': 833,\n",
              " 'motivated': 834,\n",
              " 'whether': 835,\n",
              " 'condescending': 836,\n",
              " 'management': 837,\n",
              " 'particular': 838,\n",
              " 'weekly': 839,\n",
              " 'power': 840,\n",
              " 'confidence': 841,\n",
              " 'chinese': 842,\n",
              " 'please': 843,\n",
              " 'thorough': 844,\n",
              " 'schedule': 845,\n",
              " 'flexible': 846,\n",
              " 'single': 847,\n",
              " 'honestly': 848,\n",
              " 'met': 849,\n",
              " 'videos': 850,\n",
              " 'mind': 851,\n",
              " 'side': 852,\n",
              " 'covers': 853,\n",
              " 'rewarding': 854,\n",
              " 'music': 855,\n",
              " 'slow': 856,\n",
              " 'staff': 857,\n",
              " 'fall': 858,\n",
              " 'general': 859,\n",
              " '50': 860,\n",
              " 'articles': 861,\n",
              " 'value': 862,\n",
              " 'fantastic': 863,\n",
              " 'simply': 864,\n",
              " 'changed': 865,\n",
              " 'assigns': 866,\n",
              " 'words': 867,\n",
              " 'department': 868,\n",
              " 'policy': 869,\n",
              " 'difficulty': 870,\n",
              " 'opportunity': 871,\n",
              " 'cute': 872,\n",
              " 'increased': 873,\n",
              " 'spend': 874,\n",
              " 'respect': 875,\n",
              " 'engagement': 876,\n",
              " 'tried': 877,\n",
              " 'luck': 878,\n",
              " 'graded': 879,\n",
              " 'gpa': 880,\n",
              " 'unless': 881,\n",
              " 'structure': 882,\n",
              " 'track': 883,\n",
              " 'behavior': 884,\n",
              " 'ability': 885,\n",
              " 'trouble': 886,\n",
              " 'patient': 887,\n",
              " 'past': 888,\n",
              " 'word': 889,\n",
              " 'analyze': 890,\n",
              " 'testing': 891,\n",
              " 'situations': 892,\n",
              " 'delivering': 893,\n",
              " 'passing': 894,\n",
              " 'leading': 895,\n",
              " 'delivered': 896,\n",
              " 'mastery': 897,\n",
              " 'otherwise': 898,\n",
              " 'calculus': 899,\n",
              " '100': 900,\n",
              " 'gained': 901,\n",
              " 'reach': 902,\n",
              " 'platform': 903,\n",
              " 'breaks': 904,\n",
              " 'whatever': 905,\n",
              " 'ta': 906,\n",
              " 'creativity': 907,\n",
              " 'insightful': 908,\n",
              " 'uninspiring': 909,\n",
              " 'areas': 910,\n",
              " 'sheet': 911,\n",
              " 'enhancing': 912,\n",
              " 'lady': 913,\n",
              " 'biased': 914,\n",
              " 'agree': 915,\n",
              " 'inappropriate': 916,\n",
              " \"couldn't\": 917,\n",
              " 'lives': 918,\n",
              " 'share': 919,\n",
              " 'relaxed': 920,\n",
              " 'ass': 921,\n",
              " \"he'll\": 922,\n",
              " 'seek': 923,\n",
              " 'individual': 924,\n",
              " 'lacking': 925,\n",
              " 'consider': 926,\n",
              " 'itself': 927,\n",
              " 'extensive': 928,\n",
              " 'actual': 929,\n",
              " 'section': 930,\n",
              " 'unprepared': 931,\n",
              " 'maintains': 932,\n",
              " 'second': 933,\n",
              " 'liked': 934,\n",
              " 'songs': 935,\n",
              " 'further': 936,\n",
              " 'left': 937,\n",
              " 'explained': 938,\n",
              " \"wouldn't\": 939,\n",
              " 'theories': 940,\n",
              " 'hate': 941,\n",
              " 'mostly': 942,\n",
              " 'years': 943,\n",
              " 'above': 944,\n",
              " 'below': 945,\n",
              " 'generally': 946,\n",
              " 'philosophy': 947,\n",
              " 'intimidating': 948,\n",
              " 'depending': 949,\n",
              " 'initially': 950,\n",
              " 'monotone': 951,\n",
              " 'understandable': 952,\n",
              " 'change': 953,\n",
              " 'offers': 954,\n",
              " 'visual': 955,\n",
              " 'key': 956,\n",
              " 'efforts': 957,\n",
              " 'wanted': 958,\n",
              " 'fostered': 959,\n",
              " 'collaboration': 960,\n",
              " 'visually': 961,\n",
              " 'answered': 962,\n",
              " 'suggest': 963,\n",
              " 'equations': 964,\n",
              " 'curriculum': 965,\n",
              " 'last': 966,\n",
              " 's': 967,\n",
              " 'allows': 968,\n",
              " 'keeps': 969,\n",
              " 'model': 970,\n",
              " 'public': 971,\n",
              " 'remember': 972,\n",
              " 'drop': 973,\n",
              " 'unapproachable': 974,\n",
              " 'hope': 975,\n",
              " 'figure': 976,\n",
              " '15': 977,\n",
              " 'herself': 978,\n",
              " 'state': 979,\n",
              " 'variety': 980,\n",
              " 'keeping': 981,\n",
              " 'loves': 982,\n",
              " 'deeper': 983,\n",
              " 'passed': 984,\n",
              " 'challenge': 985,\n",
              " 'limited': 986,\n",
              " 'opinion': 987,\n",
              " 'attending': 988,\n",
              " 'saying': 989,\n",
              " 'balanced': 990,\n",
              " 'full': 991,\n",
              " 'elements': 992,\n",
              " 'least': 993,\n",
              " 'main': 994,\n",
              " 'according': 995,\n",
              " 'standards': 996,\n",
              " 'thinks': 997,\n",
              " 'manageable': 998,\n",
              " 'dedication': 999,\n",
              " 'himself': 1000,\n",
              " ...}"
            ]
          },
          "execution_count": 60,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Test your function\n",
        "tokenizer = fit_tokenizer(train_sentences, OOV_TOKEN)\n",
        "\n",
        "word_index = tokenizer.word_index\n",
        "VOCAB_SIZE = len(word_index)\n",
        "\n",
        "print(f\"Vocabulary contains {VOCAB_SIZE} words\\n\")\n",
        "print(\"<OOV> token included in vocabulary\" if \"<OOV>\" in word_index else \"<OOV> token NOT included in vocabulary\")\n",
        "print(f\"\\nindex of word 'i' should be {word_index['i']}\")\n",
        "word_index"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 61,
      "metadata": {
        "id": "xUkJFtiwh7C-"
      },
      "outputs": [],
      "source": [
        "def seq_pad_and_trunc(sentences, tokenizer, padding, truncating, maxlen):\n",
        "    ### START CODE HERE\n",
        "       \n",
        "    # Convert sentences to sequences\n",
        "    sequences = tokenizer.texts_to_sequences(sentences)\n",
        "    \n",
        "    # Pad the sequences using the correct padding, truncating and maxlen\n",
        "    pad_trunc_sequences = pad_sequences(sequences, maxlen= MAXLEN, padding = PADDING, truncating = TRUNCATING)\n",
        "    \n",
        "    ### END CODE HERE\n",
        "    \n",
        "    return pad_trunc_sequences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 62,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "biCCj27uh-Ve",
        "outputId": "8d2ec628-39df-4219-e6d7-6954cabc0ce2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Padded and truncated training sequences have shape: (2724, 16)\n",
            "\n",
            "Padded and truncated validation sequences have shape: (303, 16)\n"
          ]
        }
      ],
      "source": [
        "train_pad_trunc_seq = seq_pad_and_trunc(train_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "val_pad_trunc_seq = seq_pad_and_trunc(val_sentences, tokenizer, PADDING, TRUNCATING, MAXLEN)\n",
        "\n",
        "print(f\"Padded and truncated training sequences have shape: {train_pad_trunc_seq.shape}\\n\")\n",
        "print(f\"Padded and truncated validation sequences have shape: {val_pad_trunc_seq.shape}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 63,
      "metadata": {
        "id": "OR3wfthriCnN"
      },
      "outputs": [],
      "source": [
        "train_labels = np.array(train_labels)\n",
        "val_labels = np.array(val_labels)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "j0j16iy7iI-m"
      },
      "source": [
        "# Using pre-defined Embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 64,
      "metadata": {
        "id": "tUHybcFMiE0d"
      },
      "outputs": [],
      "source": [
        "# Define path to file containing the embeddings\n",
        "GLOVE_FILE = 'glove.6B.100d.txt'\n",
        "\n",
        "# Initialize an empty embeddings index dictionary\n",
        "GLOVE_EMBEDDINGS = {}\n",
        "\n",
        "# Read file and fill GLOVE_EMBEDDINGS with its contents\n",
        "with open(GLOVE_FILE) as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        GLOVE_EMBEDDINGS[word] = coefs"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ldaN0eawl5v5"
      },
      "source": [
        "# Represent the words in your vocabulary using the embeddings"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 65,
      "metadata": {
        "id": "Rokt-fwRl8NB"
      },
      "outputs": [],
      "source": [
        "# Initialize an empty numpy array with the appropriate size\n",
        "EMBEDDINGS_MATRIX = np.zeros((VOCAB_SIZE+1, EMBEDDING_DIM))\n",
        "\n",
        "# Iterate all of the words in the vocabulary and if the vector representation for \n",
        "# each word exists within GloVe's representations, save it in the EMBEDDINGS_MATRIX array\n",
        "for word, i in word_index.items():\n",
        "    embedding_vector = GLOVE_EMBEDDINGS.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        EMBEDDINGS_MATRIX[i] = embedding_vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "HZjjYxbzl8ga"
      },
      "source": [
        "# Define a model that does not overfit"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "QPn--pNsDhEv"
      },
      "source": [
        "Model with 0.001 learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 66,
      "metadata": {
        "id": "HAWbMeF-mBg5"
      },
      "outputs": [],
      "source": [
        "# def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "#     model = tf.keras.Sequential([ \n",
        "#         # This is how you need to set the Embedding layer when using pre-trained embeddings\n",
        "#         tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "#         tf.keras.layers.Conv1D(32, 5, activation='relu'),\n",
        "#         tf.keras.layers.GlobalMaxPooling1D(),\n",
        "#         tf.keras.layers.Dropout(0.2),\n",
        "#         tf.keras.layers.Dense(32, activation='relu'),\n",
        "#         tf.keras.layers.Dense(3, activation='softmax'),\n",
        "#     ])\n",
        "    \n",
        "#     model.compile(loss='sparse_categorical_crossentropy',\n",
        "#                   optimizer='adam',\n",
        "#                   metrics=['accuracy']) \n",
        "#     return model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "_iIk4pY1DcZd"
      },
      "source": [
        "Model with 0.002 learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 67,
      "metadata": {
        "id": "eLywL_aR9KSf"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import optimizers\n",
        "\n",
        "def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "        tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "        tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
        "        tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "        tf.keras.layers.Dense(64, activation='relu'),\n",
        "        tf.keras.layers.Dropout(0.2),\n",
        "        tf.keras.layers.Dense(3, activation='softmax'),\n",
        "    ])\n",
        "    optimizer = optimizers.Adam(learning_rate = 0.002)\n",
        "    model.compile(loss='sparse_categorical_crossentropy',\n",
        "                  optimizer=optimizer,\n",
        "                  metrics=['accuracy']) \n",
        "    return model\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ewJcvwnoIwCh"
      },
      "source": [
        "Model with 0.0025 learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 68,
      "metadata": {
        "id": "BB31XCglDbsW"
      },
      "outputs": [],
      "source": [
        "# from tensorflow.keras import optimizers\n",
        "\n",
        "# def create_model(vocab_size, embedding_dim, maxlen, embeddings_matrix):\n",
        "#     model = tf.keras.Sequential([\n",
        "#         tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=maxlen, weights=[embeddings_matrix], trainable=False),\n",
        "#         tf.keras.layers.Conv1D(64, 3, activation='relu'),\n",
        "#         tf.keras.layers.MaxPooling1D(pool_size=2),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.GRU(64, return_sequences=True)),\n",
        "#         tf.keras.layers.Bidirectional(tf.keras.layers.GRU(32)),\n",
        "#         tf.keras.layers.Dense(64, activation='relu'),\n",
        "#         tf.keras.layers.Dropout(0.2),\n",
        "#         tf.keras.layers.Dense(3, activation='softmax'),\n",
        "#     ])\n",
        "#     optimizer = optimizers.Adam(learning_rate = 0.0025)\n",
        "#     model.compile(loss='sparse_categorical_crossentropy',\n",
        "#                   optimizer=optimizer,\n",
        "#                   metrics=['accuracy']) \n",
        "#     return model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 69,
      "metadata": {
        "id": "ePMfJY5UKPB4"
      },
      "outputs": [],
      "source": [
        "model = create_model(VOCAB_SIZE, EMBEDDING_DIM, MAXLEN, EMBEDDINGS_MATRIX)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 70,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nZnbOCPTQ_QK",
        "outputId": "c00f2258-e113-4e42-f09c-ea82be0c7dcf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_1 (Embedding)     (None, 16, 100)           397000    \n",
            "                                                                 \n",
            " conv1d_1 (Conv1D)           (None, 14, 64)            19264     \n",
            "                                                                 \n",
            " max_pooling1d_1 (MaxPooling  (None, 7, 64)            0         \n",
            " 1D)                                                             \n",
            "                                                                 \n",
            " bidirectional_2 (Bidirectio  (None, 7, 128)           49920     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " bidirectional_3 (Bidirectio  (None, 64)               31104     \n",
            " nal)                                                            \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 64)                4160      \n",
            "                                                                 \n",
            " dropout_1 (Dropout)         (None, 64)                0         \n",
            "                                                                 \n",
            " dense_3 (Dense)             (None, 3)                 195       \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 501,643\n",
            "Trainable params: 104,643\n",
            "Non-trainable params: 397,000\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 71,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArRf_nFcmEjZ",
        "outputId": "226ff45f-d8b0-4bc5-8b6b-2c09b55afcdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "86/86 [==============================] - 13s 43ms/step - loss: 0.9303 - accuracy: 0.5786 - val_loss: 0.7793 - val_accuracy: 0.6931\n",
            "Epoch 2/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.7276 - accuracy: 0.6964 - val_loss: 0.7013 - val_accuracy: 0.6997\n",
            "Epoch 3/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.6018 - accuracy: 0.7695 - val_loss: 0.7093 - val_accuracy: 0.7162\n",
            "Epoch 4/200\n",
            "86/86 [==============================] - 3s 30ms/step - loss: 0.5027 - accuracy: 0.8036 - val_loss: 0.7651 - val_accuracy: 0.6766\n",
            "Epoch 5/200\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 0.4079 - accuracy: 0.8491 - val_loss: 0.7167 - val_accuracy: 0.7360\n",
            "Epoch 6/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.3676 - accuracy: 0.8649 - val_loss: 0.6915 - val_accuracy: 0.7558\n",
            "Epoch 7/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.2649 - accuracy: 0.9079 - val_loss: 0.9708 - val_accuracy: 0.6964\n",
            "Epoch 8/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.2430 - accuracy: 0.9148 - val_loss: 1.0659 - val_accuracy: 0.6832\n",
            "Epoch 9/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.1788 - accuracy: 0.9365 - val_loss: 0.9664 - val_accuracy: 0.7063\n",
            "Epoch 10/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.1452 - accuracy: 0.9508 - val_loss: 1.0707 - val_accuracy: 0.7525\n",
            "Epoch 11/200\n",
            "86/86 [==============================] - 2s 27ms/step - loss: 0.1374 - accuracy: 0.9471 - val_loss: 1.4246 - val_accuracy: 0.6733\n",
            "Epoch 12/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.1190 - accuracy: 0.9570 - val_loss: 1.1393 - val_accuracy: 0.7525\n",
            "Epoch 13/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0681 - accuracy: 0.9758 - val_loss: 1.2455 - val_accuracy: 0.7492\n",
            "Epoch 14/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0875 - accuracy: 0.9706 - val_loss: 1.2692 - val_accuracy: 0.7228\n",
            "Epoch 15/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0953 - accuracy: 0.9644 - val_loss: 1.3375 - val_accuracy: 0.7459\n",
            "Epoch 16/200\n",
            "86/86 [==============================] - 3s 40ms/step - loss: 0.0874 - accuracy: 0.9714 - val_loss: 1.3187 - val_accuracy: 0.7195\n",
            "Epoch 17/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0352 - accuracy: 0.9894 - val_loss: 1.4073 - val_accuracy: 0.7393\n",
            "Epoch 18/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0372 - accuracy: 0.9853 - val_loss: 1.5038 - val_accuracy: 0.7327\n",
            "Epoch 19/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0433 - accuracy: 0.9842 - val_loss: 1.6105 - val_accuracy: 0.7327\n",
            "Epoch 20/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0578 - accuracy: 0.9794 - val_loss: 1.7037 - val_accuracy: 0.7030\n",
            "Epoch 21/200\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 0.0436 - accuracy: 0.9838 - val_loss: 1.7856 - val_accuracy: 0.7459\n",
            "Epoch 22/200\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 0.0451 - accuracy: 0.9846 - val_loss: 1.5323 - val_accuracy: 0.7558\n",
            "Epoch 23/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0299 - accuracy: 0.9868 - val_loss: 1.7725 - val_accuracy: 0.7327\n",
            "Epoch 24/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0403 - accuracy: 0.9846 - val_loss: 1.8193 - val_accuracy: 0.7591\n",
            "Epoch 25/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0672 - accuracy: 0.9732 - val_loss: 1.5476 - val_accuracy: 0.7327\n",
            "Epoch 26/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0501 - accuracy: 0.9813 - val_loss: 1.6898 - val_accuracy: 0.7426\n",
            "Epoch 27/200\n",
            "86/86 [==============================] - 3s 41ms/step - loss: 0.0154 - accuracy: 0.9945 - val_loss: 1.6929 - val_accuracy: 0.7723\n",
            "Epoch 28/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0104 - accuracy: 0.9949 - val_loss: 1.7704 - val_accuracy: 0.7624\n",
            "Epoch 29/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0160 - accuracy: 0.9934 - val_loss: 1.7693 - val_accuracy: 0.7492\n",
            "Epoch 30/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0189 - accuracy: 0.9916 - val_loss: 1.9846 - val_accuracy: 0.7558\n",
            "Epoch 31/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0682 - accuracy: 0.9758 - val_loss: 1.6638 - val_accuracy: 0.7063\n",
            "Epoch 32/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.1361 - accuracy: 0.9519 - val_loss: 1.2728 - val_accuracy: 0.7426\n",
            "Epoch 33/200\n",
            "86/86 [==============================] - 4s 52ms/step - loss: 0.0293 - accuracy: 0.9908 - val_loss: 1.4949 - val_accuracy: 0.7360\n",
            "Epoch 34/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0145 - accuracy: 0.9956 - val_loss: 1.5798 - val_accuracy: 0.7525\n",
            "Epoch 35/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0110 - accuracy: 0.9945 - val_loss: 1.6267 - val_accuracy: 0.7459\n",
            "Epoch 36/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0094 - accuracy: 0.9952 - val_loss: 1.6149 - val_accuracy: 0.7591\n",
            "Epoch 37/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0086 - accuracy: 0.9952 - val_loss: 1.6565 - val_accuracy: 0.7624\n",
            "Epoch 38/200\n",
            "86/86 [==============================] - 3s 41ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 1.6850 - val_accuracy: 0.7591\n",
            "Epoch 39/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0086 - accuracy: 0.9949 - val_loss: 1.7473 - val_accuracy: 0.7360\n",
            "Epoch 40/200\n",
            "86/86 [==============================] - 2s 23ms/step - loss: 0.0080 - accuracy: 0.9952 - val_loss: 1.6977 - val_accuracy: 0.7558\n",
            "Epoch 41/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0078 - accuracy: 0.9949 - val_loss: 1.7442 - val_accuracy: 0.7558\n",
            "Epoch 42/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0078 - accuracy: 0.9952 - val_loss: 1.7566 - val_accuracy: 0.7591\n",
            "Epoch 43/200\n",
            "86/86 [==============================] - 2s 29ms/step - loss: 0.0075 - accuracy: 0.9956 - val_loss: 1.8128 - val_accuracy: 0.7624\n",
            "Epoch 44/200\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 0.0097 - accuracy: 0.9941 - val_loss: 1.7416 - val_accuracy: 0.7624\n",
            "Epoch 45/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0074 - accuracy: 0.9960 - val_loss: 1.7962 - val_accuracy: 0.7558\n",
            "Epoch 46/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0079 - accuracy: 0.9952 - val_loss: 1.8137 - val_accuracy: 0.7558\n",
            "Epoch 47/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0072 - accuracy: 0.9952 - val_loss: 1.8159 - val_accuracy: 0.7558\n",
            "Epoch 48/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0079 - accuracy: 0.9960 - val_loss: 1.8553 - val_accuracy: 0.7525\n",
            "Epoch 49/200\n",
            "86/86 [==============================] - 3s 35ms/step - loss: 0.0079 - accuracy: 0.9934 - val_loss: 1.8834 - val_accuracy: 0.7525\n",
            "Epoch 50/200\n",
            "86/86 [==============================] - 3s 30ms/step - loss: 0.0071 - accuracy: 0.9945 - val_loss: 1.8863 - val_accuracy: 0.7591\n",
            "Epoch 51/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9952 - val_loss: 1.9481 - val_accuracy: 0.7558\n",
            "Epoch 52/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0078 - accuracy: 0.9956 - val_loss: 1.9116 - val_accuracy: 0.7525\n",
            "Epoch 53/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 1.9145 - val_accuracy: 0.7525\n",
            "Epoch 54/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0081 - accuracy: 0.9949 - val_loss: 1.9704 - val_accuracy: 0.7393\n",
            "Epoch 55/200\n",
            "86/86 [==============================] - 3s 40ms/step - loss: 0.0072 - accuracy: 0.9960 - val_loss: 1.9661 - val_accuracy: 0.7459\n",
            "Epoch 56/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0072 - accuracy: 0.9960 - val_loss: 2.0179 - val_accuracy: 0.7525\n",
            "Epoch 57/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0070 - accuracy: 0.9960 - val_loss: 2.0658 - val_accuracy: 0.7525\n",
            "Epoch 58/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0070 - accuracy: 0.9963 - val_loss: 2.0871 - val_accuracy: 0.7558\n",
            "Epoch 59/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0069 - accuracy: 0.9960 - val_loss: 2.0730 - val_accuracy: 0.7492\n",
            "Epoch 60/200\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.1239 - val_accuracy: 0.7492\n",
            "Epoch 61/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 2.1453 - val_accuracy: 0.7525\n",
            "Epoch 62/200\n",
            "86/86 [==============================] - 2s 26ms/step - loss: 0.0072 - accuracy: 0.9949 - val_loss: 2.1528 - val_accuracy: 0.7492\n",
            "Epoch 63/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9956 - val_loss: 2.1351 - val_accuracy: 0.7558\n",
            "Epoch 64/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9956 - val_loss: 2.1376 - val_accuracy: 0.7492\n",
            "Epoch 65/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0074 - accuracy: 0.9956 - val_loss: 2.1344 - val_accuracy: 0.7525\n",
            "Epoch 66/200\n",
            "86/86 [==============================] - 3s 41ms/step - loss: 0.0068 - accuracy: 0.9960 - val_loss: 2.1779 - val_accuracy: 0.7327\n",
            "Epoch 67/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.1842 - val_accuracy: 0.7426\n",
            "Epoch 68/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.1940 - val_accuracy: 0.7459\n",
            "Epoch 69/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0059 - accuracy: 0.9963 - val_loss: 2.2129 - val_accuracy: 0.7459\n",
            "Epoch 70/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.2137 - val_accuracy: 0.7459\n",
            "Epoch 71/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0061 - accuracy: 0.9952 - val_loss: 2.2407 - val_accuracy: 0.7459\n",
            "Epoch 72/200\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 0.0069 - accuracy: 0.9960 - val_loss: 2.2702 - val_accuracy: 0.7426\n",
            "Epoch 73/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0061 - accuracy: 0.9956 - val_loss: 2.2591 - val_accuracy: 0.7459\n",
            "Epoch 74/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.2531 - val_accuracy: 0.7459\n",
            "Epoch 75/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9960 - val_loss: 2.2686 - val_accuracy: 0.7426\n",
            "Epoch 76/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.2975 - val_accuracy: 0.7426\n",
            "Epoch 77/200\n",
            "86/86 [==============================] - 3s 40ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.3184 - val_accuracy: 0.7459\n",
            "Epoch 78/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 2.3532 - val_accuracy: 0.7228\n",
            "Epoch 79/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0071 - accuracy: 0.9960 - val_loss: 2.2887 - val_accuracy: 0.7426\n",
            "Epoch 80/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.2936 - val_accuracy: 0.7459\n",
            "Epoch 81/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.3619 - val_accuracy: 0.7459\n",
            "Epoch 82/200\n",
            "86/86 [==============================] - 2s 26ms/step - loss: 0.0063 - accuracy: 0.9971 - val_loss: 2.4019 - val_accuracy: 0.7426\n",
            "Epoch 83/200\n",
            "86/86 [==============================] - 3s 37ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.4450 - val_accuracy: 0.7492\n",
            "Epoch 84/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.4683 - val_accuracy: 0.7459\n",
            "Epoch 85/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.4710 - val_accuracy: 0.7459\n",
            "Epoch 86/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.4945 - val_accuracy: 0.7393\n",
            "Epoch 87/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.0060 - accuracy: 0.9956 - val_loss: 2.5049 - val_accuracy: 0.7426\n",
            "Epoch 88/200\n",
            "86/86 [==============================] - 4s 45ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.5025 - val_accuracy: 0.7426\n",
            "Epoch 89/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9960 - val_loss: 2.5665 - val_accuracy: 0.7525\n",
            "Epoch 90/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.2834 - accuracy: 0.9012 - val_loss: 0.9780 - val_accuracy: 0.6964\n",
            "Epoch 91/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.3072 - accuracy: 0.8976 - val_loss: 0.9913 - val_accuracy: 0.7261\n",
            "Epoch 92/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.1345 - accuracy: 0.9545 - val_loss: 1.1420 - val_accuracy: 0.7294\n",
            "Epoch 93/200\n",
            "86/86 [==============================] - 2s 27ms/step - loss: 0.1443 - accuracy: 0.9446 - val_loss: 1.1400 - val_accuracy: 0.7360\n",
            "Epoch 94/200\n",
            "86/86 [==============================] - 3s 37ms/step - loss: 0.0784 - accuracy: 0.9717 - val_loss: 1.3501 - val_accuracy: 0.7393\n",
            "Epoch 95/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0441 - accuracy: 0.9857 - val_loss: 1.6208 - val_accuracy: 0.7327\n",
            "Epoch 96/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0462 - accuracy: 0.9794 - val_loss: 1.5744 - val_accuracy: 0.7426\n",
            "Epoch 97/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0298 - accuracy: 0.9883 - val_loss: 1.8782 - val_accuracy: 0.7426\n",
            "Epoch 98/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0338 - accuracy: 0.9857 - val_loss: 1.5546 - val_accuracy: 0.7459\n",
            "Epoch 99/200\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 0.0283 - accuracy: 0.9886 - val_loss: 1.7778 - val_accuracy: 0.7327\n",
            "Epoch 100/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0121 - accuracy: 0.9956 - val_loss: 1.8829 - val_accuracy: 0.7492\n",
            "Epoch 101/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0079 - accuracy: 0.9945 - val_loss: 1.9290 - val_accuracy: 0.7492\n",
            "Epoch 102/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0085 - accuracy: 0.9945 - val_loss: 1.9835 - val_accuracy: 0.7558\n",
            "Epoch 103/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0076 - accuracy: 0.9956 - val_loss: 2.0235 - val_accuracy: 0.7459\n",
            "Epoch 104/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0072 - accuracy: 0.9960 - val_loss: 2.0559 - val_accuracy: 0.7459\n",
            "Epoch 105/200\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.0071 - accuracy: 0.9960 - val_loss: 2.0682 - val_accuracy: 0.7459\n",
            "Epoch 106/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.0894 - val_accuracy: 0.7525\n",
            "Epoch 107/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0077 - accuracy: 0.9960 - val_loss: 2.1029 - val_accuracy: 0.7459\n",
            "Epoch 108/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.1351 - val_accuracy: 0.7459\n",
            "Epoch 109/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0072 - accuracy: 0.9963 - val_loss: 2.1622 - val_accuracy: 0.7525\n",
            "Epoch 110/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 2.1825 - val_accuracy: 0.7492\n",
            "Epoch 111/200\n",
            "86/86 [==============================] - 3s 33ms/step - loss: 0.0067 - accuracy: 0.9960 - val_loss: 2.2104 - val_accuracy: 0.7459\n",
            "Epoch 112/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.2310 - val_accuracy: 0.7558\n",
            "Epoch 113/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9956 - val_loss: 2.2271 - val_accuracy: 0.7525\n",
            "Epoch 114/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0072 - accuracy: 0.9945 - val_loss: 2.2397 - val_accuracy: 0.7591\n",
            "Epoch 115/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0070 - accuracy: 0.9956 - val_loss: 2.2749 - val_accuracy: 0.7525\n",
            "Epoch 116/200\n",
            "86/86 [==============================] - 3s 40ms/step - loss: 0.0063 - accuracy: 0.9960 - val_loss: 2.3000 - val_accuracy: 0.7525\n",
            "Epoch 117/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9949 - val_loss: 2.3008 - val_accuracy: 0.7492\n",
            "Epoch 118/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9952 - val_loss: 2.3284 - val_accuracy: 0.7492\n",
            "Epoch 119/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9949 - val_loss: 2.3360 - val_accuracy: 0.7492\n",
            "Epoch 120/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.3408 - val_accuracy: 0.7525\n",
            "Epoch 121/200\n",
            "86/86 [==============================] - 3s 29ms/step - loss: 0.0068 - accuracy: 0.9952 - val_loss: 2.3480 - val_accuracy: 0.7558\n",
            "Epoch 122/200\n",
            "86/86 [==============================] - 3s 35ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.3681 - val_accuracy: 0.7558\n",
            "Epoch 123/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.3750 - val_accuracy: 0.7492\n",
            "Epoch 124/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.3803 - val_accuracy: 0.7492\n",
            "Epoch 125/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0062 - accuracy: 0.9960 - val_loss: 2.4157 - val_accuracy: 0.7492\n",
            "Epoch 126/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9952 - val_loss: 2.4435 - val_accuracy: 0.7459\n",
            "Epoch 127/200\n",
            "86/86 [==============================] - 3s 39ms/step - loss: 0.0061 - accuracy: 0.9956 - val_loss: 2.4831 - val_accuracy: 0.7525\n",
            "Epoch 128/200\n",
            "86/86 [==============================] - 2s 27ms/step - loss: 0.0063 - accuracy: 0.9960 - val_loss: 2.4993 - val_accuracy: 0.7525\n",
            "Epoch 129/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9963 - val_loss: 2.5046 - val_accuracy: 0.7525\n",
            "Epoch 130/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0059 - accuracy: 0.9956 - val_loss: 2.5273 - val_accuracy: 0.7525\n",
            "Epoch 131/200\n",
            "86/86 [==============================] - 2s 26ms/step - loss: 0.0065 - accuracy: 0.9952 - val_loss: 2.5385 - val_accuracy: 0.7525\n",
            "Epoch 132/200\n",
            "86/86 [==============================] - 3s 29ms/step - loss: 0.0064 - accuracy: 0.9952 - val_loss: 2.5249 - val_accuracy: 0.7525\n",
            "Epoch 133/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.5350 - val_accuracy: 0.7525\n",
            "Epoch 134/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9963 - val_loss: 2.5565 - val_accuracy: 0.7525\n",
            "Epoch 135/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9952 - val_loss: 2.5200 - val_accuracy: 0.7525\n",
            "Epoch 136/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9960 - val_loss: 2.5894 - val_accuracy: 0.7492\n",
            "Epoch 137/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0062 - accuracy: 0.9967 - val_loss: 2.7080 - val_accuracy: 0.7426\n",
            "Epoch 138/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.7067 - val_accuracy: 0.7393\n",
            "Epoch 139/200\n",
            "86/86 [==============================] - 2s 28ms/step - loss: 0.0070 - accuracy: 0.9960 - val_loss: 2.6513 - val_accuracy: 0.7525\n",
            "Epoch 140/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.2175 - accuracy: 0.9229 - val_loss: 1.0085 - val_accuracy: 0.7393\n",
            "Epoch 141/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.1272 - accuracy: 0.9593 - val_loss: 1.1773 - val_accuracy: 0.7492\n",
            "Epoch 142/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0784 - accuracy: 0.9732 - val_loss: 1.4073 - val_accuracy: 0.7426\n",
            "Epoch 143/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0518 - accuracy: 0.9842 - val_loss: 1.6752 - val_accuracy: 0.7393\n",
            "Epoch 144/200\n",
            "86/86 [==============================] - 3s 40ms/step - loss: 0.0277 - accuracy: 0.9875 - val_loss: 1.7570 - val_accuracy: 0.7327\n",
            "Epoch 145/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0202 - accuracy: 0.9923 - val_loss: 1.9524 - val_accuracy: 0.7294\n",
            "Epoch 146/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0133 - accuracy: 0.9949 - val_loss: 1.9091 - val_accuracy: 0.7426\n",
            "Epoch 147/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0084 - accuracy: 0.9960 - val_loss: 2.0106 - val_accuracy: 0.7360\n",
            "Epoch 148/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0068 - accuracy: 0.9960 - val_loss: 2.0531 - val_accuracy: 0.7393\n",
            "Epoch 149/200\n",
            "86/86 [==============================] - 3s 32ms/step - loss: 0.0068 - accuracy: 0.9963 - val_loss: 2.0896 - val_accuracy: 0.7360\n",
            "Epoch 150/200\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 0.0068 - accuracy: 0.9956 - val_loss: 2.0767 - val_accuracy: 0.7426\n",
            "Epoch 151/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9960 - val_loss: 2.1117 - val_accuracy: 0.7426\n",
            "Epoch 152/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0069 - accuracy: 0.9963 - val_loss: 2.1355 - val_accuracy: 0.7426\n",
            "Epoch 153/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0070 - accuracy: 0.9963 - val_loss: 2.1099 - val_accuracy: 0.7426\n",
            "Epoch 154/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0102 - accuracy: 0.9963 - val_loss: 2.1643 - val_accuracy: 0.7360\n",
            "Epoch 155/200\n",
            "86/86 [==============================] - 3s 39ms/step - loss: 0.0072 - accuracy: 0.9956 - val_loss: 2.1602 - val_accuracy: 0.7294\n",
            "Epoch 156/200\n",
            "86/86 [==============================] - 2s 26ms/step - loss: 0.0071 - accuracy: 0.9960 - val_loss: 2.1829 - val_accuracy: 0.7327\n",
            "Epoch 157/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9952 - val_loss: 2.2097 - val_accuracy: 0.7426\n",
            "Epoch 158/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9949 - val_loss: 2.2732 - val_accuracy: 0.7294\n",
            "Epoch 159/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9967 - val_loss: 2.2615 - val_accuracy: 0.7393\n",
            "Epoch 160/200\n",
            "86/86 [==============================] - 2s 27ms/step - loss: 0.0065 - accuracy: 0.9952 - val_loss: 2.2672 - val_accuracy: 0.7459\n",
            "Epoch 161/200\n",
            "86/86 [==============================] - 3s 37ms/step - loss: 0.0067 - accuracy: 0.9952 - val_loss: 2.2772 - val_accuracy: 0.7459\n",
            "Epoch 162/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9956 - val_loss: 2.3076 - val_accuracy: 0.7492\n",
            "Epoch 163/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.3171 - val_accuracy: 0.7459\n",
            "Epoch 164/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0058 - accuracy: 0.9967 - val_loss: 2.3377 - val_accuracy: 0.7459\n",
            "Epoch 165/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.3212 - val_accuracy: 0.7426\n",
            "Epoch 166/200\n",
            "86/86 [==============================] - 3s 35ms/step - loss: 0.0062 - accuracy: 0.9956 - val_loss: 2.3430 - val_accuracy: 0.7426\n",
            "Epoch 167/200\n",
            "86/86 [==============================] - 3s 29ms/step - loss: 0.0059 - accuracy: 0.9956 - val_loss: 2.3636 - val_accuracy: 0.7426\n",
            "Epoch 168/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0061 - accuracy: 0.9952 - val_loss: 2.3882 - val_accuracy: 0.7426\n",
            "Epoch 169/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0060 - accuracy: 0.9956 - val_loss: 2.3929 - val_accuracy: 0.7426\n",
            "Epoch 170/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0061 - accuracy: 0.9949 - val_loss: 2.3964 - val_accuracy: 0.7426\n",
            "Epoch 171/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0060 - accuracy: 0.9952 - val_loss: 2.3981 - val_accuracy: 0.7426\n",
            "Epoch 172/200\n",
            "86/86 [==============================] - 4s 41ms/step - loss: 0.0067 - accuracy: 0.9949 - val_loss: 2.4188 - val_accuracy: 0.7426\n",
            "Epoch 173/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 2.4561 - val_accuracy: 0.7459\n",
            "Epoch 174/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 2.4560 - val_accuracy: 0.7426\n",
            "Epoch 175/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0066 - accuracy: 0.9960 - val_loss: 2.4680 - val_accuracy: 0.7426\n",
            "Epoch 176/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.4834 - val_accuracy: 0.7426\n",
            "Epoch 177/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0067 - accuracy: 0.9960 - val_loss: 2.4899 - val_accuracy: 0.7393\n",
            "Epoch 178/200\n",
            "86/86 [==============================] - 3s 35ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.4989 - val_accuracy: 0.7393\n",
            "Epoch 179/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0065 - accuracy: 0.9960 - val_loss: 2.5102 - val_accuracy: 0.7393\n",
            "Epoch 180/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9963 - val_loss: 2.5289 - val_accuracy: 0.7393\n",
            "Epoch 181/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0066 - accuracy: 0.9960 - val_loss: 2.5344 - val_accuracy: 0.7393\n",
            "Epoch 182/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.5680 - val_accuracy: 0.7393\n",
            "Epoch 183/200\n",
            "86/86 [==============================] - 4s 45ms/step - loss: 0.0063 - accuracy: 0.9960 - val_loss: 2.5863 - val_accuracy: 0.7393\n",
            "Epoch 184/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.0065 - accuracy: 0.9956 - val_loss: 2.6159 - val_accuracy: 0.7393\n",
            "Epoch 185/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.6142 - val_accuracy: 0.7393\n",
            "Epoch 186/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0064 - accuracy: 0.9960 - val_loss: 2.6381 - val_accuracy: 0.7393\n",
            "Epoch 187/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9952 - val_loss: 2.6586 - val_accuracy: 0.7426\n",
            "Epoch 188/200\n",
            "86/86 [==============================] - 3s 36ms/step - loss: 0.0064 - accuracy: 0.9956 - val_loss: 2.6744 - val_accuracy: 0.7360\n",
            "Epoch 189/200\n",
            "86/86 [==============================] - 2s 28ms/step - loss: 0.0062 - accuracy: 0.9949 - val_loss: 2.7136 - val_accuracy: 0.7393\n",
            "Epoch 190/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0062 - accuracy: 0.9963 - val_loss: 2.7404 - val_accuracy: 0.7294\n",
            "Epoch 191/200\n",
            "86/86 [==============================] - 2s 24ms/step - loss: 0.0063 - accuracy: 0.9956 - val_loss: 2.7200 - val_accuracy: 0.7393\n",
            "Epoch 192/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0058 - accuracy: 0.9956 - val_loss: 2.7244 - val_accuracy: 0.7327\n",
            "Epoch 193/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0068 - accuracy: 0.9952 - val_loss: 2.7310 - val_accuracy: 0.7426\n",
            "Epoch 194/200\n",
            "86/86 [==============================] - 3s 41ms/step - loss: 0.0067 - accuracy: 0.9956 - val_loss: 2.7535 - val_accuracy: 0.7426\n",
            "Epoch 195/200\n",
            "86/86 [==============================] - 2s 26ms/step - loss: 0.0069 - accuracy: 0.9952 - val_loss: 2.7698 - val_accuracy: 0.7426\n",
            "Epoch 196/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0075 - accuracy: 0.9963 - val_loss: 2.5317 - val_accuracy: 0.7459\n",
            "Epoch 197/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.0087 - accuracy: 0.9941 - val_loss: 2.7034 - val_accuracy: 0.7162\n",
            "Epoch 198/200\n",
            "86/86 [==============================] - 2s 25ms/step - loss: 0.3094 - accuracy: 0.8954 - val_loss: 1.2778 - val_accuracy: 0.7063\n",
            "Epoch 199/200\n",
            "86/86 [==============================] - 3s 34ms/step - loss: 0.0951 - accuracy: 0.9677 - val_loss: 1.4351 - val_accuracy: 0.7459\n",
            "Epoch 200/200\n",
            "86/86 [==============================] - 3s 31ms/step - loss: 0.0597 - accuracy: 0.9809 - val_loss: 1.4624 - val_accuracy: 0.7393\n"
          ]
        }
      ],
      "source": [
        "# Train the model and save the training history\n",
        "history = model.fit(train_pad_trunc_seq, train_labels, epochs=200, validation_data=(val_pad_trunc_seq, val_labels))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 72,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "icfjRpZh0Pao",
        "outputId": "126df823-1f8a-4b37-d503-ec5f79dcccfc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "10/10 [==============================] - 1s 6ms/step\n",
            "Precision:  0.7381191607532847\n",
            "Recall:  0.7392739273927392\n",
            "F1-score:  0.7385938226649057\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score\n",
        "\n",
        "# Make predictions on the validation set\n",
        "val_predictions = model.predict(val_pad_trunc_seq)\n",
        "val_predicted_labels = np.argmax(val_predictions, axis=1)\n",
        "\n",
        "# Calculate precision, recall, and F1-score\n",
        "precision = precision_score(val_labels, val_predicted_labels, average='weighted')\n",
        "recall = recall_score(val_labels, val_predicted_labels, average='weighted')\n",
        "f1 = f1_score(val_labels, val_predicted_labels, average='weighted')\n",
        "\n",
        "print(\"Precision: \", precision)\n",
        "print(\"Recall: \", recall)\n",
        "print(\"F1-score: \", f1)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8LTrwH3183K"
      },
      "source": [
        "Based on the evaluation metrics, our model performs relatively well with relatively high values of precision, recall, and F1-score"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
